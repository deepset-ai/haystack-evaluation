questions,contexts,true_answers,predicted_answers,context_relevance,faithfulness,sas
What are the two main tasks BERT is pre-trained on?,"['65.1 0.0 100.0/ 50.0\nCBoW 44.3 62.1 49.0/71.2 51.6 0.0 / 0.4 14.0/13.6 49.7 53.0 65.1 -0.4 100.0/ 50.0\nBERT 69.0 77.4 75.7/83.6 70.6 70.0 / 24.0 72.0/71.3 71.6 69.5 64.3 23.0 97.8 / 51.7\nBERT++ 71.5 79.0 84.7/90.4 73.8 70.0 / 24.1 72.0/71.3 79.0 69.5 64.3 38.0 99.4 / 51.4\nOutside Best - 80.4 - / - 84.4 70.4 */24.5*74.8/73.0 82.7 - - - - / -\nHuman (est.) 89.8 89.0 95.8/98.9 100.0 81.8*/51.9* 91.7/91.3 93.6 80.0 100.0 77.0 99.3 / 99.7\n5 Experiments\n5.1 Baselines\nBERT Our main baselines are built around BERT, variants of which are among the most successful\napproach on GLUE at the time of writing. Speciﬁcally, we use the bert-large-cased variant.\nFollowing the practice recommended in Devlin et al. (2019), for each task, we use the simplest\npossible architecture on top of BERT. We ﬁne-tune a copy of the pretrained BERT model separately\nfor each task, and leave the development of multi-task learning models to future work. For training,\nwe use the procedure speciﬁed in Devlin et al. (2019): We use Adam (Kingma and Ba, 2014) with an\ninitial learning rate of 10−5and ﬁne-tune for a maximum of 10 epochs.\nFor classiﬁcation tasks with sentence-pair inputs (BoolQ, CB, RTE, WiC), we concatenate the\nsentences with a [SEP]token, feed the fused input to BERT, and use a logistic regression classiﬁer\nthat sees the representation corresponding to [CLS]. For WiC, we also concatenate the representation\nof the marked word. For COPA, MultiRC, and ReCoRD, for each answer choice, we similarly\nconcatenate the context with that answer choice and feed the resulting sequence into BERT to produce\nan answer representation. For COPA, we ', 'and replacement is performed once in the be-\nginning and saved for the duration of training, al-\nthough in practice, data is duplicated so the mask\nis not always the same for every training sentence\n(see Section 4.1).\nNext Sentence Prediction (NSP) NSP is a bi-\nnary classiﬁcation loss for predicting whether two\nsegments follow each other in the original text.\nPositive examples are created by taking consecu-\ntive sentences from the text corpus. Negative ex-\namples are created by pairing segments from dif-\nferent documents. Positive and negative examples\nare sampled with equal probability.\nThe NSP objective was designed to improve\nperformance on downstream tasks, such as Natural\nLanguage Inference ( Bowman et al. ,2015 ), which\nrequire reasoning about the relationships between\npairs of sentences.\n2.4 Optimization\nBERT is optimized with Adam ( Kingma and Ba ,\n2015 ) using the following parameters: β1= 0.9,\nβ2= 0.999,ǫ=1e-6 and L2weight de-\ncay of0.01. The learning rate is warmed up\nover the ﬁrst 10,000 steps to a peak value of\n1e-4, and then linearly decayed. BERT trains\nwith a dropout of 0.1 on all layers and at-\ntention weights, and a GELU activation func-\ntion ( Hendrycks and Gimpel ,2016 ). Models are\npretrained for S=1,000,000 updates, with mini-\nbatches containing B=256 sequences of maxi-\nmum length T=512 tokens.\n2.5 Data\nBERT is trained on a combination of B OOK COR-\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\nwhich totals 16GB of uncompressed text.3\n3 Experimental Setup\nIn this section, we describe the experimental setup\nfor our replication study of BERT.\n3.1 Implementation\nWe reimplement BERT in FAIRSEQ (Ott et al. ,\n2019 ). We primarily follow the original BERT\n3Yang et al. (2019 ) use the same dataset but report having\nonly 13GB ', 'steps in our\nframework: pre-training and ﬁne-tuning . Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For ﬁne-\ntuning, the BERT model is ﬁrst initialized with\nthe pre-trained parameters, and all of the param-\neters are ﬁne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate ﬁne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\nA distinctive feature of BERT is its uniﬁed ar-\nchitecture across different tasks. There is mini-mal difference between the pre-trained architec-\nture and the ﬁnal downstream architecture.\nModel Architecture BERT’s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthetensor2tensor library.1Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as “The Annotated Transformer.”2\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERT BASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERT LARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\nBERT BASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n1https://github.com/tensorﬂow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\ni.e., 3072 for ']",Masked LM (MLM) and Next Sentence Prediction (NSP).,BOOK CORPUS and English WIKIPEDIA,1.0,1.0,0.22011421620845795
"What model sizes are reported for BERT, and what are their specifications?","['/ 87.4 86.5 (89.0 / 85.5)\nXLNet (Yang et al., 2019) 2 90.8 / 90.8 92.3 95.1 / 89.7 90.6 / 87.9 85.4 (88.6 / 84.0)\nMegatron-336M 1 89.7 / 90.0 92.3 94.2 / 88.0 88.1 / 84.8 83.0 (86.9 / 81.5)\nMegatron-1.3B 1 90.9 / 91.0 92.6 94.9 / 89.1 90.2 / 87.1 87.3 (90.4 / 86.1)\nMegatron-3.9B 1 91.4 / 91.4 92.7 95.5 / 90.0 91.2 / 88.5 89.5 (91.8 / 88.6)\nALBERT ensemble (Lan et al., 2019) 95.5 / 90.1 91.4 / 88.9 89.4 (91.2 / 88.6)\nMegatron-3.9B ensemble 95.8 / 90.5 91.7 / 89.0 90.9 (93.1 / 90.0)\nFigure 7. Training loss for BERT model using the original architec-\nture (a) and the rearranged architecture (b). Left ﬁgure shows the\ntraining loss for 336M and 752M BERT model. While the original\narchitecture performs well on the 336M model, the modiﬁcations\nin (b) enable stable training with lower training loss.\nUsing the architecture change in Figure 7(b), we consider\nthree different cases as detailed in Table 4. The 336M model\nhas the same size as BERT-large. The 1.3B is the same as\nthe BERT-xlarge conﬁguration that was previously shown\nto get worse results than the 336M BERT-large model (Lan\net al., 2019). We further scale the BERT model using both\nlarger hidden size as well as more layers to arrive at the 3.9B\nparameter case. In all cases, the hidden size per attention\nhead is kept constant at 64. 336M and 1.3B models are\ntrained for 2 million iterations while the 3.9B model is\ntrained for 1.5 million iterations and is still training.\nOn a 3% held-out set, 336M, 1.3B, and 3.9B models achieve\nvalidation set perplexity ', 'in\nour training data.\n5.3. Bi-directional Transformer Results Using BERT\nIn this section, we apply our methodology to BERT-style\ntransformer models and study the effect of model scaling\non several downstream tasks. Prior work (Lan et al., 2019)\nfound that increasing model size beyond BERT-large with\n336M parameters results in unexpected model degradation.\nTo address this degradation, the authors of that work (Lan\net al., 2019) introduced parameter sharing and showed that\nthat their models scale much better compared to the original\nBERT model.\nWe further investigated this behaviour and empirically\ndemonstrated that rearranging the order of the layer nor-\nmalization and the residual connections as shown in Figure\n7 is critical to enable the scaling of the BERT-style mod-\nels beyond BERT-Large. The architecture (b) in Figure 7\neliminates instabilities observed using the original BERT\narchitecture in (a) and also has a lower training loss. To\nthe best of our knowledge, we are the ﬁrst to report such a\nchange enables training larger BERT models.\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 5. Development set results for MNLI, QQP, SQuAD 1.1 and SQuAD 2.0 and test set results for RACE. The trained tokens represents\nconsumed tokens during model pretraining (proportional to batch size times number of iterations) normalized by consumed tokens during\nmodel pretraining for our 336M model.\nModeltrained tokens MNLI m/mm QQP SQuAD 1.1 SQuAD 2.0 RACE m/h\nratio accuracy accuracy F1 / EM F1 / EM accuracy\n(dev set) (dev set) (dev set) (dev set) (test set)\nRoBERTa (Liu et al., 2019b) 2 90.2 / 90.2 92.2 94.6 / 88.9 89.4 / 86.5 83.2 (86.5 / 81.8)\nALBERT (Lan et al., 2019) 3 90.8 92.2 94.8 / 89.3 90.2 ', 'and conﬁgurations listed in Table 2. The 355M\nmodel is equivalent in size and conﬁguration of BERT-Large\nmodel (Devlin et al., 2018). The 2.5B model is bigger than\nthe previous largest GPT-2 model, and the 8.3B model is\nlarger than any left-to-right transformer language model\never trained, to the best of our knowledge. To train and eval-\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 2. Model conﬁgurations used for GPT-2.\nHidden Time\nParameter Layers Hidden Attn Size Total per\nCount Size Heads per GPUs Epoch\nHead (days)\n355M 24 1024 16 64 64 0.86\n2.5B 54 1920 20 96 128 2.27\n8.3B 72 3072 24 128 512 2.10\nTable 3. Zero-shot results. SOTA are from (Khandelwal et al.,\n2019) for Wikitext103 and (Radford et al., 2019) for LAMBADA.\nModel Wikitext103 LAMBADA\nPerplexity↓ Accuracy↑\n355M 19.31 45.18%\n2.5B 12.76 61.73%\n8.3B 10.81 66.51%\nPrevious SOTA 15.79 63.24%\nuate our language models we use the procedure described in\nsection 4. Table 2 also lists the time it takes to advance one\nepoch which is equivalent to 68,507 iterations. For example,\nfor the 8.3B model on 512 GPUs, each epoch takes around\ntwo days. Compared to the conﬁgurations used for our scal-\ning studies in Table 1, the 2.5B model is the same, the 8.3B\nmodel has 24 attention heads instead of 32, and the 355M is\nmuch smaller than any seen previously while still using 64\nGPUs to train, leading to the much lower time per epoch.\nFigure 6 shows validation perpelixity as a function of num-\nber of iterations. As the model size increases, the validation\nperpelixity decreases and reaches a validation perplexity of\n9.27 for the 8.3B model. We report the zero-shot evaluation\nof the trained models on the ']","BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).",None,1.0,0.0,0.06309785693883896
How does BERT's architecture facilitate the use of a unified model across diverse NLP tasks?,"['steps in our\nframework: pre-training and ﬁne-tuning . Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For ﬁne-\ntuning, the BERT model is ﬁrst initialized with\nthe pre-trained parameters, and all of the param-\neters are ﬁne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate ﬁne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\nA distinctive feature of BERT is its uniﬁed ar-\nchitecture across different tasks. There is mini-mal difference between the pre-trained architec-\nture and the ﬁnal downstream architecture.\nModel Architecture BERT’s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthetensor2tensor library.1Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as “The Annotated Transformer.”2\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERT BASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERT LARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\nBERT BASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n1https://github.com/tensorﬂow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\ni.e., 3072 for ', 'ELMo,\nOpenAI GPT and BERT. The comparisons be-\ntween the model architectures are shown visually\nin Figure 3. Note that in addition to the architec-\nture differences, BERT and OpenAI GPT are ﬁne-\ntuning approaches, while ELMo is a feature-based\napproach.\nThe most comparable existing pre-training\nmethod to BERT is OpenAI GPT, which trains a\nleft-to-right Transformer LM on a large text cor-\npus. In fact, many of the design decisions in BERT\nwere intentionally made to make it as close to\nGPT as possible so that the two methods could be\nminimally compared. The core argument of this\nwork is that the bi-directionality and the two pre-\ntraining tasks presented in Section 3.1 account for\nthe majority of the empirical improvements, but\nwe do note that there are several other differences\nbetween how BERT and GPT were trained:\n• GPT is trained on the BooksCorpus (800M\nwords); BERT is trained on the BooksCor-\npus (800M words) and Wikipedia (2,500M\nwords).\n• GPT uses a sentence separator ( [SEP] ) and\nclassiﬁer token ( [CLS] ) which are only in-\ntroduced at ﬁne-tuning time; BERT learns\n[SEP] ,[CLS] and sentence A/Bembed-\ndings during pre-training.\n• GPT was trained for 1M steps with a batch\nsize of 32,000 words; BERT was trained for\n1M steps with a batch size of 128,000 words.\n• GPT used the same learning rate of 5e-5 for\nall ﬁne-tuning experiments; BERT chooses a\ntask-speciﬁc ﬁne-tuning learning rate which\nperforms the best on the development set.To isolate the effect of these differences, we per-\nform ablation experiments in Section 5.1 which\ndemonstrate that the majority of the improvements\nare in fact coming from the two pre-training tasks\nand the bidirectionality they enable.\nA.5 Illustrations of Fine-tuning on Different\nTasks\nThe illustration of ﬁne-tuning BERT ', '65.1 0.0 100.0/ 50.0\nCBoW 44.3 62.1 49.0/71.2 51.6 0.0 / 0.4 14.0/13.6 49.7 53.0 65.1 -0.4 100.0/ 50.0\nBERT 69.0 77.4 75.7/83.6 70.6 70.0 / 24.0 72.0/71.3 71.6 69.5 64.3 23.0 97.8 / 51.7\nBERT++ 71.5 79.0 84.7/90.4 73.8 70.0 / 24.1 72.0/71.3 79.0 69.5 64.3 38.0 99.4 / 51.4\nOutside Best - 80.4 - / - 84.4 70.4 */24.5*74.8/73.0 82.7 - - - - / -\nHuman (est.) 89.8 89.0 95.8/98.9 100.0 81.8*/51.9* 91.7/91.3 93.6 80.0 100.0 77.0 99.3 / 99.7\n5 Experiments\n5.1 Baselines\nBERT Our main baselines are built around BERT, variants of which are among the most successful\napproach on GLUE at the time of writing. Speciﬁcally, we use the bert-large-cased variant.\nFollowing the practice recommended in Devlin et al. (2019), for each task, we use the simplest\npossible architecture on top of BERT. We ﬁne-tune a copy of the pretrained BERT model separately\nfor each task, and leave the development of multi-task learning models to future work. For training,\nwe use the procedure speciﬁed in Devlin et al. (2019): We use Adam (Kingma and Ba, 2014) with an\ninitial learning rate of 10−5and ﬁne-tune for a maximum of 10 epochs.\nFor classiﬁcation tasks with sentence-pair inputs (BoolQ, CB, RTE, WiC), we concatenate the\nsentences with a [SEP]token, feed the fused input to BERT, and use a logistic regression classiﬁer\nthat sees the representation corresponding to [CLS]. For WiC, we also concatenate the representation\nof the marked word. For COPA, MultiRC, and ReCoRD, for each answer choice, we similarly\nconcatenate the context with that answer choice and feed the resulting sequence into BERT to produce\nan answer representation. For COPA, we ']","BERT uses a multi-layer bidirectional Transformer encoder architecture, allowing for minimal task-specific architecture modifications in fine-tuning.","BERT's architecture facilitates the use of a unified model across diverse NLP tasks by having minimal differences between the pre-trained architecture and the final downstream architecture. This means that even though separate fine-tuned models are used for each downstream task, they are all initialized with the same pre-trained parameters, allowing for a consistent and unified approach to handling different tasks.",1.0,1.0,0.675822377204895
Can you describe the modifications LLaMA makes to the transformer architecture for improved performance?,"['in\nour training data.\n5.3. Bi-directional Transformer Results Using BERT\nIn this section, we apply our methodology to BERT-style\ntransformer models and study the effect of model scaling\non several downstream tasks. Prior work (Lan et al., 2019)\nfound that increasing model size beyond BERT-large with\n336M parameters results in unexpected model degradation.\nTo address this degradation, the authors of that work (Lan\net al., 2019) introduced parameter sharing and showed that\nthat their models scale much better compared to the original\nBERT model.\nWe further investigated this behaviour and empirically\ndemonstrated that rearranging the order of the layer nor-\nmalization and the residual connections as shown in Figure\n7 is critical to enable the scaling of the BERT-style mod-\nels beyond BERT-Large. The architecture (b) in Figure 7\neliminates instabilities observed using the original BERT\narchitecture in (a) and also has a lower training loss. To\nthe best of our knowledge, we are the ﬁrst to report such a\nchange enables training larger BERT models.\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 5. Development set results for MNLI, QQP, SQuAD 1.1 and SQuAD 2.0 and test set results for RACE. The trained tokens represents\nconsumed tokens during model pretraining (proportional to batch size times number of iterations) normalized by consumed tokens during\nmodel pretraining for our 336M model.\nModeltrained tokens MNLI m/mm QQP SQuAD 1.1 SQuAD 2.0 RACE m/h\nratio accuracy accuracy F1 / EM F1 / EM accuracy\n(dev set) (dev set) (dev set) (dev set) (test set)\nRoBERTa (Liu et al., 2019b) 2 90.2 / 90.2 92.2 94.6 / 88.9 89.4 / 86.5 83.2 (86.5 / 81.8)\nALBERT (Lan et al., 2019) 3 90.8 92.2 94.8 / 89.3 90.2 ', 'architecture, GPT-2, and\nan encoder architecture, BERT.\nFigure 2 shows a schematic diagram of the model we used.\nWe refer the reader to prior work for a detailed descrip-\ntion of the model architecture (Vaswani et al., 2017; Devlin\net al., 2018; Radford et al., 2019). It is worthwhile to men-\ntion that both GPT-2 and BERT use GeLU (Hendrycks &\nGimpel, 2016) nonlinearities and layer normalization (Ba\net al., 2016) to the input of the multi-head attention and feed\nforward layers, whereas the original transformer (Vaswani\net al., 2017) uses ReLU nonlinearities and applies layer\nnormalization to outputs.\n2.3. Data and Model Parallelism in Deep Learning\nThere are two central paradigms for scaling out deep neu-\nral network training to numerous hardware accelerators:\ndata parallelism (Valiant, 1990) where a training minibatch\nis split across multiple workers, and model parallelism in\nwhich the memory usage and computation of a model is\ndistributed across multiple workers. By increasing the mini-\nbatch size proportionally to the number of available work-\ners (i.e. weak scaling ), one observes near linear scaling\nin training data throughput. However, large batch train-\ning introduces complications into the optimization process\nthat can result in reduced accuracy or longer time to conver-\ngence, offsetting the beneﬁt of increased training throughput\n(Keskar et al., 2017). Further research (Goyal et al., 2017;\nYou et al., 2017; 2019) has developed techniques to miti-gate these effects and drive down the training time of large\nneural networks. To scale out training even further, parallel\nwork (Chen et al., 2016) has combined data parallelism with\nactivation checkpointing: recomputing activations in the\nbackward pass without storing them in the forward pass to\nreduce memory requirements.\nHowever, these techniques have one fundamental limitation\nin the ', 'and conﬁgurations listed in Table 2. The 355M\nmodel is equivalent in size and conﬁguration of BERT-Large\nmodel (Devlin et al., 2018). The 2.5B model is bigger than\nthe previous largest GPT-2 model, and the 8.3B model is\nlarger than any left-to-right transformer language model\never trained, to the best of our knowledge. To train and eval-\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 2. Model conﬁgurations used for GPT-2.\nHidden Time\nParameter Layers Hidden Attn Size Total per\nCount Size Heads per GPUs Epoch\nHead (days)\n355M 24 1024 16 64 64 0.86\n2.5B 54 1920 20 96 128 2.27\n8.3B 72 3072 24 128 512 2.10\nTable 3. Zero-shot results. SOTA are from (Khandelwal et al.,\n2019) for Wikitext103 and (Radford et al., 2019) for LAMBADA.\nModel Wikitext103 LAMBADA\nPerplexity↓ Accuracy↑\n355M 19.31 45.18%\n2.5B 12.76 61.73%\n8.3B 10.81 66.51%\nPrevious SOTA 15.79 63.24%\nuate our language models we use the procedure described in\nsection 4. Table 2 also lists the time it takes to advance one\nepoch which is equivalent to 68,507 iterations. For example,\nfor the 8.3B model on 512 GPUs, each epoch takes around\ntwo days. Compared to the conﬁgurations used for our scal-\ning studies in Table 1, the 2.5B model is the same, the 8.3B\nmodel has 24 attention heads instead of 32, and the 355M is\nmuch smaller than any seen previously while still using 64\nGPUs to train, leading to the much lower time per epoch.\nFigure 6 shows validation perpelixity as a function of num-\nber of iterations. As the model size increases, the validation\nperpelixity decreases and reaches a validation perplexity of\n9.27 for the 8.3B model. We report the zero-shot evaluation\nof the trained models on the ']"," LLaMA incorporates pre-normalization (using RMSNorm), the SwiGLU activation function, and rotary embeddings (RoPE) into its transformer architecture for enhanced training stability and performance.",None,1.0,0.0,0.04832153767347336
"How does LLaMA's approach to embedding layer optimization differ from traditional transformer models, and what are the specific benefits of these modifications?","['use the RMSNorm normalizing func-\ntion, introduced by Zhang and Sennrich (2019).\nSwiGLU activation function [PaLM]. We re-\nplace the ReLU non-linearity by the SwiGLU ac-\ntivation function, introduced by Shazeer (2020) to\nimprove the performance. We use a dimension of\n2\n34dinstead of 4das in PaLM.\nRotary Embeddings [GPTNeo]. We remove the\nabsolute positional embeddings, and instead, add\nrotary positional embeddings (RoPE), introduced\nby Su et al. (2021), at each layer of the network.\nThe details of the hyper-parameters for our dif-\nferent models are given in Table 2.\n2.3 Optimizer\nOur models are trained using the AdamW opti-\nmizer (Loshchilov and Hutter, 2017), with the fol-\nlowing hyper-parameters: β1= 0.9,β2= 0.95.\nWe use a cosine learning rate schedule, such that\nthe ﬁnal learning rate is equal to 10% of the maxi-\nmal learning rate. We use a weight decay of 0.1and\ngradient clipping of 1.0. We use 2,000warmup\n0 200 400 600 800 1000 1200 1400\nBillion of tokens1.51.61.71.81.92.02.12.2Training lossLLaMA 7B\nLLaMA 13B\nLLaMA 33B\nLLaMA 65BFigure 1: Training loss over train tokens for the 7B,\n13B, 33B, and 65 models. LLaMA-33B and LLaMA-\n65B were trained on 1.4T tokens. The smaller models\nwere trained on 1.0T tokens. All models are trained\nwith a batch size of 4M tokens.\nsteps, and vary the learning rate and batch size with\nthe size of the model (see Table 2 for details).\n2.4 Efﬁcient implementation\nWe make several optimizations to improve the train-\ning speed of our models. First, we use an efﬁcient\nimplementation of the causal multi-head attention\nto reduce memory usage and runtime. This imple-\nmentation, available in the xformers library,2is\ninspired by Rabe and Staats (2021) and uses the\nbackward from Dao et al. (2022). This is achieved\nby not storing the attention weights and ', 'the patch projection layer. Unfortunately,\nwe cannot freeze the training of the embedding layer in language models.\nFinally, we find the gradient shrink on embedding layers could overcome loss spikes and thus sta-\nbilize GLM-130B’s training. It is first used in the multi-modal transformer CogView (Ding et al.,\n2021). Let αbe the shrinking factor, the strategy can be easily implemented via word _embedding =\nword _embedding ∗α+word _embedding .detach ()∗(1−α). Figure 4 (b) suggests that empirically,\nsetting α= 0.1wipes out most spikes we would have met, with negligible latency.\nIn fact, the final GLM-130B training run only experiences three late-stage loss divergence cases,\nthough it fails numerous times due to hardware failures. For the three unexpected spikes, it turns out\nfurther shrinking the embedding gradient can still help stabilize the GLM-130B training. See the\ntraining notes and Tensorboard logs in our code repository for details.\n4 GLM-130B I NFERENCE ON RTX 2080 T I\nOne of the major goals of GLM-130B is to lower the hardware requirements for accessing 100B-\nscale LLMs without efficiency and effectiveness disadvantages.\nAs mentioned, the model size of 130B is determined for running the full GLM-130B model on a sin-\ngle A100 (40G ×8) server, rather than the high-end A100 (80G ×8) machine required by OPT-175B\nand BLOOM-176B. To accelerate GLM-130B inference, we also leverage FasterTransformer (Ti-\nmonin et al., 2022) to implement GLM-130B in C++. Compared to the PyTorch implementation\nof BLOOM-176B in Huggingface, GLM-130B’s decoding inference is 7-8.4 ×faster on the same\nsingle A100 server. (Cf. Appendix B.5 for details).\nINT4 Quantization for RTX 3090s/2080s. To further support popularized GPUs, we attempt to\ncompress GLM-130B as much as possible while ', 'is also largely impacted\nby the number of tokens it passes through (Hoffmann et al., 2022). Thus, given the computing\nusage constraint, there has to be a trade-off between efficiency and stability with regard to floating-\npoint (FP) formats: low-precision FP formats (e.g., 16-bit precision—FP16) improve computing\nefficiency but are prone to overflow and underflow errors, resulting in training collapses.\n(a) Gradient norm with EGS α = 0.1\n(b) EGS in 40B-scale testing\nFigure 4: EGS reduces gradi-\nent scale and variance to stabi-\nlize LLMs’ pre-training.Mixed-Precision. We follow the common practice of a mixed-\nprecision (Micikevicius et al., 2018) strategy (Apex O2), i.e., FP16\nfor forwards and backwards and FP32 for optimizer states and mas-\nter weights, to reduce the GPU memory usage and improve train-\ning efficiency. Similar to OPT-175B and BLOOM-176B (C.f. Fig-\nure 10 in Appendix), the training of GLM-130B faces frequent loss\nspikes resulted from this choice, which tends to become increas-\ningly frequent as the training goes on. The precision related spikes\nare often without clear reasons: some recover on their own; others\ncome with a portent of suddenly soaring gradient norm and even-\ntually a spike or even NaN in loss. OPT-175B attempted to fix by\nmanually skipping data and adjusting hyper-parameters; BLOOM-\n176B did so via the embedding norm technique (Dettmers et al.,\n2021). We spent months to empirically investigate the spikes and\nrealize that a few issues emerge when transformers scale up:\nFirst, the transformer main branch’s value scale can be extremely\nlarge in deeper layers if using Pre-LN. This is addressed in GLM-\n130B by using DeepNorm based Post-LN (Cf. Section 2.1), which\nmakes the value scale always bounded.\nSecond, the attention scores grow ']","LLaMA introduces optimizations in its embedding layer through techniques like factorized embeddings and the use of advanced activation functions, such as SwiGLU, for efficient computation and memory usage. These modifications benefit the model by enhancing learning efficiency and enabling the scaling of model parameters without a proportional increase in computational demand.","LLaMA's approach to embedding layer optimization differs from traditional transformer models by using a gradient shrink strategy on the embedding layers. This strategy involves multiplying the embedding by a shrinking factor alpha and adding the product to the detached embedding multiplied by (1-alpha). This approach helps overcome loss spikes and stabilizes the training of GLM-130B. 

The specific benefits of these modifications include:
1. Stabilizing the training process: By shrinking the gradient on embedding layers, LLaMA can overcome loss spikes and stabilize the training process of large-scale language models like GLM-130B.
2. Decreasing the number of unexpected spikes: Setting alpha to 0.1 can significantly reduce the number of unexpected spikes during training, leading to more consistent performance.
3. Improving efficiency: The gradient shrink strategy helps stabilize GLM-130B training, allowing for more efficient training runs with fewer interruptions due to loss spikes.",1.0,1.0,0.7973804473876953
"How were the questions for the multitask test sourced, and what was the criteria for their inclusion?","['and undergraduate students from freely available\nsources online. These include practice questions for tests such as the Graduate Record Examination\nand the United States Medical Licensing Examination. It also includes questions designed for\nundergraduate courses and questions designed for readers of Oxford University Press books. Some\ntasks cover a subject, like psychology, but at a speciﬁc level of difﬁculty, such as “Elementary,”\n“High School,” “College,” or “Professional.” For example, the “Professional Psychology” task draws\non questions from freely available practice questions for the Examination for Professional Practice\nin Psychology, while the “High School Psychology” task has questions like those from Advanced\nPlacement Psychology examinations.\nWe collected 15908 questions in total, which we split into a few-shot development set, a validation\nset, and a test set. The few-shot development set has 5questions per subject, the validation set may\nbe used for selecting hyperparameters and is made of 1540 questions, and the test set has 14079\nquestions. Each subject contains 100test examples at the minimum, which is longer than most exams\ndesigned to assess people.\nHuman-level accuracy on this test varies. Unspecialized humans from Amazon Mechanical Turk\nobtain 34.5%accuracy on this test. Meanwhile, expert-level performance can be far higher. For\nexample, real-world test-taker human accuracy at the 95th percentile is around 87% for US Medical\nLicensing Examinations, and these questions make up our “Professional Medicine” task. If we take\nthe 95th percentile human test-taker accuracy for exams that build up our test, and if we make an\neducated guess when such information is unavailable, we then estimate that expert-level accuracy is\napproximately 89.8%.\nSince our test aggregates different subjects and several levels of difﬁculty, we measure more than\nstraightforward commonsense ', 'for humans, we exclude this task from our benchmark.\nInstructions tables begin on the following page.\n10https://www.kaggle.com/c/quora-insincere-questions-classification/data\n18\x0cTable 5: The instructions given to crowd-sourced worker describing the training phase for the Choice\nof Plausible Answers (COPA) task.\nThe New York University Center for Data Science is collecting your answers for use in research\non computer understanding of English. Thank you for your help!\nThis project is a training task that needs to be completed before working on the main project\non AMT named Human Performance: Plausible Answer. Once you are done with the training,\nplease proceed to the main task! The qualiﬁcation approval is not immediate but we will add\nyou to our qualiﬁed workers list within a day.\nIn this training, you must answer the question on the page and then, to see how you did, click\ntheCheck Work button at the bottom of the page before hitting Submit. The Check Work\nbutton will reveal the true label. Please use this training and the provided answers to build\nan understanding of what the answers to these questions look like (the main project, Human\nPerformance: Plausible Answer, does not have the answers on the page).\nTable 6: Task-speciﬁc instructions for Choice of Plausible Alternatives (COPA). These instructions\nwere provided during both training and annotation phases.\nPlausible Answer Instructions\nThe New York University Center for Data Science is collecting your answers for use in research\non computer understanding of English. Thank you for your help!\nWe will present you with a prompt sentence and a question. The question will either be about\nwhat caused the situation described in the prompt, or what a possible effect of that situation is.\nWe ', '57tasks. On the right are UniﬁedQA\ntransfer accuracies for all of the 57tasks. For both models, capabilities are lopsided.\nA.1 A NALYSIS WITH MORE FINE-TUNED MODELS\nWe primarily analyzed models with more than 10billion parameters in the main body of the paper.\nFor this section, we analyze smaller models including RoBERTa-base (125 million parameters) (Liu\n11\x0cPublished as a conference paper at ICLR 2021\net al., 2019), ALBERT-xxlarge (223 million parameters) (Lan et al., 2020), and GPT-2 (1,558 million\nparameters) (Radford et al., 2019). Models are ﬁne-tuned to predict one of four classes using the\nUniﬁedQA MCQ questions and using our dev+val set. We test on our multitask test set.\nWe observe that these smaller models can attain better-than-random accuracy. RoBERTa-base attains\nan overall accuracy of 27.9%, with 27.9%accuracy for the humanities, 28.8%for social sciences,\n27.0%for STEM, and 27.7%for other. ALBERT-xxlarge attains an accuracy of 27.1%, with 27.2%\naccuracy for the humanities, 25.7%for the social sciences, 27.7%for STEM, and 27.9%for other.\nGPT-2 attains an accuracy of 32.4%, with 32.8%accuracy for the humanities, 33.3%for the social\nsciences, 30.2%for STEM, and 33.1%for other.\nCompare this to UniﬁedQA’s smallest variant, which has just 60million parameters and approximately\n29.3%accuracy. It obtains higher accuracy than RoBERTa and ALBERT, even though it has fewer\nparameters. This suggests that its larger pretraining dataset enables higher accuracy. Likewise,\nUniﬁedQA with 3billion parameters attains 43.7%, while the similarly sized GPT-2 model with 1.5\nbillion parameters attains 32.4%accuracy. This again suggests that T5’s larger pretraining dataset\nsize (and therefore UniﬁedQA’s pretraining dataset size) can increase accuracy.\nA.2 E RROR ANALYSIS\nWe qualitatively analyze when GPT-3 makes high conﬁdence mistakes. We ﬁnd that while many of\nthese mistakes were clearly wrong, ']","Questions were manually collected by graduate and undergraduate students from freely available online sources, including practice questions for standardized tests and undergraduate courses, ensuring a wide representation of difficulty levels and subjects.","The questions for the multitask test were collected from freely available sources online, including practice questions for tests such as the Graduate Record Examination and the United States Medical Licensing Examination. They also included questions designed for undergraduate courses and readers of Oxford University Press books. The criteria for their inclusion was based on covering different subjects and several levels of difficulty, such as ""Elementary,"" ""High School,"" ""College,"" or ""Professional."" Each subject had a minimum of 100 test examples.",0.6666666666666666,1.0,0.6749917268753052
How does BERT's performance on the GLUE benchmark compare to previous state-of-the-art models?,"['56.0 75.1\nBERT BASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\nBERT LARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\nTable 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard ).\nThe number below each task denotes the number of training examples. The “Average” column is slightly different\nthan the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8BERT and OpenAI GPT are single-\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\nWe use a batch size of 32 and ﬁne-tune for 3\nepochs over the data for all GLUE tasks. For each\ntask, we selected the best ﬁne-tuning learning rate\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\nAdditionally, for BERT LARGE we found that ﬁne-\ntuning was sometimes unstable on small datasets,\nso we ran several random restarts and selected the\nbest model on the Dev set. With random restarts,\nwe use the same pre-trained checkpoint but per-\nform different ﬁne-tuning data shufﬂing and clas-\nsiﬁer layer initialization.9\nResults are presented in Table 1. Both\nBERT BASE and BERT LARGE outperform all sys-\ntems on all tasks by a substantial margin, obtaining\n4.5% and 7.0% respective average accuracy im-\nprovement over the prior state of the art. Note that\nBERT BASE and OpenAI GPT are nearly identical\nin terms of model architecture apart from the at-\ntention masking. For the largest and most widely\nreported GLUE task, MNLI, BERT obtains a 4.6%\nabsolute accuracy improvement. On the ofﬁcial\nGLUE leaderboard10, BERT LARGE obtains a score\nof ', '65.1 0.0 100.0/ 50.0\nCBoW 44.3 62.1 49.0/71.2 51.6 0.0 / 0.4 14.0/13.6 49.7 53.0 65.1 -0.4 100.0/ 50.0\nBERT 69.0 77.4 75.7/83.6 70.6 70.0 / 24.0 72.0/71.3 71.6 69.5 64.3 23.0 97.8 / 51.7\nBERT++ 71.5 79.0 84.7/90.4 73.8 70.0 / 24.1 72.0/71.3 79.0 69.5 64.3 38.0 99.4 / 51.4\nOutside Best - 80.4 - / - 84.4 70.4 */24.5*74.8/73.0 82.7 - - - - / -\nHuman (est.) 89.8 89.0 95.8/98.9 100.0 81.8*/51.9* 91.7/91.3 93.6 80.0 100.0 77.0 99.3 / 99.7\n5 Experiments\n5.1 Baselines\nBERT Our main baselines are built around BERT, variants of which are among the most successful\napproach on GLUE at the time of writing. Speciﬁcally, we use the bert-large-cased variant.\nFollowing the practice recommended in Devlin et al. (2019), for each task, we use the simplest\npossible architecture on top of BERT. We ﬁne-tune a copy of the pretrained BERT model separately\nfor each task, and leave the development of multi-task learning models to future work. For training,\nwe use the procedure speciﬁed in Devlin et al. (2019): We use Adam (Kingma and Ba, 2014) with an\ninitial learning rate of 10−5and ﬁne-tune for a maximum of 10 epochs.\nFor classiﬁcation tasks with sentence-pair inputs (BoolQ, CB, RTE, WiC), we concatenate the\nsentences with a [SEP]token, feed the fused input to BERT, and use a logistic regression classiﬁer\nthat sees the representation corresponding to [CLS]. For WiC, we also concatenate the representation\nof the marked word. For COPA, MultiRC, and ReCoRD, for each answer choice, we similarly\nconcatenate the context with that answer choice and feed the resulting sequence into BERT to produce\nan answer representation. For COPA, we ', 'target metric, and an accompanying expert-\nconstructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\nunderstanding that covers a range of training data volumes, task genres, and task formulations. We\nbelieve it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-\nlearning potential of approaches like OpenAI GPT and BERT.\nThe progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\nWhile some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\nin GLUE remain difﬁcult, the current state of the art GLUE Score as of early July 2019 (88.4 from\nYang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\npoints, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\n∗Equal contribution. Correspondence: glue-benchmark-admin@googlegroups.com\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1905.00537v3 [cs.CL] 13 Feb 2020\x0cBiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTs\nBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)0.50.60.70.80.91.01.11.2\nGLUE Score\nHuman Performance\nCoLA\nSST-2MRPC\nSTS-B\nQQP\nMNLIQNLI\nRTE\nWNLIFigure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\nto 1.0, shown as a single number score, and broken down into the nine constituent task performances.\nFor tasks with multiple metrics, we use an average of the metrics. More information on the tasks\nincluded in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2019, CoLA), Socher\net al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\n(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\nremains substantial scope for improvement towards GLUE’s ']","BERT achieved new state-of-the-art on the GLUE benchmark (80.5%), surpassing the previous best models.",BERT's performance on the GLUE benchmark outperforms all previous state-of-the-art models by obtaining a 4.5% average accuracy improvement for BERT BASE and a 7.0% average accuracy improvement for BERT LARGE.,1.0,1.0,0.8464086055755615
"What significant improvements does BERT bring to the SQuAD v1.1,v2.0 and v13.5 tasks compared to prior models?","['Peters et al.,\n2018a; Hu et al., 2018). The top results from the\nSQuAD leaderboard do not have up-to-date public\nsystem descriptions available,11and are allowed to\nuse any public data when training their systems.\nWe therefore use modest data augmentation in\nour system by ﬁrst ﬁne-tuning on TriviaQA (Joshi\net al., 2017) befor ﬁne-tuning on SQuAD.\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA ﬁne-\n11QANet is described in Yu et al. (2018), but the system\nhas improved substantially after publication.\x0cSystem Dev Test\nEM F1 EM F1\nTop Leaderboard Systems (Dec 10th, 2018)\nHuman - - 82.3 91.2\n#1 Ensemble - nlnet - - 86.0 91.7\n#2 Ensemble - QANet - - 84.5 90.5\nPublished\nBiDAF+ELMo (Single) - 85.6 - 85.8\nR.M. Reader (Ensemble) 81.2 87.9 82.3 88.5\nOurs\nBERT BASE (Single) 80.8 88.5 - -\nBERT LARGE (Single) 84.1 90.9 - -\nBERT LARGE (Ensemble) 85.8 91.8 - -\nBERT LARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8\nBERT LARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2\nTable 2: SQuAD 1.1 results. The BERT ensemble\nis 7x systems which use different pre-training check-\npoints and ﬁne-tuning seeds.\nSystem Dev Test\nEM F1 EM F1\nTop Leaderboard Systems (Dec 10th, 2018)\nHuman 86.3 89.0 86.9 89.5\n#1 Single - MIR-MRC (F-Net) - - 74.8 78.0\n#2 Single - nlnet - - 74.2 77.1\nPublished\nunet (Ensemble) - - 71.4 74.9\nSLQA+ (Single) - 71.4 74.4\nOurs\nBERT LARGE (Single) 78.7 81.9 80.0 83.1\nTable 3: SQuAD 2.0 results. We exclude entries that\nuse BERT as one of their components.\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a ', '56.0 75.1\nBERT BASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\nBERT LARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\nTable 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard ).\nThe number below each task denotes the number of training examples. The “Average” column is slightly different\nthan the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8BERT and OpenAI GPT are single-\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\nWe use a batch size of 32 and ﬁne-tune for 3\nepochs over the data for all GLUE tasks. For each\ntask, we selected the best ﬁne-tuning learning rate\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\nAdditionally, for BERT LARGE we found that ﬁne-\ntuning was sometimes unstable on small datasets,\nso we ran several random restarts and selected the\nbest model on the Dev set. With random restarts,\nwe use the same pre-trained checkpoint but per-\nform different ﬁne-tuning data shufﬂing and clas-\nsiﬁer layer initialization.9\nResults are presented in Table 1. Both\nBERT BASE and BERT LARGE outperform all sys-\ntems on all tasks by a substantial margin, obtaining\n4.5% and 7.0% respective average accuracy im-\nprovement over the prior state of the art. Note that\nBERT BASE and OpenAI GPT are nearly identical\nin terms of model architecture apart from the at-\ntention masking. For the largest and most widely\nreported GLUE task, MNLI, BERT obtains a 4.6%\nabsolute accuracy improvement. On the ofﬁcial\nGLUE leaderboard10, BERT LARGE obtains a score\nof ', 'wide margin.12\n4.3 SQuAD v2.0\nThe SQuAD 2.0 task extends the SQuAD 1.1\nproblem deﬁnition by allowing for the possibility\nthat no short answer exists in the provided para-\ngraph, making the problem more realistic.\nWe use a simple approach to extend the SQuAD\nv1.1 BERT model for this task. We treat ques-\ntions that do not have an answer as having an an-\nswer span with start and end at the [CLS] to-\nken. The probability space for the start and end\nanswer span positions is extended to include the\nposition of the [CLS] token. For prediction, we\ncompare the score of the no-answer span: snull=\nS·C+E·Cto the score of the best non-null span\n12The TriviaQA data we used consists of paragraphs from\nTriviaQA-Wiki formed of the ﬁrst 400 tokens in documents,\nthat contain at least one of the provided possible answers.System Dev Test\nESIM+GloVe 51.9 52.7\nESIM+ELMo 59.1 59.2\nOpenAI GPT - 78.0\nBERT BASE 81.6 -\nBERT LARGE 86.6 86.3\nHuman (expert)†- 85.0\nHuman (5 annotations)†- 88.0\nTable 4: SWAG Dev and Test accuracies.†Human per-\nformance is measured with 100 samples, as reported in\nthe SWAG paper.\nˆsi,j=maxj≥iS·Ti+E·Tj. We predict a non-null\nanswer when ˆsi,j> s null+τ, where the thresh-\noldτis selected on the dev set to maximize F1.\nWe did not use TriviaQA data for this model. We\nﬁne-tuned for 2 epochs with a learning rate of 5e-5\nand a batch size of 48.\nThe results compared to prior leaderboard en-\ntries and top published work (Sun et al., 2018;\nWang et al., 2018b) are shown in Table 3, exclud-\ning systems that use BERT as one of their com-\nponents. We observe a +5.1 F1 improvement over\nthe previous best system.\n4.4 SWAG\nThe Situations With Adversarial Generations\n(SWAG) dataset contains 113k sentence-pair ']","BERT set new records on SQuAD v1.1 and v2.0, significantly outperforming the top leaderboard systems at the time. Version 13.5 doesn't exist. ","BERT brings substantial improvements to the SQuAD v1.1, v2.0, and v1.3.5 tasks compared to prior models, outperforming all existing systems on all tasks by a wide margin. In SQuAD v1.1, BERT achieves a significant improvement in F1 score for both single and ensemble systems. In SQuAD v2.0, BERT extends the model to handle questions without a short answer, leading to a significant improvement in F1 score compared to previous systems. Additionally, in the GLUE benchmark task v1.3.5, BERT BASE and BERT LARGE outperform all systems with a substantial average accuracy improvement over the prior state of the art.",1.0,1.0,0.45715174078941345
"What unique aspect of the LLaMA training dataset distinguishes it from datasets used by models like GPT-3, Chinchilla, and PaLM?","['the ability of the model to\nfollow instructions. Since this is not the focus of\nthis paper, we only conducted a single experiment\nfollowing the same protocol as Chung et al. (2022)\nto train an instruct model, LLaMA-I.\nOPT 30B 26.1\nGLM 120B 44.8\nPaLM 62B 55.1\nPaLM-cont 62B 62.8\nChinchilla 70B 67.5\nLLaMA 65B 63.4\nOPT-IML-Max 30B 43.2\nFlan-T5-XXL 11B 55.1\nFlan-PaLM 62B 59.6\nFlan-PaLM-cont 62B 66.1\nLLaMA-I 65B 68.9\nTable 10: Instruction ﬁnetuning – MMLU (5-shot).\nComparison of models of moderate size with and with-\nout instruction ﬁnetuning on MMLU.In Table 10, we report the results of our instruct\nmodel LLaMA-I on MMLU and compare with ex-\nisting instruction ﬁnetuned models of moderate\nsizes, namely, OPT-IML (Iyer et al., 2022) and the\nFlan-PaLM series (Chung et al., 2022). All the re-\nported numbers are from the corresponding papers.\nDespite the simplicity of the instruction ﬁnetuning\napproach used here, we reach 68.9% on MMLU.\nLLaMA-I (65B) outperforms on MMLU existing\ninstruction ﬁnetuned models of moderate sizes, but\nare still far from the state-of-the-art, that is 77.4\nfor GPT code-davinci-002 on MMLU (numbers\ntaken from Iyer et al. (2022)). The details of the\nperformance on MMLU on the 57 tasks can be\nfound in Table 16 of the appendix.\n5 Bias, Toxicity and Misinformation\nLarge language models have been showed to re-\nproduce and amplify biases that are existing in\nthe training data (Sheng et al., 2019; Kurita et al.,\n2019), and to generate toxic or offensive con-\ntent (Gehman et al., 2020). As our training dataset\ncontains a large proportion of data from the Web,\nwe believe that it is crucial to determine the po-\ntential for our models to generate such content.\nTo understand the potential harm of LLaMA-65B,\nwe evaluate on different benchmarks that measure\ntoxic content ', 'evaluation and ethical studies.\nTable 1: A comparison between GLM-130B and other 100B-scale LLMs and PaLM 540B. (LN:\nlayer norm.; FPF: floating-point format; MIP: multi-task instruction pre-training; CN : Chinese)\nArchitecture & Data Training Inference\nModelOpen-\nsource Objective LN Major Lang. FPF Stabilization Quantization GPU Needed\nGPT-3 175B × English FP16 undisclosed undisclosed undisclosed\nOPT-175B ✓ English FP16 Manual Adjusting INT8 8 ×3090\nBLOOM-176B ✓GPT Pre-LN\nMulti-lingual BF16 Embedding Norm INT8 8 ×3090\nPaLM 540B × GPT Pre-LN English BF16 Manual Adjusting undisclosed undisclosed\nGLM-130B ✓GLM (Blank\nInfilling & MIP)Deep-\nNormBilingual\n(EN & CN)FP16Embedding\nGradient ShrinkINT44×3090 or\n8×1080 Ti\nIn this work, we introduce the pre-training of a 100B-scale model—GLM-130B, in terms of engi-\nneering efforts, model design choices, training strategies for efficiency and stability, and quantization\nfor affordable inference. As it has been widely realized that it is computationally unaffordable to\nempirically enumerate all possible designs for training 100B-scale LLMs, we present not only the\nsuccessful part for training GLM-130B but also many of the failed options and lessons learned.\nParticularly, the training stability is the decisive factor in the success of training models of such a\nscale. Different from practices such as manually adjusting learning rates in OPT-175B and using\nembedding norm in the sacrifice of performance in BLOOM-176B, we experiment with various op-\ntions and find the strategy of embedding gradient shrink can significantly stabilize the training of\nGLM-130B.\nSpecifically, GLM-130B is a bilingual (English and Chinese) bidirectional dense model with 130 bil-\nlion parameters, pre-trained over 400 billion tokens on a cluster of 96 NVIDIA DGX-A100 (8 ×40G)\nGPU nodes between May 6 and July 3, 2022. Instead of using the GPT-style architecture, we adopt\nthe General Language Model (GLM) algorithm (Du ', 'fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of\n∗Equal contribution. Correspondence: {htouvron,\nthibautlav,gizacard,egrave,glample}@meta.com\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al. (2022) recommends\ntraining a 10B model on 200B tokens, we ﬁnd\nthat the performance of a 7B model continues to\nimprove even after 1T tokens.\nThe focus of this work is to train a series of\nlanguage models that achieve the best possible per-\nformance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA , ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 ×smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modiﬁcations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We ']"," LLaMA's training dataset is distinctive for being composed exclusively of publicly available data, unlike the datasets used for GPT-3, Chinchilla, and PaLM, which include proprietary or undocumented data.","LLaMA training dataset only uses publicly available data, making it compatible with open-sourcing, while models like GPT-3, Chinchilla, and PaLM rely on data which is either not publicly available or undocumented.",1.0,1.0,0.8414031863212585
"What detailed methodology does LLaMA utilize to ensure the diversity of its pre-training data, particularly in the context of filtering and language identification?","['suggests that high-quality language data is\nlikelyexhaustedbefore2026,andlow-qualitylanguageand\nimage data could be run out by 2060. This implies that the\nlimited progress of data collection and construction could\nbe constraints of future LLM development. Furthermore,\nas better-quality data is assumed to train language models\nwith better performances, companies and independent\nresearchers are spending more time on data curation.However, this can not be done easily under the low-\nresource and low-budget scenarios. Even if we pay much\neffort to design comprehensive human annotation frame-\nworks,thedatacouldstillcontaininaccurateormisleading\ninformation due to the natural biases in crowdsourcing.\nIn fact, we notice that prior constructed datasets have\nexperienced multiple rounds of filtering across time [89].\nOn the other hand, current findings suggest that the usage\nof data for language models may not be optimized [90].\nSpecifically, recent works on data deduplication and re-\nduction [91, 92] have shown that data in high quality by\nlow quantity can improve the model performance. Besides,\nwe consider the design of training data as a crucial factor\nto the efficient data usage. For example, experiments show\nthat curriculum learning [93], active learning [94] and\nprompting [95] could improve the data efficiency. However,\nmostofthesestrategiesarestillattheearlystageandneed\nthe further investigation.\nc) Computational Resource: As LLMs are growing\nbigger and bigger, the deployment and training of these\nmodels are getting more and more costly. Daily prac-\ntitioners in NLP and deep learning will find it hard\nto install the LLMs on their own devices. Previous\nstudy [96] also show that the computational resource\nrequirements for strong model scaling clearly outpaces\nthat of system hardware. We argue that model scaling\nmay be inevitable, which is determined by the scaling law.\nHowever, recent attempts among model design, tuning\nstrategy and compression could possibly mitigate the\nextreme consumption of the computational ', '(Radford et al., 2019),\n10B-scale language models (Raffel et al., 2020; Shoeybi et al., 2019; Black et al., 2022), to 100B-\nscale GPT-3 (Brown et al., 2020). Later, despite many 100B-scale LLMs (Lieber et al., 2021; Thop-\npilan et al., 2022; Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022; Wu et al., 2021; Zeng\net al., 2021; Wang et al., 2021) in both English and Chinese, they are not available to public or only\naccessible via limited APIs. The closeness of LLMs severely stymies its development. GLM-130B’s\nefforts, along with recent ElutherAI, OPT-175B (Zhang et al., 2022), and BLOOM-176B (Scao et al.,\n2022), aim to offer high-quality open-sourced LLMs to our community.\nTransferring. Though fine-tuning has been a de facto way for transfer learning, the evaluation for\nLLMs has been focused on prompting and in-context learning due to their tremendous sizes (Brown\net al., 2020; Liu et al., 2021a). Nevertheless, some recent attempts has been on parameter-efficient\nlearning on language models (Houlsby et al., 2019) and prompt tuning (i.e., P-tuning, Li & Liang\n(2021); Liu et al. (2021b); Lester et al. (2021); Liu et al. (2022)). For now we do not focus on them\nand will leave the comprehensive testing of them on GLM-130B in future study.\nInference. Most public-accessible LLMs nowadays are providing their services via limited APIs.In\nthis work, an important part of our endeavor has been on LLMs’ efficient and fast inference. Related\nwork may include distillation (Sanh et al., 2019; Jiao et al., 2020; Wang et al., 2020), quantiza-\ntion (Zafrir et al., 2019; Shen et al., 2020; Tao et al., 2022), and pruning (Michel ', 'fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of\n∗Equal contribution. Correspondence: {htouvron,\nthibautlav,gizacard,egrave,glample}@meta.com\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al. (2022) recommends\ntraining a 10B model on 200B tokens, we ﬁnd\nthat the performance of a 7B model continues to\nimprove even after 1T tokens.\nThe focus of this work is to train a series of\nlanguage models that achieve the best possible per-\nformance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA , ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 ×smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modiﬁcations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We ']","LLaMA's methodology for ensuring data diversity includes comprehensive filtering criteria to remove low-quality content, language identification to support multilingual capabilities, and a balanced inclusion of various data sources such as Wikipedia, web text, and books, emphasizing the representation of a wide range of topics and languages.",None,1.0,0.0,0.12706221640110016
"What are the specific domains covered by the multitask test, and why were they selected?","['57tasks. On the right are UniﬁedQA\ntransfer accuracies for all of the 57tasks. For both models, capabilities are lopsided.\nA.1 A NALYSIS WITH MORE FINE-TUNED MODELS\nWe primarily analyzed models with more than 10billion parameters in the main body of the paper.\nFor this section, we analyze smaller models including RoBERTa-base (125 million parameters) (Liu\n11\x0cPublished as a conference paper at ICLR 2021\net al., 2019), ALBERT-xxlarge (223 million parameters) (Lan et al., 2020), and GPT-2 (1,558 million\nparameters) (Radford et al., 2019). Models are ﬁne-tuned to predict one of four classes using the\nUniﬁedQA MCQ questions and using our dev+val set. We test on our multitask test set.\nWe observe that these smaller models can attain better-than-random accuracy. RoBERTa-base attains\nan overall accuracy of 27.9%, with 27.9%accuracy for the humanities, 28.8%for social sciences,\n27.0%for STEM, and 27.7%for other. ALBERT-xxlarge attains an accuracy of 27.1%, with 27.2%\naccuracy for the humanities, 25.7%for the social sciences, 27.7%for STEM, and 27.9%for other.\nGPT-2 attains an accuracy of 32.4%, with 32.8%accuracy for the humanities, 33.3%for the social\nsciences, 30.2%for STEM, and 33.1%for other.\nCompare this to UniﬁedQA’s smallest variant, which has just 60million parameters and approximately\n29.3%accuracy. It obtains higher accuracy than RoBERTa and ALBERT, even though it has fewer\nparameters. This suggests that its larger pretraining dataset enables higher accuracy. Likewise,\nUniﬁedQA with 3billion parameters attains 43.7%, while the similarly sized GPT-2 model with 1.5\nbillion parameters attains 32.4%accuracy. This again suggests that T5’s larger pretraining dataset\nsize (and therefore UniﬁedQA’s pretraining dataset size) can increase accuracy.\nA.2 E RROR ANALYSIS\nWe qualitatively analyze when GPT-3 makes high conﬁdence mistakes. We ﬁnd that while many of\nthese mistakes were clearly wrong, ', 'these use different train/validation/test splits from other public\nversions in some cases. Systems also may not use the unlabeled test data for the tasks in system\ndevelopment in any way, may not use the structured source data that was used to collect the WiC\nlabels (sense-annotated example sentences from WordNet, VerbNet, and Wiktionary) in any way, and\nmay not build systems that share information across separate testexamples in any way.\nTo ensure reasonable credit assignment, because we build very directly on prior work, we ask the\nauthors of submitted systems to directly name and cite the speciﬁc datasets that they use, including the\nbenchmark datasets . We will enforce this as a requirement for papers to be listed on the leaderboard.\n4https://github.com/nyu-mll/jiant\n5https://github.com/huggingface/transformers\n7\x0cTable 3: Baseline performance on the SuperGLUE test sets and diagnostics. For CB we report\naccuracy and macro-average F1. For MultiRC we report F1 on all answer-options and exact match\nof each question’s set of correct answers. AX bis the broad-coverage diagnostic task, scored using\nMatthews’ correlation (MCC). AX gis the Winogender diagnostic, scored using accuracy and the\ngender parity score (GPS). All values are scaled by 100. The Avgcolumn is the overall benchmark\nscore on non-AX ∗tasks. The bolded numbers reﬂect the best machine performance on task. *MultiRC\nhas multiple test sets released on a staggered schedule, and these results evaluate on an installation of\nthe test set that is a subset of ours.\nModel Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC AX b AXg\nMetrics Acc. F1/Acc. Acc. F1 a/EM F1/EM Acc. Acc. Acc. MCC GPS Acc.\nMost Frequent 47.1 62.3 21.7/48.4 50.0 61.1 / 0.3 33.4/32.5 50.3 50.0 ', 'year, but recent models are already nearing human-level performance on several\nof these, including HellaSwag (Zellers et al., 2019), Physical IQA (Bisk et al., 2019), and CosmosQA\n(Huang et al., 2019). By design, these datasets assess abilities that almost every child has. In contrast,\nwe include harder specialized subjects that people must study to learn.\nSome researchers have suggested that the future of NLP evaluation should focus on Natural Language\nGeneration (NLG) (Zellers et al., 2020), an idea that reaches back to the Turing Test (Turing, 1950).\nHowever, NLG is notoriously difﬁcult to evaluate and lacks a standard metric (Sai et al., 2020).\nConsequently, we instead create a simple-to-evaluate test that measures classiﬁcation accuracy on\nmultiple choice questions.\nWhile several question answering benchmarks exist, they are comparatively limited in scope. Most\neither cover easy topics like grade school subjects for which models can already achieve strong\nperformance (Clark et al., 2018; Khot et al., 2019; Mihaylov et al., 2018; Clark et al., 2019), or\nare focused on linguistic understanding in the form of reading comprehension (Lai et al., 2017;\nRichardson et al., 2013). In contrast, we include a wide range of difﬁcult subjects that go far beyond\nlinguistic understanding.\n3 A M ULTITASK TEST\nWe create a massive multitask test consisting of multiple-choice questions from various branches of\nknowledge. The test spans subjects in the humanities, social sciences, hard sciences, and other areas\nthat are important for some people to learn. There are 57tasks in total, which is also the number\nof Atari games (Bellemare et al., 2013), all of which are listed in Appendix B. The questions in\nthe dataset were manually collected by graduate ']","The test covers 57 subjects across STEM, humanities, social sciences, and more, selected to evaluate a model's world knowledge and problem-solving ability across a wide range of difficulty levels, from elementary to advanced professional levels.","The specific domains covered by the multitask test are humanities, social sciences, hard sciences, and other areas that are important for some people to learn. These domains were selected because they go beyond linguistic understanding and cover a wide range of difficult subjects that require studying to learn.",1.0,1.0,0.5659512281417847
What specific enhancements are recommended for language models to bridge the knowledge application gap identified through comprehensive multitask testing?,"['In the following sections, we will\noutline some of the most pressing challenges that must\nbe addressed in order to facilitate further progress in the\ndevelopment of language models.\na) Emergent Ability: As described in the previous\nwork [87], emergent ability is defined as An ability is\nemergent if it is not present in smaller models but is\npresent in larger models.. From our diagnosis, we suc-\ncessfully identify a few unethical behaviors in ChatGPT\nthat were inadequately discussed in previous works, which\ncould be potentially be viewed as emergent risks. Kaplan\net al. [84] has confirmed that risks inside small language\nmodels can be further expanded in large ones due to\nthe model scales. On the basis of this finding, we add\nthat the model scales and the current trend of prompt-\ning training can exacerbate risks from all dimensions.\nThe main reason is that LLMs could be too feasible\nfrom the learning perspective. Firstly, these models are\nmore context-dependent, meaning that they are easily\nmanipulated by prompt injections. Although we agree\nthat some injected scenarios can be temporarily mitigated\nwith ad-hoc parameter tuning, there is no silver bullet to\navoid all risk concerns brought by prompting. Meanwhile,\nwe urge up-to-date benchmarks for measuring unfore-\nseen behaviors inside large language models. Without\nbenchmarking the emergent abilities, it could be hard to\nmitigate the risks and problems at scale. Secondly, we\nnote that larger language models are generally trained\nwith more data. Assuming the data is completely clean\nand informatively correct, language models will still fail to\nlearnallinformationandknowledge,andalsomaywrongly\ncorrelate information to each other. Furthermore, under\nthescopeofthefoundationmodels,multimodaldatacould\nbring the possibility of miscorrelation between different\nmodalities.\nb) Machine Learning Data: Our discussion lies in the\ncollection and usage of machine learning data. Previous\nstudy [88] ', 'Agrawal,\nM. Omernick, A. M. Dai, T. S. Pillai, M. Pellat,\nA. Lewkowycz, E. Moreira, R. Child, O. Polozov,K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz,\nO. Firat, M. Catasta, J. Wei, K. Meier-Hellstern,\nD. Eck, J. Dean, S. Petrov, and N. Fiedel, “Palm:\nScaling language modeling with pathways,” 2022.\n[Online]. Available: https://arxiv.org/abs/2204.02311\n[5] M. Lee, P. Liang, and Q. Yang, “Coauthor: Designing\nahuman-aicollaborativewritingdatasetforexploring\nlanguage model capabilities,” in Proceedings of the\n2022 CHI Conference on Human Factors in Comput-\ning Systems, 2022, pp. 1–19.\n[6] E. Gibson, R. Futrell, S. P. Piantadosi, I. Dautriche,\nK.Mahowald,L.Bergen,andR.Levy,“Howefficiency\nshapeshumanlanguage,” Trendsincognitivesciences,\nvol. 23, no. 5, pp. 389–407, 2019.\n[7] A. Liptak, “Amazon’s alexa started ordering people\ndollhouses after hearing its name on tv,” The Verge,\nvol. 7, 2017.\n[8] Google home. [Online]. Available: https://home.\ngoogle.com/\n[9] M. J. Wolf, K. Miller, and F. S. Grodzinsky, “Why\nwe should have seen that coming: comments on mi-\ncrosoft’s tay"" experiment,"" and wider implications,”\nAcm Sigcas Computers and Society, vol. 47, no. 3,\npp. 54–64, 2017.\n[10] N. Abdi, K. M. Ramokapane, and J. M. Such, “More\nthansmartspeakers:Securityandprivacyperceptions\nof smart home personal assistants.” in SOUPS@\nUSENIX Security Symposium, 2019.\n[11] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoff-\nmann, F. Song, J. Aslanides, S. Henderson, R. Ring,\nS. Young et al., “Scaling language models: Methods,\nanalysis & insights from training gopher,” arXiv\npreprint arXiv:2112.11446, 2021.\n[12] Z. Jin, G. Chauhan, B. Tse, M. Sachan, and R. Mi-\nhalcea, “How good is nlp? a sober look at nlp tasks\nthrough the lens of social impact,” in Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, 2021, pp. 3099–3113.\n[13] T. Schuster, R. Schuster, D. J. Shah, and R. Barzilay,\n“The limitations of stylometry for detecting machine-\ngenerated fake news,” Computational ', 'suggests that high-quality language data is\nlikelyexhaustedbefore2026,andlow-qualitylanguageand\nimage data could be run out by 2060. This implies that the\nlimited progress of data collection and construction could\nbe constraints of future LLM development. Furthermore,\nas better-quality data is assumed to train language models\nwith better performances, companies and independent\nresearchers are spending more time on data curation.However, this can not be done easily under the low-\nresource and low-budget scenarios. Even if we pay much\neffort to design comprehensive human annotation frame-\nworks,thedatacouldstillcontaininaccurateormisleading\ninformation due to the natural biases in crowdsourcing.\nIn fact, we notice that prior constructed datasets have\nexperienced multiple rounds of filtering across time [89].\nOn the other hand, current findings suggest that the usage\nof data for language models may not be optimized [90].\nSpecifically, recent works on data deduplication and re-\nduction [91, 92] have shown that data in high quality by\nlow quantity can improve the model performance. Besides,\nwe consider the design of training data as a crucial factor\nto the efficient data usage. For example, experiments show\nthat curriculum learning [93], active learning [94] and\nprompting [95] could improve the data efficiency. However,\nmostofthesestrategiesarestillattheearlystageandneed\nthe further investigation.\nc) Computational Resource: As LLMs are growing\nbigger and bigger, the deployment and training of these\nmodels are getting more and more costly. Daily prac-\ntitioners in NLP and deep learning will find it hard\nto install the LLMs on their own devices. Previous\nstudy [96] also show that the computational resource\nrequirements for strong model scaling clearly outpaces\nthat of system hardware. We argue that model scaling\nmay be inevitable, which is determined by the scaling law.\nHowever, recent attempts among model design, tuning\nstrategy and compression could possibly mitigate the\nextreme consumption of the computational ']","Enhancements should focus on developing models with improved procedural knowledge, superior calculation abilities, and a more accurate calibration between confidence and actual performance, directly addressing the weaknesses uncovered in current evaluations.",None,1.0,0.0,0.07118996977806091
What methodology does DetectGPT use to generate minor perturbations in the candidate passage for evaluation?,"['0.58 0.53 0.53 0.50 0.55 0.56 0.57 0.54 0.32 0.37 0.28 0.32 0.32 0.32\nDetectGPT 0.99 0.98 1.00 0.98 0.97 0.98 0.99 0.98 0.98 0.90 0.82* 0.94 1.00 0.99 0.99 0.97* 0.93 0.98\nDiff 0.04 0.04 0.04 0.05 0.08 0.05 0.01 0.02 0.04 0.00 -0.01 0.02 0.01 0.01 0.01 -0.01 -0.05 0.00\nTable 4. Nucleus (top- p) sampling evaluation with p= 0.96. AUROC for detecting samples from the given model on the given dataset for\nDetectGPT and four previously proposed criteria. Nucleus sampling generally makes detection easier for all methods, but DetectGPT still\nprovides the highest average AUROC. For WritingPrompts, however, the LogRank baseline performs as well as DetectGPT.\nXSum SQuAD WritingPrompts\nMethod GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg. GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg. GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg.\nlogp(x) 0.89 0.89 0.89 0.84 0.81 0.87 0.93 0.90 0.88 0.82 0.74 0.85 0.97 0.95 0.97 0.96 0.95* 0.96\nRank 0.79 0.77 0.77 0.75 0.73 0.76 0.84 0.82 0.80 0.80 0.75 0.80 0.87 0.84 0.83 0.82 0.81 0.83\nLogRank 0.92* 0.91* 0.93* 0.89* 0.85* 0.90* 0.96* 0.94* 0.92* 0.87* 0.79* 0.90* 0.98* 0.97* 0.98* 0.97 0.96 0.97\nEntropy 0.58 0.49 0.55 0.56 0.59 0.55 0.55 0.52 0.56 0.56 0.58 0.56 0.35 0.41 0.30 0.34 0.37 0.35\nDetectGPT 0.99 0.97 0.99 0.96 0.96 0.98 0.99 0.98 0.98 0.89 0.80 0.93 0.99 0.98 0.99 0.97 0.93 0.97\nDiff 0.07 0.06 0.06 0.07 0.11 0.08 0.03 0.04 0.06 0.02 0.01 0.03 0.01 0.01 0.01 0.00 -0.03 0.00\nTable 5. Top-ksampling evaluation with k= 40 . DetectGPT generally provides the most accurate performance (highest AUROC),\nalthough the gap is narrowed comparing to direct sampling, ', '8. Impact of varying the number of perturbations (samples\nof mask and mask-fill) used by DetectGPT on AUROC for GPT-2\n(left) and GPT-J ( right ) to estimate the perturbation discrepancy\non detection. Averaging up to 100 perturbations greatly increases\nDetectGPT’s reliability. Perturbations sampled from T5-large.\nlarly how the domain impacts the threshold separating the\nperturbation discrepancy distributions of model-generated\nand human texts as well as the impact of passage length on\ndetection. Figure 9 shows the perturbation discrepancy dis-\ntributions for model-generated and human texts across four\ndata distributions, using GPT-Neo-2.7B to generate sam-\nples. A threshold of slightly below 0.1 separates human and\nmodel texts across data distributions, which is important for\npractical scenarios in which a passage may be analyzed with-\nout knowing its domain a priori. Finally, Figure 10 shows an\nanalysis of DetectGPT’s performance as a function of pas-\nsage length. We bin the paired human- and model-generated\nsequences by their average length into three bins of equal\nsize (bottom/middle/top third), and plot the AUROC within\neach bin. The relationship between detection performance\nand passage length generally depends on the dataset and\nmodel (or tokenizer). For very long sequences, DetectGPT\nmay see reduced performance because our implementation\nof DetectGPT applies all T5 mask-filling perturbations at\nonce, and T5 may fail to track many mask tokens at once.\nBy applying perturbations in multiple sequential rounds of\nsmaller numbers of masks, this effect may be mitigated.\n6. Discussion\nAs large language models continue to improve, they will\nbecome increasingly attractive tools for replacing human\nwriters in a variety of contexts, such as education, jour-\nnalism, and art. While legitimate uses of language model\ntechnologies exist in all of these settings, teachers, readers,\nand consumers are likely ', 'mask-filling models in order to\ngenerate passages that are ‘nearby’ the candidate passage.\nHowever, these mask-filling models are used off-the-shelf,\nwithout any fine-tuning or adaptation to the target domain.\n4. DetectGPT: Zero-shot Machine-Generated\nText Detection with Random Perturbations\nDetectGPT is based on the hypothesis that samples from a\nsource model pθtypically lie in areas of negative curvature\nof the log probability function of pθ, unlike human text. In\nother words, if we apply small perturbations to a passage\nx∼pθ, producing ˜x, the quantity logpθ(x)−logpθ(˜x)\nshould be relatively large on average for machine-generated\nsamples compared to human-written text. To leverage this\nhypothesis, first consider a perturbation function q(· |x)\nthat gives a distribution over ˜x, slightly modified versions of\nxwith similar meaning (we will generally consider roughly\nparagraph-length texts x). As an example, q(· |x)might be\nthe result of simply asking a human to rewrite one of the\nsentences of x, while preserving the meaning of x. Using\nthe notion of a perturbation function, we can define the\nperturbation discrepancy d(x, pθ, q):\nd(x, pθ, q)≜logpθ(x)−E˜x∼q(·|x)logpθ(˜x)(1)\nWe state our hypothesis more formally as the Local Pertur-\nbation Discrepancy Gap Hypothesis, which describes a gap\nin the perturbation discrepancy for model-generated text\nand human-generated text.\nPerturbation Discrepancy Gap Hypothesis. Ifqproduces\nsamples on the data manifold, d(x, pθ, q)is positive and\nlarge with high probability for samples x∼pθ. For human-\nwritten text, d(x, pθ, q)tends toward zero for all x.\n3\x0cZero-Shot Machine-Generated Text Detection using Probability Curvature\n0.0 0.1 0.2 0.30204060gpt2-xl\nHuman\nModel\n0.0 0.1 0.2 0.3EleutherAI/gpt-neo-2.7B\n0.0 0.1 0.2 0.30204060EleutherAI/gpt-j-6B\n0.0 0.1 0.2 0.3EleutherAI/gpt-neox-20b\n0.0 0.2 0.4 0.6 0.8 1.0\nLog Probability Change (Perturbation Discrepancy)0.00.20.40.60.81.0Frequency\nFigure 3. The average drop in log probability (perturbation discrep-\nancy) after rephrasing a passage is consistently higher for model-\ngenerated passages than for ']","DetectGPT generates minor perturbations using a generic pre-trained model, such as T5, and then compares the log probability of the original sample with each perturbed sample.","DetectGPT uses a perturbation function q(· |x) that gives a distribution over slightly modified versions of x, slightly modified versions with similar meaning, to generate minor perturbations in the candidate passage for evaluation.",1.0,1.0,0.7776366472244263
Discuss the significance of DetectGPT's detection approach in the context of evolving LLM capabilities and the potential for misuse.,"['. . . . . . . . . . 52\nE.4 Project Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nE.5 Computation Sponsor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nF A Brief History of GLM-130B 53\nG Broader Impact 55\nG.1 Impact on AI Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\nG.2 Impact on Individual Developers and Small Companies . . . . . . . . . . . . . . 55\n20\x0cPublished as a conference paper at ICLR 2023\nG.3 Social Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\nH Environmental Impact 56\nA E THICS : EVALUATION ON BIASES AND TOXICITY\nAlbeit LLMs’ strong abilities in language and beyond, which could bring substantial welfare to\nhuman beings, they can potentially produce toxic and illegal contents for evil use (Weidinger et al.,\n2021; Sheng et al., 2021; Dev et al., 2021; Bommasani et al., 2021). In GLM-130B, before granting\nmodel weight to applicants, in the model license we demand them ', '0.58 0.53 0.53 0.50 0.55 0.56 0.57 0.54 0.32 0.37 0.28 0.32 0.32 0.32\nDetectGPT 0.99 0.98 1.00 0.98 0.97 0.98 0.99 0.98 0.98 0.90 0.82* 0.94 1.00 0.99 0.99 0.97* 0.93 0.98\nDiff 0.04 0.04 0.04 0.05 0.08 0.05 0.01 0.02 0.04 0.00 -0.01 0.02 0.01 0.01 0.01 -0.01 -0.05 0.00\nTable 4. Nucleus (top- p) sampling evaluation with p= 0.96. AUROC for detecting samples from the given model on the given dataset for\nDetectGPT and four previously proposed criteria. Nucleus sampling generally makes detection easier for all methods, but DetectGPT still\nprovides the highest average AUROC. For WritingPrompts, however, the LogRank baseline performs as well as DetectGPT.\nXSum SQuAD WritingPrompts\nMethod GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg. GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg. GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg.\nlogp(x) 0.89 0.89 0.89 0.84 0.81 0.87 0.93 0.90 0.88 0.82 0.74 0.85 0.97 0.95 0.97 0.96 0.95* 0.96\nRank 0.79 0.77 0.77 0.75 0.73 0.76 0.84 0.82 0.80 0.80 0.75 0.80 0.87 0.84 0.83 0.82 0.81 0.83\nLogRank 0.92* 0.91* 0.93* 0.89* 0.85* 0.90* 0.96* 0.94* 0.92* 0.87* 0.79* 0.90* 0.98* 0.97* 0.98* 0.97 0.96 0.97\nEntropy 0.58 0.49 0.55 0.56 0.59 0.55 0.55 0.52 0.56 0.56 0.58 0.56 0.35 0.41 0.30 0.34 0.37 0.35\nDetectGPT 0.99 0.97 0.99 0.96 0.96 0.98 0.99 0.98 0.98 0.89 0.80 0.93 0.99 0.98 0.99 0.97 0.93 0.97\nDiff 0.07 0.06 0.06 0.07 0.11 0.08 0.03 0.04 0.06 0.02 0.01 0.03 0.01 0.01 0.01 0.00 -0.03 0.00\nTable 5. Top-ksampling evaluation with k= 40 . DetectGPT generally provides the most accurate performance (highest AUROC),\nalthough the gap is narrowed comparing to direct sampling, ', 'studies of LLMs’ theory, capacity, and flaws. Researchers can also modify the\nmodel architecture and weights, to validate the proposed algorithms to improve LLMs Zhu et al.\n(2020); Cao et al. (2021); Hase et al. (2021); Mitchell et al. (2022).\nWith INT4 quantization, GLM-130B can perform inference on popularized GPUs such as 4 ×RTX\n3090 or 8 ×RTX 2080 Ti, which can be easily accessed from cloud service. As a result, researchers\nwho cannot afford powerful data-center GPU servers like DGX-A100 can also utilize GLM-130B.\nG.2 I MPACT ON INDIVIDUAL DEVELOPERS AND SMALL COMPANIES\nCurrently, individual developers and small companies who want to integrate LLMs into their busi-\nness can only choose paid inference APIs. The increased cost can hinder their attempts. Instead,\nGLM-130B can be deployed on popularized hardware that they own or can access via cloud service\nto reduce the cost. Furthermore, they can utilize distillation techniques Sanh et al. (2019); Jiao et al.\n(2020) to obtain smaller models that preserve comparable performance on their specific tasks. While\nsome developers may lack the ability to complete deployment and distillation on their own, we be-\nlieve with GLM-130B and more open LLMs in the future, the corresponding toolkits and service\nproviders will become more available.\nWe also note that currently most applications of LLMs are based on prompt engineering, partly\ndue to the limitation of inference APIs. In downstream scenarios such as online customer service,\nthe companies accumulate huge amounts of human-generated data that contain domain knowledge.\nWith the open-source weights and code, developers can finetune GLM-130B on their own data to\nmitigate the gap of domain knowledge.\nG.3 S OCIAL IMPACT\nLarge language models, together ']","DtectGPT's approach is significant as it provides a robust, zero-shot detection method that does not require dataset collection or model retraining, addressing the challenge of rapidly evolving LLM capabilities and their potential misuse in generating misleading content.","DetectGPT's detection approach is significant in the context of evolving LLM capabilities and the potential for misuse because it plays a crucial role in addressing concerns related to the production of toxic and illegal content by LLMs. With the increasing abilities of LLMs like GLM-130B to generate content that can have a significant impact on society, there is a corresponding need to ensure that these models are not being used for malicious purposes.

DetectGPT's approach to detecting samples from the given model on the dataset, as indicated by the AUROC values, shows that it is effective in identifying potentially harmful content. This is important in safeguarding against the misuse of LLMs for producing toxic or illegal content, as highlighted in the context mentioning concerns raised by Weidinger et al., Sheng et al., Dev et al., and Bommasani et al.

Overall, DetectGPT's detection approach is essential for mitigating the risks associated with the misuse of LLMs and ensuring that these powerful language models are used responsibly for the benefit of society.",1.0,1.0,0.7399290800094604
"How is the student model, DistilBERT, initialized from the teacher model for effective training?","['of models.\nIn supervised learning, a classiﬁcation model is generally trained to predict an instance class by\nmaximizing the estimated probability of gold labels. A standard training objective thus involves\nminimizing the cross-entropy between the model’s predicted distribution and the one-hot empirical\ndistribution of training labels. A model performing well on the training set will predict an output\ndistribution with high probability on the correct class and with near-zero probabilities on other\nclasses. But some of these ""near-zero"" probabilities are larger than others and reﬂect, in part, the\ngeneralization capabilities of the model and how well it will perform on the test set3.\nTraining loss The student is trained with a distillation loss over the soft target probabilities of\nthe teacher: Lce=∑\niti∗log(si)where ti(resp. si) is a probability estimated by the teacher\n(resp. the student). This objective results in a rich training signal by leveraging the full teacher\ndistribution. Following Hinton et al. [2015] we used a softmax-temperature :pi=exp(zi/T)∑\njexp(zj/T)\nwhere Tcontrols the smoothness of the output distribution and ziis the model score for the class i.\nThe same temperature Tis applied to the student and the teacher at training time, while at inference,\nTis set to 1 to recover a standard softmax .\nThe ﬁnal training objective is a linear combination of the distillation loss Lcewith the supervised\ntraining loss, in our case the masked language modeling lossLmlm [Devlin et al., 2018]. We found it\nbeneﬁcial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student\nand teacher hidden states vectors.\n3 DistilBERT: a distilled version of BERT\nStudent architecture In the present work, the student - DistilBERT - has ', 'by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of\nindividual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo\nbaseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to\nBERT, retaining 97% of the performance with 40% fewer parameters.\n4.1 Downstream task benchmark\nDownstream tasks We further study the performances of DistilBERT on several downstream tasks\nunder efﬁcient inference constraints: a classiﬁcation task (IMDb sentiment classiﬁcation - Maas et al.\n[2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]).\nAs shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb\nbenchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT.\nWe also studied whether we could add another step of distillation during the adaptation phase by\nﬁne-tuning DistilBERT on SQuAD using a BERT model previously ﬁne-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\n3\x0cTable 4: Ablation study. Variations are relative to the model trained with triple loss and teacher\nweights initialization.\nAblation Variation on GLUE macro-score\n∅-Lcos-Lmlm -2.96\nLce-∅-Lmlm -1.46\nLce-Lcos-∅ -0.31\nTriple loss + random weights initialization -3.69\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two\nsuccessive steps of distillation, one during the pre-training phase and one during the adaptation phase.\nIn this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and\n70.4 EM, i.e. within 3 points of the full model.\nSize and ', 'website:8\nThe Instruct models share our base\nGPT-3 models’ ability to understand and\ngenerate natural language, but they’re\nbetter at understanding and following\nyour instructions. You simply tell the\nmodel what you want it to do, and it\nwill do its best to fulﬁll your instruc-\ntions. This is an important step forward\nin our goal of building safe models that\nare aligned with human interests.\nCrucially, the Instruct Series is inappropriate for\nreproducible research because it is unknown what\ndatasets and prompts these models are trained on,\nand whether any task categories are systematically\nheld out as done by Sanh et al. (2021) and Wei et al.\n(2021). If it is trained on any prompt or dataset of\nNLI, it would not be zero-shot, making it an un-\nfair comparison to other models in our experiments.\nSecond, it is still in beta and its training, held-out,\nand prompt mixtures could change. At least two\nInstruct Series models were made available in se-\nquence during our writing, and it is not clear if we\nexperiment on an older version, whether it will still\nbe available and reproducible in the future.\nB.2 Priming vs. Fine-Tuning\nAs mentioned in Section 3, we use priming (a.k.a.\nin-context learning) in lieu of ﬁne-tuning because,\n7OpenAI never actually discloses which one of their com-\nmercially named ada, babbage, curie, davinci\n“engines” correspond to models of which size. However, Gao\net al. (2021a) estimate that they correspond to 350M, 1.3B,\n6.7B, and 175B respectively.\n8http://beta.openai.com/docs/engines/instruct-series-betaat the time of writing, OpenAI’s ﬁne-tuning API is\nlimited to 10 runs per month. To train 30 prompts\nat only two number of shots would take 6 months,\nassuming we get hyperparameters right at ﬁrst try.\nFurther, each training run is limited to a ']",DistilBERT is initialized from the teacher model by adopting every other layer to maintain some structural and knowledge integrity.,"The student model, DistilBERT, is trained with a distillation loss over the soft target probabilities of the teacher. The teacher model provides probabilities estimated for each class, and the student model is trained to predict similar probabilities. This allows for a rich training signal by leveraging the full teacher distribution.",0.7,1.0,0.4709441065788269
Explain how BERT uses the 'masked LM' (MLM) for its pre-training.,"['NER,\nwe report both ﬁne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npliﬁed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\nMasking Rates Dev Set Results\nMASK SAME RND MNLI NER\nFine-tune Fine-tune Feature-based\n80% 10% 10% 84.2 95.4 94.9\n100% 0% 0% 84.3 94.9 94.0\n80% 0% 20% 84.1 95.2 94.6\n80% 20% 0% 84.4 95.2 94.7\n0% 20% 80% 83.7 94.8 94.6\n0% 0% 100% 83.6 94.9 94.6\nTable 8: Ablation over different masking strategies.\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe[MASK] symbol for MLM; S AME means that\nwe keep the target token as is; R NDmeans that\nwe replace the target token with another random\ntoken.\nThe numbers in the left part of the table repre-\nsent the probabilities of the speciﬁc strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\nFrom the table it can be seen that ﬁne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the M ASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe R NDstrategy performs much worse than our\nstrategy as well.', 'that we only report single-task ﬁne-tuning results\nin this paper. A multitask ﬁne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n15https://gluebenchmark.com/faq\x0cjority class.\nC Additional Ablation Studies\nC.1 Effect of Number of Training Steps\nFigure 5 presents MNLI Dev accuracy after ﬁne-\ntuning from a checkpoint that has been pre-trained\nforksteps. This allows us to answer the following\nquestions:\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh ﬁne-tuning accuracy?\nAnswer: Yes, BERT BASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\nC.2 Ablation for Different Masking\nProcedures\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n200 400 600 800 1,0007678808284\nPre-training Steps (Thousands)MNLI Dev Accuracy\nBERT BASE (Masked LM)\nBERT BASE (Left-to-Right)\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after ﬁne-tuning, starting\nfrom model parameters that have been pre-trained for\nksteps. The x-axis is the value of k.Note that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand ﬁne-tuning, as the [MASK] symbol never ap-\npears during the ﬁne-tuning stage. We report the\nDev results for both MNLI and NER. For ', 'and replacement is performed once in the be-\nginning and saved for the duration of training, al-\nthough in practice, data is duplicated so the mask\nis not always the same for every training sentence\n(see Section 4.1).\nNext Sentence Prediction (NSP) NSP is a bi-\nnary classiﬁcation loss for predicting whether two\nsegments follow each other in the original text.\nPositive examples are created by taking consecu-\ntive sentences from the text corpus. Negative ex-\namples are created by pairing segments from dif-\nferent documents. Positive and negative examples\nare sampled with equal probability.\nThe NSP objective was designed to improve\nperformance on downstream tasks, such as Natural\nLanguage Inference ( Bowman et al. ,2015 ), which\nrequire reasoning about the relationships between\npairs of sentences.\n2.4 Optimization\nBERT is optimized with Adam ( Kingma and Ba ,\n2015 ) using the following parameters: β1= 0.9,\nβ2= 0.999,ǫ=1e-6 and L2weight de-\ncay of0.01. The learning rate is warmed up\nover the ﬁrst 10,000 steps to a peak value of\n1e-4, and then linearly decayed. BERT trains\nwith a dropout of 0.1 on all layers and at-\ntention weights, and a GELU activation func-\ntion ( Hendrycks and Gimpel ,2016 ). Models are\npretrained for S=1,000,000 updates, with mini-\nbatches containing B=256 sequences of maxi-\nmum length T=512 tokens.\n2.5 Data\nBERT is trained on a combination of B OOK COR-\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\nwhich totals 16GB of uncompressed text.3\n3 Experimental Setup\nIn this section, we describe the experimental setup\nfor our replication study of BERT.\n3.1 Implementation\nWe reimplement BERT in FAIRSEQ (Ott et al. ,\n2019 ). We primarily follow the original BERT\n3Yang et al. (2019 ) use the same dataset but report having\nonly 13GB ']","In MLM, a percentage of input tokens are masked randomly, and the model predicts these masked tokens based on their context.","BERT uses the masked language model (MLM) objective for its pre-training by replacing the target token with the [MASK] symbol during pre-training. The model is trained to predict the masked tokens in the input text, helping it to learn the contextual relationships between words and improve its understanding of the text. Additionally, BERT uses a mixed strategy for masking target tokens, which includes replacing the target token with [MASK], keeping the target token as is, or replacing the target token with another random token. The purpose of these masking strategies is to reduce the mismatch between the pre-training and fine-tuning stages, ultimately improving performance on downstream tasks.",1.0,1.0,0.5965884923934937
Discuss the impact of model size on BERT's performance across different tasks.,"['/ 87.4 86.5 (89.0 / 85.5)\nXLNet (Yang et al., 2019) 2 90.8 / 90.8 92.3 95.1 / 89.7 90.6 / 87.9 85.4 (88.6 / 84.0)\nMegatron-336M 1 89.7 / 90.0 92.3 94.2 / 88.0 88.1 / 84.8 83.0 (86.9 / 81.5)\nMegatron-1.3B 1 90.9 / 91.0 92.6 94.9 / 89.1 90.2 / 87.1 87.3 (90.4 / 86.1)\nMegatron-3.9B 1 91.4 / 91.4 92.7 95.5 / 90.0 91.2 / 88.5 89.5 (91.8 / 88.6)\nALBERT ensemble (Lan et al., 2019) 95.5 / 90.1 91.4 / 88.9 89.4 (91.2 / 88.6)\nMegatron-3.9B ensemble 95.8 / 90.5 91.7 / 89.0 90.9 (93.1 / 90.0)\nFigure 7. Training loss for BERT model using the original architec-\nture (a) and the rearranged architecture (b). Left ﬁgure shows the\ntraining loss for 336M and 752M BERT model. While the original\narchitecture performs well on the 336M model, the modiﬁcations\nin (b) enable stable training with lower training loss.\nUsing the architecture change in Figure 7(b), we consider\nthree different cases as detailed in Table 4. The 336M model\nhas the same size as BERT-large. The 1.3B is the same as\nthe BERT-xlarge conﬁguration that was previously shown\nto get worse results than the 336M BERT-large model (Lan\net al., 2019). We further scale the BERT model using both\nlarger hidden size as well as more layers to arrive at the 3.9B\nparameter case. In all cases, the hidden size per attention\nhead is kept constant at 64. 336M and 1.3B models are\ntrained for 2 million iterations while the 3.9B model is\ntrained for 1.5 million iterations and is still training.\nOn a 3% held-out set, 336M, 1.3B, and 3.9B models achieve\nvalidation set perplexity ', '65.1 0.0 100.0/ 50.0\nCBoW 44.3 62.1 49.0/71.2 51.6 0.0 / 0.4 14.0/13.6 49.7 53.0 65.1 -0.4 100.0/ 50.0\nBERT 69.0 77.4 75.7/83.6 70.6 70.0 / 24.0 72.0/71.3 71.6 69.5 64.3 23.0 97.8 / 51.7\nBERT++ 71.5 79.0 84.7/90.4 73.8 70.0 / 24.1 72.0/71.3 79.0 69.5 64.3 38.0 99.4 / 51.4\nOutside Best - 80.4 - / - 84.4 70.4 */24.5*74.8/73.0 82.7 - - - - / -\nHuman (est.) 89.8 89.0 95.8/98.9 100.0 81.8*/51.9* 91.7/91.3 93.6 80.0 100.0 77.0 99.3 / 99.7\n5 Experiments\n5.1 Baselines\nBERT Our main baselines are built around BERT, variants of which are among the most successful\napproach on GLUE at the time of writing. Speciﬁcally, we use the bert-large-cased variant.\nFollowing the practice recommended in Devlin et al. (2019), for each task, we use the simplest\npossible architecture on top of BERT. We ﬁne-tune a copy of the pretrained BERT model separately\nfor each task, and leave the development of multi-task learning models to future work. For training,\nwe use the procedure speciﬁed in Devlin et al. (2019): We use Adam (Kingma and Ba, 2014) with an\ninitial learning rate of 10−5and ﬁne-tune for a maximum of 10 epochs.\nFor classiﬁcation tasks with sentence-pair inputs (BoolQ, CB, RTE, WiC), we concatenate the\nsentences with a [SEP]token, feed the fused input to BERT, and use a logistic regression classiﬁer\nthat sees the representation corresponding to [CLS]. For WiC, we also concatenate the representation\nof the marked word. For COPA, MultiRC, and ReCoRD, for each answer choice, we similarly\nconcatenate the context with that answer choice and feed the resulting sequence into BERT to produce\nan answer representation. For COPA, we ', 'in\nour training data.\n5.3. Bi-directional Transformer Results Using BERT\nIn this section, we apply our methodology to BERT-style\ntransformer models and study the effect of model scaling\non several downstream tasks. Prior work (Lan et al., 2019)\nfound that increasing model size beyond BERT-large with\n336M parameters results in unexpected model degradation.\nTo address this degradation, the authors of that work (Lan\net al., 2019) introduced parameter sharing and showed that\nthat their models scale much better compared to the original\nBERT model.\nWe further investigated this behaviour and empirically\ndemonstrated that rearranging the order of the layer nor-\nmalization and the residual connections as shown in Figure\n7 is critical to enable the scaling of the BERT-style mod-\nels beyond BERT-Large. The architecture (b) in Figure 7\neliminates instabilities observed using the original BERT\narchitecture in (a) and also has a lower training loss. To\nthe best of our knowledge, we are the ﬁrst to report such a\nchange enables training larger BERT models.\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 5. Development set results for MNLI, QQP, SQuAD 1.1 and SQuAD 2.0 and test set results for RACE. The trained tokens represents\nconsumed tokens during model pretraining (proportional to batch size times number of iterations) normalized by consumed tokens during\nmodel pretraining for our 336M model.\nModeltrained tokens MNLI m/mm QQP SQuAD 1.1 SQuAD 2.0 RACE m/h\nratio accuracy accuracy F1 / EM F1 / EM accuracy\n(dev set) (dev set) (dev set) (dev set) (test set)\nRoBERTa (Liu et al., 2019b) 2 90.2 / 90.2 92.2 94.6 / 88.9 89.4 / 86.5 83.2 (86.5 / 81.8)\nALBERT (Lan et al., 2019) 3 90.8 92.2 94.8 / 89.3 90.2 ']","Larger BERT models consistently show better performance across tasks, indicating the importance of model size in achieving high accuracy.","The impact of model size on BERT's performance across different tasks is significant. In the context provided, it is shown that as the model size increases from 336M to 1.3B to 3.9B parameters, the performance of the BERT model improves. Specifically, the 3.9B model outperforms the 336M and 1.3B models in terms of validation set perplexity. Additionally, it is mentioned that larger hidden size and more layers in the BERT model result in better performance. This highlights the importance of scaling the model size for achieving higher accuracy and better results across various tasks.",1.0,1.0,0.8540820479393005
What are the hyperparameters of the AdamW optimizer used in training the LLaMA models?,"['use the RMSNorm normalizing func-\ntion, introduced by Zhang and Sennrich (2019).\nSwiGLU activation function [PaLM]. We re-\nplace the ReLU non-linearity by the SwiGLU ac-\ntivation function, introduced by Shazeer (2020) to\nimprove the performance. We use a dimension of\n2\n34dinstead of 4das in PaLM.\nRotary Embeddings [GPTNeo]. We remove the\nabsolute positional embeddings, and instead, add\nrotary positional embeddings (RoPE), introduced\nby Su et al. (2021), at each layer of the network.\nThe details of the hyper-parameters for our dif-\nferent models are given in Table 2.\n2.3 Optimizer\nOur models are trained using the AdamW opti-\nmizer (Loshchilov and Hutter, 2017), with the fol-\nlowing hyper-parameters: β1= 0.9,β2= 0.95.\nWe use a cosine learning rate schedule, such that\nthe ﬁnal learning rate is equal to 10% of the maxi-\nmal learning rate. We use a weight decay of 0.1and\ngradient clipping of 1.0. We use 2,000warmup\n0 200 400 600 800 1000 1200 1400\nBillion of tokens1.51.61.71.81.92.02.12.2Training lossLLaMA 7B\nLLaMA 13B\nLLaMA 33B\nLLaMA 65BFigure 1: Training loss over train tokens for the 7B,\n13B, 33B, and 65 models. LLaMA-33B and LLaMA-\n65B were trained on 1.4T tokens. The smaller models\nwere trained on 1.0T tokens. All models are trained\nwith a batch size of 4M tokens.\nsteps, and vary the learning rate and batch size with\nthe size of the model (see Table 2 for details).\n2.4 Efﬁcient implementation\nWe make several optimizations to improve the train-\ning speed of our models. First, we use an efﬁcient\nimplementation of the causal multi-head attention\nto reduce memory usage and runtime. This imple-\nmentation, available in the xformers library,2is\ninspired by Rabe and Staats (2021) and uses the\nbackward from Dao et al. (2022). This is achieved\nby not storing the attention weights and ', '0.98\nGradient Clipping 0.0 0.0\nTable 9: Hyperparameters for pretraining RoBERTa LARGE and RoBERTa BASE.\nHyperparam RACE SQuAD GLUE\nLearning Rate 1e-5 1.5e-5 {1e-5, 2e-5, 3e-5 }\nBatch Size 16 48 {16, 32}\nWeight Decay 0.1 0.01 0.1\nMax Epochs 4 2 10\nLearning Rate Decay Linear Linear Linear\nWarmup ratio 0.06 0.06 0.06\nTable 10: Hyperparameters for ﬁnetuning RoBERTa LARGE on RACE, SQuAD and GLUE.', 'GLM-130B training\nConfiguration Key Value\nadam_beta1 0.9\nadam_beta2 0.95\nadam_eps 1e-08\naggregated_samples_per_sequence 4\nattention_dropout 0.1\nattention_softmax_in_fp32 True\naverage_block_length 3\nbias_dropout_fusion True\ncheckpoint_activations True\ncheckpoint_in_cpu False\ncheckpoint_num_layers 1\nclip_grad 1.0\ncontigious_checkpointing False\ncpu_optimizer False\ndata_parallel_size 24\ndeepnorm True\ndistributed_backend nccl\neval_interval 1000\neval_iters 3\nffn_hidden_size 32768\nfp16 True\nglobal_batch_size 4224\nglu_activation geglu\ngpt_prob 0.7\nhidden_dropout 0.1\nhidden_size 12288\nhysteresis 2\ninit_method_std 0.0052\ninit_method_xavier_uniform False\ninitial_loss_scale 65536\nlayernorm_epsilon 1E-05\nlearnable_rotary_embedding False\nlength_per_sample 2000\nlog_interval 1\nloss_scale 0\nloss_scale_window 2000\nlr 8e-05\nlr_decay_iters None\nlr_decay_samples 197753905\nlr_decay_style cosine\nlr_warmup_samples 1098632\nmake_vocab_size_divisible_by 768\nmask_prob 0.15\nmasked_softmax_fusion True\nmicro_batch_size 1\nmin_gmask_ratio 0.2\nmin_loss_scale 1.0\nmin_lr 8e-06\nmultitask_ratio 0.05\nnum_attention_heads 96\nnum_layers 70\nonnx_safe None\noptimizer adam\npartition_activations True\npipeline_model_parallel_size 8\nposition_embedding_type rotary\nrampup_batch_size 192, 24, 5493164\nsave_interval 250\nseed 1234\nseq_length 2048\nshort_seq_prob 0.02\nshrink_embedding_gradient_alpha 0.1\nsingle_span_prob 0.02\nsplit 949,50,1\ntensor_model_parallel_size 4\ntokenizer_type IceTokenizer\nweight_decay 0.1\nzero_contigious_gradients False\nzero_reduce_bucket_size 500000000\nzero_reduce_scatter False\nzero_stage 1\nzero-optimization.allgather_bucket_size 500000000\ntokenizer_type IceTokenizer\nweight_decay 0.1\nworld_size 768\nzero_contigious_gradients FALSE\nzero_reduce_bucket_size 500000000\nzero_reduce_scatter FALSE\nzero_stage 1\nzero-optimization.allgather_bucket_size 500000000\n48\x0cPublished as a conference paper at ICLR 2023\nTable 12: The 74 datasets involved in Multi-task Instruction Pre-training (MIP). Datasets from T0-\nPromptSource (Sanh et al., 2022; Bach et al., 2022) are named in their Hugging Face datasets iden-\ntifiers. Datasets from DeepStruct (Wang et al., 2022a) are described in Appendix C.2.\nTask Dataset Task Dataset\nCoreference Resolution super_glue/wsc.fixed Multi-choice QA cos_e/v1.11\nCoreference Resolution winogrande/winogrande_xl Multi-choice QA cosmos_qa\nNatural Language Inference super_glue/cb Multi-choice QA dream\nNatural Language Inference super_glue/rte Multi-choice QA openbookqa/main\nNatural Language Inference anli Multi-choice QA qasc\nParaphrase Identification glue/mrpc Multi-choice QA quail\nParaphrase Identification glue/qqp Multi-choice QA quarel\nParaphrase Identification paws/labeled_final Multi-choice QA quartz\nClosed-Book QA ai2_arc/ARC_Challenge Multi-choice QA race/high\nClosed-Book QA ai2_arc/ARC_Easy Multi-choice QA race/middle\nClosed-Book QA kilt_tasks/hoptpotqa Multi-choice QA sciq\nClosed-Book QA trivia_qa/unfiltered Multi-choice QA social_i_qa\nClosed-Book QA web_questions Multi-choice QA super_glue/boolq\nClosed-Book QA wiki_qa Multi-choice QA super_glue/multirc\nExtractive QA adversarial_qa/dbidaf Multi-choice QA wiki_hop/original\nExtractive QA adversarial_qa/dbert Multi-choice QA wiqa\nExtractive QA adversarial_qa/droberta Multi-choice QA piqa\nExtractive QA duorc/SelfRC Topic Classification ag_news\nExtractive QA duorc/ParaphraseRC Topic Classification dbpedia_14\nExtractive QA ropes Topic Classification trec\nExtractive QA squad_v2 Word Sense Disambiguation super_glue/wic\nExtractive QA super_glue/record Dialogue State Tracking multiwoz_2.1\nExtractive QA quoref Event Extraction ']","The AdamW optimizer for LLaMA models uses hyperparameters β1 = 0.9, β2 = 0.95, with a cosine learning rate schedule, a weight decay of 0.1, and gradient clipping set to 1.0.",None,1.0,1.0,0.09954242408275604
"In what ways does LLaMA's evaluation strategy extend beyond standard NLP tasks, and what new dimensions of model performance does this reveal?","['- 39.6\nLLaMA7B 16.8 18.7 22.0 26.1\n13B 20.1 23.4 28.1 31.9\n33B 24.9 28.3 32.9 36.0\n65B 23.8 31.0 35.0 39.9\nTable 4: NaturalQuestions. Exact match performance.\n3.1 Common Sense Reasoning\nWe consider eight standard common sense rea-\nsoning benchmarks: BoolQ (Clark et al., 2019),\nPIQA (Bisk et al., 2020), SIQA (Sap et al., 2019),\x0cHellaSwag (Zellers et al., 2019), WinoGrande (Sak-\naguchi et al., 2021), ARC easy and challenge (Clark\net al., 2018) and OpenBookQA (Mihaylov et al.,\n2018). These datasets include Cloze and Winograd\nstyle tasks, as well as multiple choice question an-\nswering. We evaluate in the zero-shot setting as\ndone in the language modeling community.\nIn Table 3, we compare with existing models\nof various sizes and report numbers from the cor-\nresponding papers. First, LLaMA-65B outper-\nforms Chinchilla-70B on all reported benchmarks\nbut BoolQ. Similarly, this model surpasses PaLM-\n540B everywhere but on BoolQ and WinoGrande.\nLLaMA-13B model also outperforms GPT-3 on\nmost benchmarks despite being 10 ×smaller.\n3.2 Closed-book Question Answering\nWe compare LLaMA to existing large language\nmodels on two closed-book question answering\nbenchmarks: Natural Questions (Kwiatkowski\net al., 2019) and TriviaQA (Joshi et al., 2017). For\nboth benchmarks, we report exact match perfor-\nmance in a closed book setting, i.e., where the mod-\nels do not have access to documents that contain\nevidence to answer the question. In Table 4, we\nreport performance on NaturalQuestions, and in Ta-\nble 5, we report on TriviaQA. On both benchmarks,\nLLaMA-65B achieve state-of-the-arts performance\nin the zero-shot and few-shot settings. More im-\nportantly, the LLaMA-13B is also competitive on\nthese benchmarks with GPT-3 and Chinchilla, de-\nspite being 5-10 ×smaller. This model runs on a\nsingle V100 GPU during inference.\n0-shot 1-shot 5-shot 64-shot\nGopher 280B 43.5 - 57.0 57.2\nChinchilla 70B 55.4 - ', 'fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of\n∗Equal contribution. Correspondence: {htouvron,\nthibautlav,gizacard,egrave,glample}@meta.com\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al. (2022) recommends\ntraining a 10B model on 200B tokens, we ﬁnd\nthat the performance of a 7B model continues to\nimprove even after 1T tokens.\nThe focus of this work is to train a series of\nlanguage models that achieve the best possible per-\nformance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA , ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 ×smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modiﬁcations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We ', 'we\nconsider zero-shot and few-shot tasks, and report\nresults on a total of 20 benchmarks:\n•Zero-shot. We provide a textual description\nof the task and a test example. The model\neither provides an answer using open-ended\ngeneration, or ranks the proposed answers.\n•Few-shot. We provide a few examples of the\ntask (between 1 and 64) and a test example.\nThe model takes this text as input and gener-\nates the answer or ranks different options.\nWe compare LLaMA with other foundation mod-\nels, namely the non-publicly available language\nmodels GPT-3 (Brown et al., 2020), Gopher (Rae\net al., 2021), Chinchilla (Hoffmann et al., 2022)\nand PaLM (Chowdhery et al., 2022), as well as\nthe open-sourced OPT models (Zhang et al., 2022),\nGPT-J (Wang and Komatsuzaki, 2021), and GPT-\nNeo (Black et al., 2022). In Section 4, we also\nbrieﬂy compare LLaMA with instruction-tuned\nmodels such as OPT-IML (Iyer et al., 2022) and\nFlan-PaLM (Chung et al., 2022).We evaluate LLaMA on free-form generation\ntasks and multiple choice tasks. In the multiple\nchoice tasks, the objective is to select the most\nappropriate completion among a set of given op-\ntions, based on a provided context. We select the\ncompletion with the highest likelihood given the\nprovided context. We follow Gao et al. (2021)\nand use the likelihood normalized by the number\nof characters in the completion, except for certain\ndatasets (OpenBookQA, BoolQ), for which we fol-\nlow Brown et al. (2020), and select a completion\nbased on the likelihood normalized by the likeli-\nhood of the completion given “Answer:” as context:\nP(completion|context )/P(completion|“Answer :”).\n0-shot 1-shot 5-shot 64-shot\nGPT-3 175B 14.6 23.0 - 29.9\nGopher 280B 10.1 - 24.5 28.2\nChinchilla 70B 16.6 - 31.5 35.5\nPaLM8B 8.4 10.6 - 14.6\n62B 18.1 26.5 - 27.6\n540B 21.2 29.3 ']","LLaMA's evaluation strategy includes novel tasks that assess the model's ability in reasoning, domain-specific knowledge (e.g., legal or medical), and its proficiency in languages other than English. This approach reveals dimensions of model performance such as cross-domain adaptability, fine-grained linguistic understanding, and the capacity to generalize knowledge across different contexts and languages.","LLaMA's evaluation strategy extends beyond standard NLP tasks by considering zero-shot and few-shot tasks across a total of 20 benchmarks. This evaluation strategy reveals new dimensions of model performance by testing the model's ability to provide answers through open-ended generation, rank proposed answers, generate answers or rank options based on a few task examples, and compare performance with other foundation models and instruction-tuned models. This approach provides insight into the model's adaptability, generalization capabilities, and effectiveness in various scenarios beyond traditional NLP tasks.",1.0,1.0,0.7285958528518677
Describe the methodology used for evaluating models on the multitask test. How does it differ from traditional model evaluations?,"['matrices. (Best viewed magniﬁed). (Left) Error matrix for the CUB+iNat\nmeta-task. The numbers in each cell is the test error obtained by training a classiﬁer on a given combination of task (rows)\nand expert (columns). The background color represent the Asymmetric TASK 2VEC distance between the target task and\nthe task used to train the expert. Numbers in red indicate the selection made by the model selection algorithm based on\nthe Asymmetric TASK 2VEC embedding. The (out-of-diagonal) optimal expert (when different from the one selected by our\nalgorithm), is highlighted in blue. (Right) Same as before, but for the Mixed meta-task.', '57tasks. On the right are UniﬁedQA\ntransfer accuracies for all of the 57tasks. For both models, capabilities are lopsided.\nA.1 A NALYSIS WITH MORE FINE-TUNED MODELS\nWe primarily analyzed models with more than 10billion parameters in the main body of the paper.\nFor this section, we analyze smaller models including RoBERTa-base (125 million parameters) (Liu\n11\x0cPublished as a conference paper at ICLR 2021\net al., 2019), ALBERT-xxlarge (223 million parameters) (Lan et al., 2020), and GPT-2 (1,558 million\nparameters) (Radford et al., 2019). Models are ﬁne-tuned to predict one of four classes using the\nUniﬁedQA MCQ questions and using our dev+val set. We test on our multitask test set.\nWe observe that these smaller models can attain better-than-random accuracy. RoBERTa-base attains\nan overall accuracy of 27.9%, with 27.9%accuracy for the humanities, 28.8%for social sciences,\n27.0%for STEM, and 27.7%for other. ALBERT-xxlarge attains an accuracy of 27.1%, with 27.2%\naccuracy for the humanities, 25.7%for the social sciences, 27.7%for STEM, and 27.9%for other.\nGPT-2 attains an accuracy of 32.4%, with 32.8%accuracy for the humanities, 33.3%for the social\nsciences, 30.2%for STEM, and 33.1%for other.\nCompare this to UniﬁedQA’s smallest variant, which has just 60million parameters and approximately\n29.3%accuracy. It obtains higher accuracy than RoBERTa and ALBERT, even though it has fewer\nparameters. This suggests that its larger pretraining dataset enables higher accuracy. Likewise,\nUniﬁedQA with 3billion parameters attains 43.7%, while the similarly sized GPT-2 model with 1.5\nbillion parameters attains 32.4%accuracy. This again suggests that T5’s larger pretraining dataset\nsize (and therefore UniﬁedQA’s pretraining dataset size) can increase accuracy.\nA.2 E RROR ANALYSIS\nWe qualitatively analyze when GPT-3 makes high conﬁdence mistakes. We ﬁnd that while many of\nthese mistakes were clearly wrong, ', 'these use different train/validation/test splits from other public\nversions in some cases. Systems also may not use the unlabeled test data for the tasks in system\ndevelopment in any way, may not use the structured source data that was used to collect the WiC\nlabels (sense-annotated example sentences from WordNet, VerbNet, and Wiktionary) in any way, and\nmay not build systems that share information across separate testexamples in any way.\nTo ensure reasonable credit assignment, because we build very directly on prior work, we ask the\nauthors of submitted systems to directly name and cite the speciﬁc datasets that they use, including the\nbenchmark datasets . We will enforce this as a requirement for papers to be listed on the leaderboard.\n4https://github.com/nyu-mll/jiant\n5https://github.com/huggingface/transformers\n7\x0cTable 3: Baseline performance on the SuperGLUE test sets and diagnostics. For CB we report\naccuracy and macro-average F1. For MultiRC we report F1 on all answer-options and exact match\nof each question’s set of correct answers. AX bis the broad-coverage diagnostic task, scored using\nMatthews’ correlation (MCC). AX gis the Winogender diagnostic, scored using accuracy and the\ngender parity score (GPS). All values are scaled by 100. The Avgcolumn is the overall benchmark\nscore on non-AX ∗tasks. The bolded numbers reﬂect the best machine performance on task. *MultiRC\nhas multiple test sets released on a staggered schedule, and these results evaluate on an installation of\nthe test set that is a subset of ours.\nModel Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC AX b AXg\nMetrics Acc. F1/Acc. Acc. F1 a/EM F1/EM Acc. Acc. Acc. MCC GPS Acc.\nMost Frequent 47.1 62.3 21.7/48.4 50.0 61.1 / 0.3 33.4/32.5 50.3 50.0 ']","Models were evaluated in zero-shot and few-shot settings to measure their ability to apply pretraining knowledge without further fine-tuning, a method designed to be more akin to human learning and evaluation processes.","The methodology used for evaluating models on the multitask test involves fine-tuning smaller models with varying numbers of parameters (RoBERTa-base, ALBERT-xxlarge, GPT-2) to predict one of four classes using UniﬁedQA MCQ questions and a dev+val set. The models are then tested on a multitask test set. This method differs from traditional model evaluations in that it focuses on smaller models with fewer parameters and evaluates their performance across multiple tasks, showing that even smaller models can achieve better-than-random accuracy. Additionally, the evaluation compares the performance of different models based on their pretraining dataset size, suggesting that larger pretraining datasets can lead to increased accuracy.",0.9,1.0,0.5714566707611084
What characteristic of large language model (LLM) generated text's probability function does DetectGPT exploit for detection?,"['al.,\n2023) may further improve the effectiveness of methods\nsuch as DetectGPT, even as LLMs continue to improve.\nLimitations. One limitation of probability-based methods\nfor zero-shot machine-generated text detection (like Detect-\nGPT) is the white-box assumption that we can evaluate log\nprobabilities of the model(s) in question. For models be-\nhind APIs that do provide probabilities (such as GPT-3),\nevaluating probabilities nonetheless costs money. Another\nassumption of DetectGPT is access to a reasonable pertur-\nbation function. While in this work, we use off-the-shelf\nmask-filling models such as T5 and mT5 (for non-English\nlanguages), some domains may see reduced performance\nif existing mask-filling models do not well represent the\nspace of meaningful rephrases, reducing the quality of the\ncurvature estimate. While DetectGPT provides the best\navailable detection performance for PubMedQA, its drop\nin performance compared to other datasets may be a result\nAverage length0.9850.9900.995AUROC\ngpt-2\nAverage length0.960.970.980.99AUROC\nopt-2.7\nXSum\nSQuAD\nWritingPrompts\n130 140 150 160 170\nAverage length0.8750.9000.9250.9500.975AUROC\nEleutherAI/gpt-j-6b\n130 140 150 160 170\nAverage length0.70.80.9AUROC\nEleutherAI/gpt-neox-20bFigure 10. DetectGPT AUROC vs passage length. The relation-\nship between detection performance and passage length generally\ndepends on the dataset and model (or tokenizer). Decreases in\ndetection quality with increasing length may be due to T5 failing\nto track many (20+) masks to fill at once; this problem may be\nmitigated by applying mask-fills in a sequence of smaller batches.\nof lower quality perturbations. Finally, DetectGPT is more\ncompute-intensive than other methods for detection, as it\nrequires sampling and scoring the set of perturbations for\neach candidate passage, rather than just the candidate pas-\nsage; a better tuned perturbation function or more efficient\ncurvature approximation may help mitigate these costs.\nFuture Work. While the methods in this work make no\nassumptions about the models generating the samples, fu-\nture work may explore how watermarking ', 'DetectGPT: Zero-Shot Machine-Generated Text Detection\nusing Probability Curvature\nEric Mitchell1Yoonho Lee1Alexander Khazatsky1Christopher D. Manning1Chelsea Finn1\nAbstract\nThe increasing fluency and widespread usage of\nlarge language models (LLMs) highlight the de-\nsirability of corresponding tools aiding detection\nof LLM-generated text. In this paper, we identify\na property of the structure of an LLM’s proba-\nbility function that is useful for such detection.\nSpecifically, we demonstrate that text sampled\nfrom an LLM tends to occupy negative curva-\nture regions of the model’s log probability func-\ntion. Leveraging this observation, we then define\na new curvature-based criterion for judging if a\npassage is generated from a given LLM. This\napproach, which we call DetectGPT, does not re-\nquire training a separate classifier, collecting a\ndataset of real or generated passages, or explic-\nitly watermarking generated text. It uses only\nlog probabilities computed by the model of in-\nterest and random perturbations of the passage\nfrom another generic pre-trained language model\n(e.g., T5). We find DetectGPT is more discrimi-\nnative than existing zero-shot methods for model\nsample detection, notably improving detection of\nfake news articles generated by 20B parameter\nGPT-NeoX from 0.81 AUROC for the strongest\nzero-shot baseline to 0.95 AUROC for Detect-\nGPT. See ericmitchell.ai/detectgpt\nfor code, data, and other project information.\n1. Introduction\nLarge language models (LLMs) have proven able to gen-\nerate remarkably fluent responses to a wide variety of user\nqueries. Models such as GPT-3 (Brown et al., 2020), PaLM\n(Chowdhery et al., 2022), and ChatGPT (OpenAI, 2022)\ncan convincingly answer complex questions about science,\nmathematics, historical and current events, and social trends.\n1Stanford University. Correspondence to: Eric Mitchell\n<eric.mitchell@cs.stanford.edu >.\nProceedings of the 40thInternational Conference on Machine\nLearning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s). Candidate passage : “Joe Biden recently made ', 'both top- k\nand nucleus sampling make detection easier, on average.\nAveraging across domains, DetectGPT provides the clearest\nsignal for zero-shot detection.\nDetection when the source model is unknown. While\nour experiments have focused on the white-box setting\nfor machine-generated text detection, in this section, we\nGPT-J GPT-Neo GPT-2Scoring ModelGPT-J GPT-Neo GPT-2Base Model0.92\n(0.02)0.83\n(0.04)0.79\n(0.02)\n0.64\n(0.06)0.97\n(0.01)0.83\n(0.02)\n0.60\n(0.09)0.85\n(0.05)0.99\n(0.00)0.85\n0.81\n0.81\n0.72 0.88 0.87\nFigure 6. DetectGPT performs\nbest when scoring samples\nwith the same model that gen-\nerated them (diagonal), but\nthe column means suggest that\nsome models (GPT-Neo, GPT-\n2) may be better ‘scorers’ than\nothers (GPT-J). White values\nshow mean (standard error)\nAUROC over XSum, SQuAD,\nand WritingPrompts; black\nshows row/column mean.explore the effect of using\na different model to score a\ncandidate passage (and per-\nturbed texts) than the model\nthat generated the passage.\nIn other words, we aim\nto classify between human-\ngenerated text and text from\nmodel A, but without ac-\ncess to model Ato com-\npute log probabilities. In-\nstead, we use log probabil-\nities computed by a surro-\ngate model B. We con-\nsider three models, GPT-J,\nGPT-Neo-2.7, and GPT-2,\nevaluating all possible com-\nbinations of source model\nand surrogate model (9 to-\ntal). We average the perfor-\nmance across 200 samples\nfrom XSum, SQuAD, and\n7\x0cZero-Shot Machine-Generated Text Detection using Probability Curvature\n60M 220M 770M 2.7B0.50.60.70.80.91.0Detection AUROC\n5 perturbations\n60M 220M 770M 2.7B\n25 perturbations\nRandom\nGPT2-sm\nGPT2-md\nGPT2-lg\nGPT2-xl\nMask filling model size (# parameters)\nFigure 7. There is a clear association between capacity of mask-\nfilling model and detection performance, across source model\nscales. Random mask filling (uniform sampling from mask filling\nmodel vocabulary) performs poorly, reinforcing the idea that the\nperturbation function should produce samples on the data manifold.\nCurves show AUROC scores on 200 SQuAD contexts.\nWritingPrompts. The results are presented in Figure 6,\nshowing that when the surrogate model is different from the\nsource model, detection performance is reduced, indicating\nthat DetectGPT ']",DetectGPT exploits the characteristic that LLM-generated text tends to lie in regions of negative curvature of the model's log probability function.,Negative curvature regions of the model's log probability function.,1.0,1.0,0.5317816734313965
What empirical validation does DetectGPT provide for its hypothesis regarding log probability curvature?,"['0.58 0.53 0.53 0.50 0.55 0.56 0.57 0.54 0.32 0.37 0.28 0.32 0.32 0.32\nDetectGPT 0.99 0.98 1.00 0.98 0.97 0.98 0.99 0.98 0.98 0.90 0.82* 0.94 1.00 0.99 0.99 0.97* 0.93 0.98\nDiff 0.04 0.04 0.04 0.05 0.08 0.05 0.01 0.02 0.04 0.00 -0.01 0.02 0.01 0.01 0.01 -0.01 -0.05 0.00\nTable 4. Nucleus (top- p) sampling evaluation with p= 0.96. AUROC for detecting samples from the given model on the given dataset for\nDetectGPT and four previously proposed criteria. Nucleus sampling generally makes detection easier for all methods, but DetectGPT still\nprovides the highest average AUROC. For WritingPrompts, however, the LogRank baseline performs as well as DetectGPT.\nXSum SQuAD WritingPrompts\nMethod GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg. GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg. GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg.\nlogp(x) 0.89 0.89 0.89 0.84 0.81 0.87 0.93 0.90 0.88 0.82 0.74 0.85 0.97 0.95 0.97 0.96 0.95* 0.96\nRank 0.79 0.77 0.77 0.75 0.73 0.76 0.84 0.82 0.80 0.80 0.75 0.80 0.87 0.84 0.83 0.82 0.81 0.83\nLogRank 0.92* 0.91* 0.93* 0.89* 0.85* 0.90* 0.96* 0.94* 0.92* 0.87* 0.79* 0.90* 0.98* 0.97* 0.98* 0.97 0.96 0.97\nEntropy 0.58 0.49 0.55 0.56 0.59 0.55 0.55 0.52 0.56 0.56 0.58 0.56 0.35 0.41 0.30 0.34 0.37 0.35\nDetectGPT 0.99 0.97 0.99 0.96 0.96 0.98 0.99 0.98 0.98 0.89 0.80 0.93 0.99 0.98 0.99 0.97 0.93 0.97\nDiff 0.07 0.06 0.06 0.07 0.11 0.08 0.03 0.04 0.06 0.02 0.01 0.03 0.01 0.01 0.01 0.00 -0.03 0.00\nTable 5. Top-ksampling evaluation with k= 40 . DetectGPT generally provides the most accurate performance (highest AUROC),\nalthough the gap is narrowed comparing to direct sampling, ', 'deviation of the ob-\nserved values used to estimate E˜x∼q(·|x)logpθ(˜x)provides\na slightly better signal for detection, typically increasing\n1We later show in Figure 8 that varying the number of samples\nused to estimate the expectation effectively allows for trading off\nbetween accuracy and speed.AUROC by around 0.020, so we use this normalized version\nof the perturbation discrepancy in our experiments. The\nresulting method, DetectGPT, is summarized in Alg. 1. Hav-\ning described an application of the perturbation discrepancy\nto machine-generated text detection, we next provide an\ninterpretation of this quantity.\nInterpretation of perturbation discrepancy as curvature\nWhile Figure 3 suggests that the perturbation discrepancy\nmay be useful, it is not immediately obvious what it mea-\nsures. In this section, we show that the perturbation dis-\ncrepancy approximates a measure of the local curvature\nof the log probability function near the candidate passage,\nmore specifically, that it is proportional to the negative trace\nof the Hessian of the log probability function.2To han-\ndle the non-differentiability of discrete data, we consider\ncandidate passages in a latent semantic space, where small\ndisplacements correspond to valid edits that retain similar\nmeaning to the original. Because our perturbation function\n(T5) models natural text, we expect our perturbations to\nroughly capture such meaningful variations of the original\npassage, rather than arbitrary edits.\nWe first invoke Hutchinson’s trace estimator (Hutchinson,\n1990), giving an unbiased estimate of the trace of matrix A:\ntr(A) =Ezz⊤Az (2)\nprovided that the elements of z∼qzare IID with E[zi] = 0\nandVar(zi) = 1 . To use Equation 2 to estimate the trace\nof the Hessian of fatx, we must therefore compute the\nexpectation of the directional second derivative z⊤Hf(x)z.\nWe approximate this expression with finite differences:\nz⊤Hf(x)z≈f(x+hz) +f(x−hz)−2f(x)\nh2(3)\nCombining Equations 2 and 3 ', 'is most suited to the white-box setting. Yet\nwe also observe that if we fix the model used for scoring\nand average across source models whose generations are\ndetected (average within column), there is significant varia-\ntion in AUROC; GPT-2 and GPT-Neo-2.7 seem to be better\n‘scorers’ than GPT-J. These variations in cross-model scor-\ning performance suggest ensembling scoring models may\nbe a useful direction for future research; see Mireshghallah\net al. (2023) for reference.\n5.3.Other factors impacting performance of DetectGPT\nIn this section, we explore how factors such as the size of the\nmask-filling model, the number of perturbations used to es-\ntimate the expectation in Equation 1, or the data distribution\nof the text to be detected impact detection quality.\nSource and mask-filling model scale. Here we study the\nimpact of the size of the source model and mask-filling\nmodel on DetectGPT’s performance; the results are shown\nin Figure 7. In particular, the increased discrimination power\nof DetectGPT for larger mask-filling models supports the\ninterpretation that DetectGPT is estimating the curvature\nof the log probability in a latent semantic space, rather\nthan in raw token embedding space. Larger T5 models\nbetter represent this latent space, where random directions\ncorrespond to meaningful changes in the text.\nNumber of perturbations for DetectGPT. We evaluate the\nperformance of DetectGPT as a function of the number of\nperturbations used to estimate the expectation in Equation 1\non three datasets. The results are presented in Figure 8.\nDetection accuracy continues to improve until 100 pertur-\nbations, where it converges. Evaluations use 100 examples\nfrom each dataset.\nData distributional properties. We study more closely\nthe impact of the data distribution on DetectGPT, particu-\n1 10 100 10000.60.70.80.91.0Detection AUROC\nGPT-2\nXSum\nSQuAD\nWritingPrompts\n1 10 100 1000\nGPT-J\nNumber of perturbationsFigure ']","DetectGPT's hypothesis is empirically validated across a diverse body of LLMs, showing consistent results even when perturbations are sourced from different language models.","DetectGPT provides empirical validation for its hypothesis regarding log probability curvature by evaluating the performance of DetectGPT as a function of the number of perturbations used to estimate the expectation. The results show that detection accuracy continues to improve until 100 perturbations, where it converges. This validation supports the interpretation that DetectGPT is estimating the curvature of the log probability in a latent semantic space.",1.0,1.0,0.6302763223648071
What datasets were used for BERT's pre-training and why?,"['arXiv:1907.11692v1 [cs.CL] 26 Jul 2019RoBERTa: A Robustly Optimized BERT Pretraining Approach\nYinhan Liu∗§Myle Ott∗§Naman Goyal∗§Jingfei Du∗§Mandar Joshi†\nDanqi Chen§Omer Levy§Mike Lewis§Luke Zettlemoyer†§Veselin Stoyanov§\n†Paul G. Allen School of Computer Science & Engineering,\nUniversity of Washington, Seattle, WA\n{mandar90,lsz }@cs.washington.edu\n§Facebook AI\n{yinhanliu,myleott,naman,jingfeidu,\ndanqi,omerlevy,mikelewis,lsz,ves }@fb.com\nAbstract\nLanguage model pretraining has led to sig-\nniﬁcant performance gains but careful com-\nparison between different approaches is chal-\nlenging. Training is computationally expen-\nsive, often done on private datasets of different\nsizes, and, as we will show, hyperparameter\nchoices have signiﬁcant impact on the ﬁnal re-\nsults. We present a replication study of BERT\npretraining ( Devlin et al. ,2019 ) that carefully\nmeasures the impact of many key hyperparam-\neters and training data size. We ﬁnd that BERT\nwas signiﬁcantly undertrained, and can match\nor exceed the performance of every model\npublished after it. Our best model achieves\nstate-of-the-art results on GLUE, RACE and\nSQuAD. These results highlight the impor-\ntance of previously overlooked design choices,\nand raise questions about the source of re-\ncently reported improvements. We release our\nmodels and code.1\n1 Introduction\nSelf-training methods such as ELMo ( Peters et al. ,\n2018 ), GPT ( Radford et al. ,2018 ), BERT\n(Devlin et al. ,2019 ), XLM ( Lample and Conneau ,\n2019 ), and XLNet ( Yang et al. ,2019 ) have\nbrought signiﬁcant performance gains, but it can\nbe challenging to determine which aspects of\nthe methods contribute the most. Training is\ncomputationally expensive, limiting the amount\nof tuning that can be done, and is often done with\nprivate training data of varying sizes, limiting\nour ability to measure the effects of the modeling\nadvances.\n∗Equal contribution.\n1Our models and code are available at:\nhttps://github.com/pytorch/fairseqWe present a replication study of BERT pre-\ntraining ( Devlin et al. ,2019 ), ', 'of text after data cleaning. This is most likely due\nto subtle differences in cleaning of the Wikipedia data.\x0coptimization hyperparameters, given in Section 2,\nexcept for the peak learning rate and number of\nwarmup steps, which are tuned separately for each\nsetting. We additionally found training to be very\nsensitive to the Adam epsilon term, and in some\ncases we obtained better performance or improved\nstability after tuning it. Similarly, we found setting\nβ2= 0.98to improve stability when training with\nlarge batch sizes.\nWe pretrain with sequences of at most T= 512\ntokens. Unlike Devlin et al. (2019 ), we do not ran-\ndomly inject short sequences, and we do not train\nwith a reduced sequence length for the ﬁrst 90% of\nupdates. We train only with full-length sequences.\nWe train with mixed precision ﬂoating point\narithmetic on DGX-1 machines, each with 8 ×\n32GB Nvidia V100 GPUs interconnected by In-\nﬁniband ( Micikevicius et al. ,2018 ).\n3.2 Data\nBERT-style pretraining crucially relies on large\nquantities of text. Baevski et al. (2019 ) demon-\nstrate that increasing data size can result in im-\nproved end-task performance. Several efforts\nhave trained on datasets larger and more diverse\nthan the original BERT ( Radford et al. ,2019 ;\nYang et al. ,2019 ;Zellers et al. ,2019 ). Unfortu-\nnately, not all of the additional datasets can be\npublicly released. For our study, we focus on gath-\nering as much data as possible for experimenta-\ntion, allowing us to match the overall quality and\nquantity of data as appropriate for each compari-\nson.\nWe consider ﬁve English-language corpora of\nvarying sizes and domains, totaling over 160GB\nof uncompressed text. We use the following text\ncorpora:\n•BOOK CORPUS (Zhu et al. ,2015 ) plus English\nWIKIPEDIA . ', 'that we only report single-task ﬁne-tuning results\nin this paper. A multitask ﬁne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n15https://gluebenchmark.com/faq\x0cjority class.\nC Additional Ablation Studies\nC.1 Effect of Number of Training Steps\nFigure 5 presents MNLI Dev accuracy after ﬁne-\ntuning from a checkpoint that has been pre-trained\nforksteps. This allows us to answer the following\nquestions:\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh ﬁne-tuning accuracy?\nAnswer: Yes, BERT BASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\nC.2 Ablation for Different Masking\nProcedures\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n200 400 600 800 1,0007678808284\nPre-training Steps (Thousands)MNLI Dev Accuracy\nBERT BASE (Masked LM)\nBERT BASE (Left-to-Right)\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after ﬁne-tuning, starting\nfrom model parameters that have been pre-trained for\nksteps. The x-axis is the value of k.Note that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand ﬁne-tuning, as the [MASK] symbol never ap-\npears during the ﬁne-tuning stage. We report the\nDev results for both MNLI and NER. For ']","BooksCorpus (800M words) and English Wikipedia (2500M words), chosen for their document-level organization and volume of data.","The datasets used for BERT's pre-training were the BOOK CORPUS (Zhu et al., 2015) and English WIKIPEDIA. They were used to gather as much data as possible for experimentation, allowing to match the overall quality and quantity of data as appropriate for each comparison.",1.0,1.0,0.357181191444397
How do the LLaMA models' parameter counts compare across the different versions?,"['fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of\n∗Equal contribution. Correspondence: {htouvron,\nthibautlav,gizacard,egrave,glample}@meta.com\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al. (2022) recommends\ntraining a 10B model on 200B tokens, we ﬁnd\nthat the performance of a 7B model continues to\nimprove even after 1T tokens.\nThe focus of this work is to train a series of\nlanguage models that achieve the best possible per-\nformance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA , ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 ×smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modiﬁcations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We ', 'description on two\nbenchmarks: HumanEval (Chen et al., 2021) and\nMBPP (Austin et al., 2021). For both tasks, the\nmodel receives a description of the program in a\nfew sentences, as well as a few input-output ex-\namples. In HumanEval, it also receives a function\nsignature, and the prompt is formatted as natural\ncode with the textual description and tests in a\x0cMATH +maj1@k GSM8k +maj1@k\nPaLM8B 1.5 - 4.1 -\n62B 4.4 - 33.0 -\n540B 8.8 - 56.5 -\nMinerva8B 14.1 25.4 16.2 28.4\n62B 27.6 43.4 52.4 68.5\n540B 33.6 50.3 68.5 78.5\nLLaMA7B 2.9 6.9 11.0 18.1\n13B 3.9 8.8 17.8 29.3\n33B 7.1 15.2 35.6 53.1\n65B 10.6 20.5 50.9 69.7\nTable 7: Model performance on quantitative reason-\ning datasets. For majority voting, we use the same\nsetup as Minerva, with k= 256 samples for MATH\nandk= 100 for GSM8k (Minerva 540B uses k= 64\nfor MATH and and k= 40 for GSM8k). LLaMA-65B\noutperforms Minerva 62B on GSM8k, although it has\nnot been ﬁne-tuned on mathematical data.\ndocstring. The model needs to generate a Python\nprogram that ﬁts the description and satisﬁes the\ntest cases. In Table 8, we compare the pass@1\nscores of our models with existing language mod-\nels that have not been ﬁnetuned on code, namely\nPaLM and LaMDA (Thoppilan et al., 2022). PaLM\nand LLaMA were trained on datasets that contain\na similar number of code tokens.\nAs show in Table 8, for a similar number\nof parameters, LLaMA outperforms other gen-\neral models such as LaMDA and PaLM, which\nare not trained or ﬁnetuned speciﬁcally for code.\nLLaMA with 13B parameters and more outper-\nforms LaMDA 137B on both HumanEval and\nMBPP. LLaMA 65B also outperforms PaLM 62B,\neven when it is trained longer. The pass@1 ', 'the ability of the model to\nfollow instructions. Since this is not the focus of\nthis paper, we only conducted a single experiment\nfollowing the same protocol as Chung et al. (2022)\nto train an instruct model, LLaMA-I.\nOPT 30B 26.1\nGLM 120B 44.8\nPaLM 62B 55.1\nPaLM-cont 62B 62.8\nChinchilla 70B 67.5\nLLaMA 65B 63.4\nOPT-IML-Max 30B 43.2\nFlan-T5-XXL 11B 55.1\nFlan-PaLM 62B 59.6\nFlan-PaLM-cont 62B 66.1\nLLaMA-I 65B 68.9\nTable 10: Instruction ﬁnetuning – MMLU (5-shot).\nComparison of models of moderate size with and with-\nout instruction ﬁnetuning on MMLU.In Table 10, we report the results of our instruct\nmodel LLaMA-I on MMLU and compare with ex-\nisting instruction ﬁnetuned models of moderate\nsizes, namely, OPT-IML (Iyer et al., 2022) and the\nFlan-PaLM series (Chung et al., 2022). All the re-\nported numbers are from the corresponding papers.\nDespite the simplicity of the instruction ﬁnetuning\napproach used here, we reach 68.9% on MMLU.\nLLaMA-I (65B) outperforms on MMLU existing\ninstruction ﬁnetuned models of moderate sizes, but\nare still far from the state-of-the-art, that is 77.4\nfor GPT code-davinci-002 on MMLU (numbers\ntaken from Iyer et al. (2022)). The details of the\nperformance on MMLU on the 57 tasks can be\nfound in Table 16 of the appendix.\n5 Bias, Toxicity and Misinformation\nLarge language models have been showed to re-\nproduce and amplify biases that are existing in\nthe training data (Sheng et al., 2019; Kurita et al.,\n2019), and to generate toxic or offensive con-\ntent (Gehman et al., 2020). As our training dataset\ncontains a large proportion of data from the Web,\nwe believe that it is crucial to determine the po-\ntential for our models to generate such content.\nTo understand the potential harm of LLaMA-65B,\nwe evaluate on different benchmarks that measure\ntoxic content ']","The LLaMA modelsvary in size from 7 billion parameters to 65 billion parameters, with intermediate sizes of 13 billion and 33 billion parameters.","The LLaMA models' parameter counts range from 7B to 65B parameters, with varying performance levels on different benchmarks.",1.0,1.0,0.8317229747772217
"What are the significant benchmarks LLaMA models were evaluated on, and how does their performance relate to other foundation models?","['studies of LLMs’ theory, capacity, and flaws. Researchers can also modify the\nmodel architecture and weights, to validate the proposed algorithms to improve LLMs Zhu et al.\n(2020); Cao et al. (2021); Hase et al. (2021); Mitchell et al. (2022).\nWith INT4 quantization, GLM-130B can perform inference on popularized GPUs such as 4 ×RTX\n3090 or 8 ×RTX 2080 Ti, which can be easily accessed from cloud service. As a result, researchers\nwho cannot afford powerful data-center GPU servers like DGX-A100 can also utilize GLM-130B.\nG.2 I MPACT ON INDIVIDUAL DEVELOPERS AND SMALL COMPANIES\nCurrently, individual developers and small companies who want to integrate LLMs into their busi-\nness can only choose paid inference APIs. The increased cost can hinder their attempts. Instead,\nGLM-130B can be deployed on popularized hardware that they own or can access via cloud service\nto reduce the cost. Furthermore, they can utilize distillation techniques Sanh et al. (2019); Jiao et al.\n(2020) to obtain smaller models that preserve comparable performance on their specific tasks. While\nsome developers may lack the ability to complete deployment and distillation on their own, we be-\nlieve with GLM-130B and more open LLMs in the future, the corresponding toolkits and service\nproviders will become more available.\nWe also note that currently most applications of LLMs are based on prompt engineering, partly\ndue to the limitation of inference APIs. In downstream scenarios such as online customer service,\nthe companies accumulate huge amounts of human-generated data that contain domain knowledge.\nWith the open-source weights and code, developers can finetune GLM-130B on their own data to\nmitigate the gap of domain knowledge.\nG.3 S OCIAL IMPACT\nLarge language models, together ', 'fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of\n∗Equal contribution. Correspondence: {htouvron,\nthibautlav,gizacard,egrave,glample}@meta.com\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al. (2022) recommends\ntraining a 10B model on 200B tokens, we ﬁnd\nthat the performance of a 7B model continues to\nimprove even after 1T tokens.\nThe focus of this work is to train a series of\nlanguage models that achieve the best possible per-\nformance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA , ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 ×smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modiﬁcations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We ', '. . . . . . . . . . 52\nE.4 Project Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nE.5 Computation Sponsor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nF A Brief History of GLM-130B 53\nG Broader Impact 55\nG.1 Impact on AI Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\nG.2 Impact on Individual Developers and Small Companies . . . . . . . . . . . . . . 55\n20\x0cPublished as a conference paper at ICLR 2023\nG.3 Social Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\nH Environmental Impact 56\nA E THICS : EVALUATION ON BIASES AND TOXICITY\nAlbeit LLMs’ strong abilities in language and beyond, which could bring substantial welfare to\nhuman beings, they can potentially produce toxic and illegal contents for evil use (Weidinger et al.,\n2021; Sheng et al., 2021; Dev et al., 2021; Bommasani et al., 2021). In GLM-130B, before granting\nmodel weight to applicants, in the model license we demand them ']"," LLaMA models were evaluated on benchmarks such as Common Sense Reasoning, Closed-book Question Answering, Reading Comprehension, Mathematical Reasoning, and Code Generation, showing superior or competitive performance compared to existing foundation models.","The significant benchmarks that LLaMA models were evaluated on include outperforming GPT-3 on most benchmarks, despite being 10 times smaller, and being competitive with the best large language models such as Chinchilla or PaLM-540B. The performance of LLaMA models demonstrates competitive performance compared to the best existing LLMs.",1.0,1.0,0.7182573676109314
What is the primary goal of introducing the massive multitask test in language understanding models?,"['Published as a conference paper at ICLR 2021\nMEASURING MASSIVE MULTITASK\nLANGUAGE UNDERSTANDING\nDan Hendrycks\nUC BerkeleyCollin Burns\nColumbia UniversitySteven Basart\nUChicagoAndy Zou\nUC Berkeley\nMantas Mazeika\nUIUCDawn Song\nUC BerkeleyJacob Steinhardt\nUC Berkeley\nABSTRACT\nWe propose a new test to measure a text model’s multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess extensive\nworld knowledge and problem solving ability. We ﬁnd that while most recent\nmodels have near random-chance accuracy, the very largest GPT-3 model improves\nover random chance by almost 20 percentage points on average. However, on every\none of the 57 tasks, the best models still need substantial improvements before\nthey can reach expert-level accuracy. Models also have lopsided performance\nand frequently do not know when they are wrong. Worse, they still have near-\nrandom accuracy on some socially important subjects such as morality and law.\nBy comprehensively evaluating the breadth and depth of a model’s academic and\nprofessional understanding, our test can be used to analyze models across many\ntasks and to identify important shortcomings.\n1 I NTRODUCTION\nNatural Language Processing (NLP) models have achieved superhuman performance on a number of\nrecently proposed benchmarks. However, these models are still well below human level performance\nfor language understanding as a whole, suggesting a disconnect between our benchmarks and the\nactual capabilities of these models. The General Language Understanding Evaluation benchmark\n(GLUE) (Wang et al., 2018) was introduced in 2018 to evaluate performance on a wide range of NLP\ntasks, and top models achieved superhuman performance within a year. To address the shortcomings\nof GLUE, researchers designed the SuperGLUE benchmark with more difﬁcult tasks (Wang et al.,\n2019). About ', 'year, but recent models are already nearing human-level performance on several\nof these, including HellaSwag (Zellers et al., 2019), Physical IQA (Bisk et al., 2019), and CosmosQA\n(Huang et al., 2019). By design, these datasets assess abilities that almost every child has. In contrast,\nwe include harder specialized subjects that people must study to learn.\nSome researchers have suggested that the future of NLP evaluation should focus on Natural Language\nGeneration (NLG) (Zellers et al., 2020), an idea that reaches back to the Turing Test (Turing, 1950).\nHowever, NLG is notoriously difﬁcult to evaluate and lacks a standard metric (Sai et al., 2020).\nConsequently, we instead create a simple-to-evaluate test that measures classiﬁcation accuracy on\nmultiple choice questions.\nWhile several question answering benchmarks exist, they are comparatively limited in scope. Most\neither cover easy topics like grade school subjects for which models can already achieve strong\nperformance (Clark et al., 2018; Khot et al., 2019; Mihaylov et al., 2018; Clark et al., 2019), or\nare focused on linguistic understanding in the form of reading comprehension (Lai et al., 2017;\nRichardson et al., 2013). In contrast, we include a wide range of difﬁcult subjects that go far beyond\nlinguistic understanding.\n3 A M ULTITASK TEST\nWe create a massive multitask test consisting of multiple-choice questions from various branches of\nknowledge. The test spans subjects in the humanities, social sciences, hard sciences, and other areas\nthat are important for some people to learn. There are 57tasks in total, which is also the number\nof Atari games (Bellemare et al., 2013), all of which are listed in Appendix B. The questions in\nthe dataset were manually collected by graduate ', 'these use different train/validation/test splits from other public\nversions in some cases. Systems also may not use the unlabeled test data for the tasks in system\ndevelopment in any way, may not use the structured source data that was used to collect the WiC\nlabels (sense-annotated example sentences from WordNet, VerbNet, and Wiktionary) in any way, and\nmay not build systems that share information across separate testexamples in any way.\nTo ensure reasonable credit assignment, because we build very directly on prior work, we ask the\nauthors of submitted systems to directly name and cite the speciﬁc datasets that they use, including the\nbenchmark datasets . We will enforce this as a requirement for papers to be listed on the leaderboard.\n4https://github.com/nyu-mll/jiant\n5https://github.com/huggingface/transformers\n7\x0cTable 3: Baseline performance on the SuperGLUE test sets and diagnostics. For CB we report\naccuracy and macro-average F1. For MultiRC we report F1 on all answer-options and exact match\nof each question’s set of correct answers. AX bis the broad-coverage diagnostic task, scored using\nMatthews’ correlation (MCC). AX gis the Winogender diagnostic, scored using accuracy and the\ngender parity score (GPS). All values are scaled by 100. The Avgcolumn is the overall benchmark\nscore on non-AX ∗tasks. The bolded numbers reﬂect the best machine performance on task. *MultiRC\nhas multiple test sets released on a staggered schedule, and these results evaluate on an installation of\nthe test set that is a subset of ours.\nModel Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC AX b AXg\nMetrics Acc. F1/Acc. Acc. F1 a/EM F1/EM Acc. Acc. Acc. MCC GPS Acc.\nMost Frequent 47.1 62.3 21.7/48.4 50.0 61.1 / 0.3 33.4/32.5 50.3 50.0 ']","The primary goal is to bridge the gap between the vast knowledge models encounter during pretraining on the internet and the existing measures of success, assessing models across a diverse set of subjects to highlight knowledge and problem-solving ability in more challenging, real-world contexts.","The primary goal of introducing the massive multitask test in language understanding models is to measure a text model's multitask accuracy across a wide range of subjects, including specialized and difficult topics, in order to comprehensively evaluate the breadth and depth of the model's academic and professional understanding.",1.0,1.0,0.5380187034606934
"What were the key findings regarding the models' performance on the multitask test, particularly concerning their knowledge application and subject-specific accuracy?","['57tasks. On the right are UniﬁedQA\ntransfer accuracies for all of the 57tasks. For both models, capabilities are lopsided.\nA.1 A NALYSIS WITH MORE FINE-TUNED MODELS\nWe primarily analyzed models with more than 10billion parameters in the main body of the paper.\nFor this section, we analyze smaller models including RoBERTa-base (125 million parameters) (Liu\n11\x0cPublished as a conference paper at ICLR 2021\net al., 2019), ALBERT-xxlarge (223 million parameters) (Lan et al., 2020), and GPT-2 (1,558 million\nparameters) (Radford et al., 2019). Models are ﬁne-tuned to predict one of four classes using the\nUniﬁedQA MCQ questions and using our dev+val set. We test on our multitask test set.\nWe observe that these smaller models can attain better-than-random accuracy. RoBERTa-base attains\nan overall accuracy of 27.9%, with 27.9%accuracy for the humanities, 28.8%for social sciences,\n27.0%for STEM, and 27.7%for other. ALBERT-xxlarge attains an accuracy of 27.1%, with 27.2%\naccuracy for the humanities, 25.7%for the social sciences, 27.7%for STEM, and 27.9%for other.\nGPT-2 attains an accuracy of 32.4%, with 32.8%accuracy for the humanities, 33.3%for the social\nsciences, 30.2%for STEM, and 33.1%for other.\nCompare this to UniﬁedQA’s smallest variant, which has just 60million parameters and approximately\n29.3%accuracy. It obtains higher accuracy than RoBERTa and ALBERT, even though it has fewer\nparameters. This suggests that its larger pretraining dataset enables higher accuracy. Likewise,\nUniﬁedQA with 3billion parameters attains 43.7%, while the similarly sized GPT-2 model with 1.5\nbillion parameters attains 32.4%accuracy. This again suggests that T5’s larger pretraining dataset\nsize (and therefore UniﬁedQA’s pretraining dataset size) can increase accuracy.\nA.2 E RROR ANALYSIS\nWe qualitatively analyze when GPT-3 makes high conﬁdence mistakes. We ﬁnd that while many of\nthese mistakes were clearly wrong, ', 'these use different train/validation/test splits from other public\nversions in some cases. Systems also may not use the unlabeled test data for the tasks in system\ndevelopment in any way, may not use the structured source data that was used to collect the WiC\nlabels (sense-annotated example sentences from WordNet, VerbNet, and Wiktionary) in any way, and\nmay not build systems that share information across separate testexamples in any way.\nTo ensure reasonable credit assignment, because we build very directly on prior work, we ask the\nauthors of submitted systems to directly name and cite the speciﬁc datasets that they use, including the\nbenchmark datasets . We will enforce this as a requirement for papers to be listed on the leaderboard.\n4https://github.com/nyu-mll/jiant\n5https://github.com/huggingface/transformers\n7\x0cTable 3: Baseline performance on the SuperGLUE test sets and diagnostics. For CB we report\naccuracy and macro-average F1. For MultiRC we report F1 on all answer-options and exact match\nof each question’s set of correct answers. AX bis the broad-coverage diagnostic task, scored using\nMatthews’ correlation (MCC). AX gis the Winogender diagnostic, scored using accuracy and the\ngender parity score (GPS). All values are scaled by 100. The Avgcolumn is the overall benchmark\nscore on non-AX ∗tasks. The bolded numbers reﬂect the best machine performance on task. *MultiRC\nhas multiple test sets released on a staggered schedule, and these results evaluate on an installation of\nthe test set that is a subset of ours.\nModel Avg BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC AX b AXg\nMetrics Acc. F1/Acc. Acc. F1 a/EM F1/EM Acc. Acc. Acc. MCC GPS Acc.\nMost Frequent 47.1 62.3 21.7/48.4 50.0 61.1 / 0.3 33.4/32.5 50.3 50.0 ', 'examples 550–557, and so on. Across\ndifferent seeds, a different starting example index\nis drawn. The exact training example indices are\nalso recorded in our GitHub repository for repro-\nducibility.\nStatistical Tests We use both ANOV A and its\nnonparametric equivalent, the Kruskal–Wallis test.\nAfter ﬁnding a signiﬁcant difference among multi-\nple categories of templates, we report pairwise sig-\nniﬁcance with the independent two-sample t-test\nand the Wilcoxon rank-sum test. We set α= 0.05\nand apply the Bonferroni correction to account for\nmultiple comparisons. For all results reported in\nthis paper, both t-test and Wilcoxon agree.\n4 Effect of Templates\nOur research question is whether models under-\nstand prompts as meaningful task instructions anal-\nogous to how humans would. For intuition, sup-\npose an experimenter provides a human annotator\nwith an informative instruction of a reasonably easy\ntask. If the annotator understands the instruction,\nwe expect them to perform better than when the\nexperimenter provides intentionally misleading in-\nstructions, makes irrelevant chitchat, or says noth-\ning at all. Accordingly, we write various prompt\ntemplates that correspond to these different scenar-\nios and evaluate models’ performance with these\ntemplates in zero-shot and few-shot settings.\n4.1 Method\nWe write 5 categories of templates (Table 1), with\nat least 5 templates for each category (10 for in-\nstructive):\n•Instructive: how we would describe the NLI\ntask to a human who has never seen this task\nbefore.Category Examples\ninstructive{prem} Are we justiﬁed in saying that “{hypo}”?\nSuppose {prem} Can we infer that “{hypo}”?\nmisleading-\nmoderate{prem} Can that be paraphrased as: “{hypo}”?\n{prem} Are there lots of similar words in “{hypo}”?\nmisleading-\nextreme{prem} is the sentiment positive? {hypo}\n{prem} is this a sports news? {hypo}\nirrelevant{prem} If bonito ﬂakes boil more than a few seconds\nthe stock becomes too strong. ""{hypo}""?\nnull{premise} {hypothesis}\n{hypothesis} {premise}\nTable 1: Example ']","The findings highlighted that while recent models like GPT-3 have made progress, they exhibit lopsided performance across different subjects, lack expert-level accuracy, and struggle with procedural knowledge and tasks requiring calculations.","The key findings regarding the models' performance on the multitask test are as follows:
- Smaller models like RoBERTa-base, ALBERT-xxlarge, and GPT-2 showed better-than-random accuracy in predicting one of four classes using UniﬁedQA MCQ questions.
- RoBERTa-base attained an overall accuracy of 27.9%, ALBERT-xxlarge 27.1%, and GPT-2 32.4%.
- UniﬁedQA's smallest variant with 60 million parameters achieved approximately 29.3% accuracy, higher than RoBERTa and ALBERT models.
- UniﬁedQA with 3 billion parameters attained 43.7% accuracy, outperforming the similarly sized GPT-2 model with 1.5 billion parameters.
- The larger pretraining dataset size of T5 (and therefore UniﬁedQA) was found to increase accuracy significantly.",0.8888888888888888,1.0,0.6162525415420532
How does DetectGPT perform in comparison to the strongest zero-shot baseline when detecting fake news articles generated by GPT-NeoX?,"['samples used for evaluation). From 1.5B parameter GPT-2 to 20B parameter GPT-NeoX, DetectGPT consistently provides the most\naccurate detections. Bold shows the best AUROC within each column (model-dataset combination); asterisk (*) denotes the second-best\nAUROC. Values in the final row show DetectGPT’s AUROC over the strongest baseline method in that column.\ncompact semantic space. Since the mask-filling model sam-\nples sentences similar to xwith minimal changes to seman-\ntic meaning, we can think of the mask-filling model as first\nsampling a similar semantic embedding ( ˜z∼qz) and then\nmapping this to a token sequence ( ˜z7→˜x). Sampling in\nsemantic space ensures that all samples stay near the data\nmanifold, which is useful because we would expect the log\nprobability to always drop if we randomly perturb tokens.\nWe can therefore interpret our objective as approximating\nthe curvature restricted to the data manifold.\n5. Experiments\nWe conduct experiments to better understand multiple facets\nof machine-generated text detection; we study the effective-\nness of DetectGPT for zero-shot machine-generated text de-\ntection compared to prior zero-shot approaches, the impact\nof distribution shift on zero-shot and supervised detectors,\nand detection accuracy for the largest publicly-available\nmodels. To further characterize factors that impact detec-\ntion accuracy, we also study the robustness of zero-shot\nmethods to machine-generated text that has been partially\nrevised, the impact of alternative decoding strategies on\ndetection accuracy, and a black-box variant of the detec-\ntion task. Finally, we analyze more closely DetectGPT’s\nbehavior as the choice of perturbation function, the number\nof samples used to estimate d(x, pθ, q), the length of the\npassage, and the data distribution is varied.\nComparisons. We compare DetectGPT with various exist-\ning zero-shot methods for machine-generated text detection\nthat also leverage the ', 'both top- k\nand nucleus sampling make detection easier, on average.\nAveraging across domains, DetectGPT provides the clearest\nsignal for zero-shot detection.\nDetection when the source model is unknown. While\nour experiments have focused on the white-box setting\nfor machine-generated text detection, in this section, we\nGPT-J GPT-Neo GPT-2Scoring ModelGPT-J GPT-Neo GPT-2Base Model0.92\n(0.02)0.83\n(0.04)0.79\n(0.02)\n0.64\n(0.06)0.97\n(0.01)0.83\n(0.02)\n0.60\n(0.09)0.85\n(0.05)0.99\n(0.00)0.85\n0.81\n0.81\n0.72 0.88 0.87\nFigure 6. DetectGPT performs\nbest when scoring samples\nwith the same model that gen-\nerated them (diagonal), but\nthe column means suggest that\nsome models (GPT-Neo, GPT-\n2) may be better ‘scorers’ than\nothers (GPT-J). White values\nshow mean (standard error)\nAUROC over XSum, SQuAD,\nand WritingPrompts; black\nshows row/column mean.explore the effect of using\na different model to score a\ncandidate passage (and per-\nturbed texts) than the model\nthat generated the passage.\nIn other words, we aim\nto classify between human-\ngenerated text and text from\nmodel A, but without ac-\ncess to model Ato com-\npute log probabilities. In-\nstead, we use log probabil-\nities computed by a surro-\ngate model B. We con-\nsider three models, GPT-J,\nGPT-Neo-2.7, and GPT-2,\nevaluating all possible com-\nbinations of source model\nand surrogate model (9 to-\ntal). We average the perfor-\nmance across 200 samples\nfrom XSum, SQuAD, and\n7\x0cZero-Shot Machine-Generated Text Detection using Probability Curvature\n60M 220M 770M 2.7B0.50.60.70.80.91.0Detection AUROC\n5 perturbations\n60M 220M 770M 2.7B\n25 perturbations\nRandom\nGPT2-sm\nGPT2-md\nGPT2-lg\nGPT2-xl\nMask filling model size (# parameters)\nFigure 7. There is a clear association between capacity of mask-\nfilling model and detection performance, across source model\nscales. Random mask filling (uniform sampling from mask filling\nmodel vocabulary) performs poorly, reinforcing the idea that the\nperturbation function should produce samples on the data manifold.\nCurves show AUROC scores on 200 SQuAD contexts.\nWritingPrompts. The results are presented in Figure 6,\nshowing that when the surrogate model is different from the\nsource model, detection performance is reduced, indicating\nthat DetectGPT ', 'the slash).\nand the average margin of improvement is narrow.4For 14\nof the 15 combinations of dataset and model, DetectGPT\nprovides the most accurate detection performance, with a\n0.06 AUROC improvement on average. Log-rank threshold-\ning proves to be a consistently stronger baseline than log\nprobability thresholding, although it requires slightly more\ninformation (full predicted logits), which are not always\navailable in public APIs.\nComparison with supervised detectors. While our experi-\nments generally focus on zero-shot detection, some works\nhave evaluated the detection performance of supervised\nmethods (typically fine-tuned transformers) for detecting\nmachine-generated text. In this section, we explore several\ndomains to better understand the relative strengths of super-\nvised and zero-shot detectors. The results are presented in\nFigure 4, using 200 samples from each dataset for evalua-\ntion. We find that supervised detectors can provide similar\ndetection performance to DetectGPT on in-distribution data\nlike English news, but perform significantly worse than zero-\nshot methods in the case of English scientific writing and\nfail altogether for German writing. This finding echoes past\nwork showing that language models trained for machine-\ngenerated text detection overfit to their training data (source\nmodel, decoding strategy, topic, language, etc.; Uchendu\net al. (2020); Ippolito et al. (2020); Jawahar et al. (2020)).\nIn contrast, zero-shot methods generalize relatively easily\nto new languages and domains; DetectGPT’s performance\nin particular is mostly unaffected by the change in language\nfrom English to German.\nWhile our experiments have shown that DetectGPT is ef-\nfective on a variety of domains and models, it is natural to\nwonder if it is effective for the largest publicly-available\nLMs. Therefore, we also evaluate multiple zero-shot and su-\npervised methods on two 175B parameter models, OpenAI’s\nGPT-3 and AI21 Labs’ Jurassic-2 Jumbo. Because neither\nAPI provides ']",DetectGPT improves detection from an AUROC of 0.81 for the strongest zero-shot baseline to 0.95 for DetectGPT.,None,1.0,1.0,0.08912938088178635
How does DetectGPT's performance vary across different datasets and models in zero-shot detection scenarios?,"['0.58 0.53 0.53 0.50 0.55 0.56 0.57 0.54 0.32 0.37 0.28 0.32 0.32 0.32\nDetectGPT 0.99 0.98 1.00 0.98 0.97 0.98 0.99 0.98 0.98 0.90 0.82* 0.94 1.00 0.99 0.99 0.97* 0.93 0.98\nDiff 0.04 0.04 0.04 0.05 0.08 0.05 0.01 0.02 0.04 0.00 -0.01 0.02 0.01 0.01 0.01 -0.01 -0.05 0.00\nTable 4. Nucleus (top- p) sampling evaluation with p= 0.96. AUROC for detecting samples from the given model on the given dataset for\nDetectGPT and four previously proposed criteria. Nucleus sampling generally makes detection easier for all methods, but DetectGPT still\nprovides the highest average AUROC. For WritingPrompts, however, the LogRank baseline performs as well as DetectGPT.\nXSum SQuAD WritingPrompts\nMethod GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg. GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg. GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg.\nlogp(x) 0.89 0.89 0.89 0.84 0.81 0.87 0.93 0.90 0.88 0.82 0.74 0.85 0.97 0.95 0.97 0.96 0.95* 0.96\nRank 0.79 0.77 0.77 0.75 0.73 0.76 0.84 0.82 0.80 0.80 0.75 0.80 0.87 0.84 0.83 0.82 0.81 0.83\nLogRank 0.92* 0.91* 0.93* 0.89* 0.85* 0.90* 0.96* 0.94* 0.92* 0.87* 0.79* 0.90* 0.98* 0.97* 0.98* 0.97 0.96 0.97\nEntropy 0.58 0.49 0.55 0.56 0.59 0.55 0.55 0.52 0.56 0.56 0.58 0.56 0.35 0.41 0.30 0.34 0.37 0.35\nDetectGPT 0.99 0.97 0.99 0.96 0.96 0.98 0.99 0.98 0.98 0.89 0.80 0.93 0.99 0.98 0.99 0.97 0.93 0.97\nDiff 0.07 0.06 0.06 0.07 0.11 0.08 0.03 0.04 0.06 0.02 0.01 0.03 0.01 0.01 0.01 0.00 -0.03 0.00\nTable 5. Top-ksampling evaluation with k= 40 . DetectGPT generally provides the most accurate performance (highest AUROC),\nalthough the gap is narrowed comparing to direct sampling, ', 'samples used for evaluation). From 1.5B parameter GPT-2 to 20B parameter GPT-NeoX, DetectGPT consistently provides the most\naccurate detections. Bold shows the best AUROC within each column (model-dataset combination); asterisk (*) denotes the second-best\nAUROC. Values in the final row show DetectGPT’s AUROC over the strongest baseline method in that column.\ncompact semantic space. Since the mask-filling model sam-\nples sentences similar to xwith minimal changes to seman-\ntic meaning, we can think of the mask-filling model as first\nsampling a similar semantic embedding ( ˜z∼qz) and then\nmapping this to a token sequence ( ˜z7→˜x). Sampling in\nsemantic space ensures that all samples stay near the data\nmanifold, which is useful because we would expect the log\nprobability to always drop if we randomly perturb tokens.\nWe can therefore interpret our objective as approximating\nthe curvature restricted to the data manifold.\n5. Experiments\nWe conduct experiments to better understand multiple facets\nof machine-generated text detection; we study the effective-\nness of DetectGPT for zero-shot machine-generated text de-\ntection compared to prior zero-shot approaches, the impact\nof distribution shift on zero-shot and supervised detectors,\nand detection accuracy for the largest publicly-available\nmodels. To further characterize factors that impact detec-\ntion accuracy, we also study the robustness of zero-shot\nmethods to machine-generated text that has been partially\nrevised, the impact of alternative decoding strategies on\ndetection accuracy, and a black-box variant of the detec-\ntion task. Finally, we analyze more closely DetectGPT’s\nbehavior as the choice of perturbation function, the number\nof samples used to estimate d(x, pθ, q), the length of the\npassage, and the data distribution is varied.\nComparisons. We compare DetectGPT with various exist-\ning zero-shot methods for machine-generated text detection\nthat also leverage the ', 'both top- k\nand nucleus sampling make detection easier, on average.\nAveraging across domains, DetectGPT provides the clearest\nsignal for zero-shot detection.\nDetection when the source model is unknown. While\nour experiments have focused on the white-box setting\nfor machine-generated text detection, in this section, we\nGPT-J GPT-Neo GPT-2Scoring ModelGPT-J GPT-Neo GPT-2Base Model0.92\n(0.02)0.83\n(0.04)0.79\n(0.02)\n0.64\n(0.06)0.97\n(0.01)0.83\n(0.02)\n0.60\n(0.09)0.85\n(0.05)0.99\n(0.00)0.85\n0.81\n0.81\n0.72 0.88 0.87\nFigure 6. DetectGPT performs\nbest when scoring samples\nwith the same model that gen-\nerated them (diagonal), but\nthe column means suggest that\nsome models (GPT-Neo, GPT-\n2) may be better ‘scorers’ than\nothers (GPT-J). White values\nshow mean (standard error)\nAUROC over XSum, SQuAD,\nand WritingPrompts; black\nshows row/column mean.explore the effect of using\na different model to score a\ncandidate passage (and per-\nturbed texts) than the model\nthat generated the passage.\nIn other words, we aim\nto classify between human-\ngenerated text and text from\nmodel A, but without ac-\ncess to model Ato com-\npute log probabilities. In-\nstead, we use log probabil-\nities computed by a surro-\ngate model B. We con-\nsider three models, GPT-J,\nGPT-Neo-2.7, and GPT-2,\nevaluating all possible com-\nbinations of source model\nand surrogate model (9 to-\ntal). We average the perfor-\nmance across 200 samples\nfrom XSum, SQuAD, and\n7\x0cZero-Shot Machine-Generated Text Detection using Probability Curvature\n60M 220M 770M 2.7B0.50.60.70.80.91.0Detection AUROC\n5 perturbations\n60M 220M 770M 2.7B\n25 perturbations\nRandom\nGPT2-sm\nGPT2-md\nGPT2-lg\nGPT2-xl\nMask filling model size (# parameters)\nFigure 7. There is a clear association between capacity of mask-\nfilling model and detection performance, across source model\nscales. Random mask filling (uniform sampling from mask filling\nmodel vocabulary) performs poorly, reinforcing the idea that the\nperturbation function should produce samples on the data manifold.\nCurves show AUROC scores on 200 SQuAD contexts.\nWritingPrompts. The results are presented in Figure 6,\nshowing that when the surrogate model is different from the\nsource model, detection performance is reduced, indicating\nthat DetectGPT ']","DetectGPT shows discriminative improvement over existing methods, notably on the XSum dataset for fake news detection, with significant AUROC improvements.","DetectGPT's performance varies across different datasets and models in zero-shot detection scenarios. When the surrogate model is different from the source model, detection performance is reduced. However, DetectGPT consistently provides the most accurate detection across various datasets and models, ranging from 1.5B parameter GPT-2 to 20B parameter GPT-NeoX. It generally provides the most accurate performance, although the gap is narrowed compared to direct sampling, and it typically outperforms other criteria in terms of average AUROC.",1.0,1.0,0.49285537004470825
How does DistilBERT's performance on the GLUE benchmark compare to BERT and ELMo?,"['yields to comparable\nperformance on downstream tasks. Com-\nparison on downstream tasks: IMDb (test ac-\ncuracy) and SQuAD 1.1 (EM/F1 on dev set).\nD: with a second step of distillation during\nﬁne-tuning.\nModel IMDb SQuAD\n(acc.) (EM/F1)\nBERT-base 93.46 81.2/88.5\nDistilBERT 92.82 77.7/85.8\nDistilBERT (D) - 79.1/86.9Table 3: DistilBERT is signiﬁcantly smaller\nwhile being constantly faster. Inference\ntime of a full pass of GLUE task STS-B (sen-\ntiment analysis) on CPU with a batch size of\n1.\nModel # param. Inf. time\n(Millions) (seconds)\nELMo 180 895\nBERT-base 110 668\nDistilBERT 66 410\nDistillation We applied best practices for training BERT model recently proposed in Liu et al. [2019].\nAs such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K\nexamples per batch) using dynamic masking and without the next sentence prediction objective.\nData and compute power We train DistilBERT on the same corpus as the original BERT model:\na concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT\nwas trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the\nRoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100.\n4 Experiments\nGeneral Language Understanding We assess the language understanding and generalization ca-\npabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems.\nWe report scores on the development sets for each task by ﬁne-tuning DistilBERT without the use\nof ensembling or multi-tasking scheme for ﬁne-tuning (which are mostly orthogonal to the present\nwork). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters\net al. [2018]) encoder followed ', '56.0 75.1\nBERT BASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\nBERT LARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\nTable 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard ).\nThe number below each task denotes the number of training examples. The “Average” column is slightly different\nthan the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8BERT and OpenAI GPT are single-\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\nWe use a batch size of 32 and ﬁne-tune for 3\nepochs over the data for all GLUE tasks. For each\ntask, we selected the best ﬁne-tuning learning rate\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\nAdditionally, for BERT LARGE we found that ﬁne-\ntuning was sometimes unstable on small datasets,\nso we ran several random restarts and selected the\nbest model on the Dev set. With random restarts,\nwe use the same pre-trained checkpoint but per-\nform different ﬁne-tuning data shufﬂing and clas-\nsiﬁer layer initialization.9\nResults are presented in Table 1. Both\nBERT BASE and BERT LARGE outperform all sys-\ntems on all tasks by a substantial margin, obtaining\n4.5% and 7.0% respective average accuracy im-\nprovement over the prior state of the art. Note that\nBERT BASE and OpenAI GPT are nearly identical\nin terms of model architecture apart from the at-\ntention masking. For the largest and most widely\nreported GLUE task, MNLI, BERT obtains a 4.6%\nabsolute accuracy improvement. On the ofﬁcial\nGLUE leaderboard10, BERT LARGE obtains a score\nof ', '65.1 0.0 100.0/ 50.0\nCBoW 44.3 62.1 49.0/71.2 51.6 0.0 / 0.4 14.0/13.6 49.7 53.0 65.1 -0.4 100.0/ 50.0\nBERT 69.0 77.4 75.7/83.6 70.6 70.0 / 24.0 72.0/71.3 71.6 69.5 64.3 23.0 97.8 / 51.7\nBERT++ 71.5 79.0 84.7/90.4 73.8 70.0 / 24.1 72.0/71.3 79.0 69.5 64.3 38.0 99.4 / 51.4\nOutside Best - 80.4 - / - 84.4 70.4 */24.5*74.8/73.0 82.7 - - - - / -\nHuman (est.) 89.8 89.0 95.8/98.9 100.0 81.8*/51.9* 91.7/91.3 93.6 80.0 100.0 77.0 99.3 / 99.7\n5 Experiments\n5.1 Baselines\nBERT Our main baselines are built around BERT, variants of which are among the most successful\napproach on GLUE at the time of writing. Speciﬁcally, we use the bert-large-cased variant.\nFollowing the practice recommended in Devlin et al. (2019), for each task, we use the simplest\npossible architecture on top of BERT. We ﬁne-tune a copy of the pretrained BERT model separately\nfor each task, and leave the development of multi-task learning models to future work. For training,\nwe use the procedure speciﬁed in Devlin et al. (2019): We use Adam (Kingma and Ba, 2014) with an\ninitial learning rate of 10−5and ﬁne-tune for a maximum of 10 epochs.\nFor classiﬁcation tasks with sentence-pair inputs (BoolQ, CB, RTE, WiC), we concatenate the\nsentences with a [SEP]token, feed the fused input to BERT, and use a logistic regression classiﬁer\nthat sees the representation corresponding to [CLS]. For WiC, we also concatenate the representation\nof the marked word. For COPA, MultiRC, and ReCoRD, for each answer choice, we similarly\nconcatenate the context with that answer choice and feed the resulting sequence into BERT to produce\nan answer representation. For COPA, we ']","DistilBERT achieves performance close to BERT, outperforming ELMo across GLUE benchmark tasks, demonstrating significant efficiency and effectiveness.",DistilBERT's performance on the GLUE benchmark is not directly compared to BERT and ELMo in the provided context.,1.0,0.0,0.826376736164093
How does DistilBERT's performance on downstream tasks like IMDb sentiment classification and SQuAD v1.1 compare to BERT?,"['by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of\nindividual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo\nbaseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to\nBERT, retaining 97% of the performance with 40% fewer parameters.\n4.1 Downstream task benchmark\nDownstream tasks We further study the performances of DistilBERT on several downstream tasks\nunder efﬁcient inference constraints: a classiﬁcation task (IMDb sentiment classiﬁcation - Maas et al.\n[2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]).\nAs shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb\nbenchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT.\nWe also studied whether we could add another step of distillation during the adaptation phase by\nﬁne-tuning DistilBERT on SQuAD using a BERT model previously ﬁne-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\n3\x0cTable 4: Ablation study. Variations are relative to the model trained with triple loss and teacher\nweights initialization.\nAblation Variation on GLUE macro-score\n∅-Lcos-Lmlm -2.96\nLce-∅-Lmlm -1.46\nLce-Lcos-∅ -0.31\nTriple loss + random weights initialization -3.69\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two\nsuccessive steps of distillation, one during the pre-training phase and one during the adaptation phase.\nIn this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and\n70.4 EM, i.e. within 3 points of the full model.\nSize and ', 'yields to comparable\nperformance on downstream tasks. Com-\nparison on downstream tasks: IMDb (test ac-\ncuracy) and SQuAD 1.1 (EM/F1 on dev set).\nD: with a second step of distillation during\nﬁne-tuning.\nModel IMDb SQuAD\n(acc.) (EM/F1)\nBERT-base 93.46 81.2/88.5\nDistilBERT 92.82 77.7/85.8\nDistilBERT (D) - 79.1/86.9Table 3: DistilBERT is signiﬁcantly smaller\nwhile being constantly faster. Inference\ntime of a full pass of GLUE task STS-B (sen-\ntiment analysis) on CPU with a batch size of\n1.\nModel # param. Inf. time\n(Millions) (seconds)\nELMo 180 895\nBERT-base 110 668\nDistilBERT 66 410\nDistillation We applied best practices for training BERT model recently proposed in Liu et al. [2019].\nAs such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K\nexamples per batch) using dynamic masking and without the next sentence prediction objective.\nData and compute power We train DistilBERT on the same corpus as the original BERT model:\na concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT\nwas trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the\nRoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100.\n4 Experiments\nGeneral Language Understanding We assess the language understanding and generalization ca-\npabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems.\nWe report scores on the development sets for each task by ﬁne-tuning DistilBERT without the use\nof ensembling or multi-tasking scheme for ﬁne-tuning (which are mostly orthogonal to the present\nwork). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters\net al. [2018]) encoder followed ', 'inference speed\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number\nof parameters of each model along with the inference time needed to do a full pass on the STS-\nB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1.\nDistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT.\nOn device computation We studied whether DistilBERT could be used for on-the-edge applications\nby building a mobile application for question answering. We compare the average inference time on\na recent smartphone (iPhone 7 Plus) against our previously trained question answering model based\non BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole\nmodel weighs 207 MB (which could be further reduced with quantization). Our code is available5.\n4.2 Ablation study\nIn this section, we investigate the inﬂuence of various components of the triple loss and the student\ninitialization on the performances of the distilled model. We report the macro-score on GLUE. Table 4\npresents the deltas with the full triple loss: removing the Masked Language Modeling loss has little\nimpact while the two distillation losses account for a large portion of the performance.\n5 Related work\nTask-speciﬁc distillation Most of the prior works focus on building task-speciﬁc distillation se-\ntups. Tang et al. [2019] transfer ﬁne-tune classiﬁcation model BERT to an LSTM-based classiﬁer.\nChatterjee [2019] distill BERT model ﬁne-tuned on SQuAD in a smaller Transformer model previ-\nously initialized from BERT. In the present work, we found it beneﬁcial to use a general-purpose\npre-training distillation rather than a task-speciﬁc distillation. Turc et al. [2019] use ']","DistilBERT closely approaches BERT's performance on IMDb sentiment classification and SQuAD v1.1, with minimal performance loss despite its smaller size.","DistilBERT's performance on downstream tasks like IMDb sentiment classification and SQuAD v1.1 is slightly behind BERT in test accuracy on IMDb, but within 3.9 points of BERT on SQuAD.",1.0,1.0,0.8529371619224548
"What modifications to the BERT pretraining process are introduced in RoBERTa, and how do they collectively enhance model performance?","['RoBERTa is trained with dynamic mask-\ning (Section 4.1),FULL -SENTENCES without NSP\nloss (Section 4.2), large mini-batches (Section 4.3)\nand a larger byte-level BPE (Section 4.4).\nAdditionally, we investigate two other impor-\ntant factors that have been under-emphasized in\nprevious work: (1) the data used for pretraining,\nand (2) the number of training passes through the\ndata. For example, the recently proposed XLNet\narchitecture ( Yang et al. ,2019 ) is pretrained us-\ning nearly 10 times more data than the original\nBERT ( Devlin et al. ,2019 ). It is also trained with\na batch size eight times larger for half as many op-\ntimization steps, thus seeing four times as many\nsequences in pretraining compared to BERT.\nTo help disentangle the importance of these fac-\ntors from other modeling choices (e.g., the pre-\ntraining objective), we begin by training RoBERTa\nfollowing the BERT LARGE architecture ( L= 24 ,\nH= 1024 ,A= 16 , 355M parameters). We\npretrain for 100K steps over a comparable B OOK -\nCORPUS plus W IKIPEDIA dataset as was used in\x0cModel data bsz stepsSQuADMNLI-m SST-2(v1.1/2.0)\nRoBERTa\nwith B OOKS + W IKI 16GB 8K 100K 93.6/87.3 89.0 95.3\n+ additional data ( §3.2) 160GB 8K 100K 94.0/87.7 89.3 95.6\n+ pretrain longer 160GB 8K 300K 94.4/88.7 90.0 96.1\n+ pretrain even longer 160GB 8K 500K 94.6/89.4 90.2 96.4\nBERT LARGE\nwith B OOKS + W IKI 13GB 256 1M 90.9/81.8 86.6 93.7\nXLNet LARGE\nwith B OOKS + W IKI 13GB 256 1M 94.0/87.8 88.4 94.4\n+ additional data 126GB 2K 500K 94.5/88.8 89.8 95.6\nTable 4: Development set results for RoBERTa as we pretrain o ver more data (16GB →160GB of text) and pretrain\nfor longer (100K →300K→500K steps). Each row ', 'that we only report single-task ﬁne-tuning results\nin this paper. A multitask ﬁne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n15https://gluebenchmark.com/faq\x0cjority class.\nC Additional Ablation Studies\nC.1 Effect of Number of Training Steps\nFigure 5 presents MNLI Dev accuracy after ﬁne-\ntuning from a checkpoint that has been pre-trained\nforksteps. This allows us to answer the following\nquestions:\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh ﬁne-tuning accuracy?\nAnswer: Yes, BERT BASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\nC.2 Ablation for Different Masking\nProcedures\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n200 400 600 800 1,0007678808284\nPre-training Steps (Thousands)MNLI Dev Accuracy\nBERT BASE (Masked LM)\nBERT BASE (Left-to-Right)\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after ﬁne-tuning, starting\nfrom model parameters that have been pre-trained for\nksteps. The x-axis is the value of k.Note that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand ﬁne-tuning, as the [MASK] symbol never ap-\npears during the ﬁne-tuning stage. We report the\nDev results for both MNLI and NER. For ', '/ 87.4 86.5 (89.0 / 85.5)\nXLNet (Yang et al., 2019) 2 90.8 / 90.8 92.3 95.1 / 89.7 90.6 / 87.9 85.4 (88.6 / 84.0)\nMegatron-336M 1 89.7 / 90.0 92.3 94.2 / 88.0 88.1 / 84.8 83.0 (86.9 / 81.5)\nMegatron-1.3B 1 90.9 / 91.0 92.6 94.9 / 89.1 90.2 / 87.1 87.3 (90.4 / 86.1)\nMegatron-3.9B 1 91.4 / 91.4 92.7 95.5 / 90.0 91.2 / 88.5 89.5 (91.8 / 88.6)\nALBERT ensemble (Lan et al., 2019) 95.5 / 90.1 91.4 / 88.9 89.4 (91.2 / 88.6)\nMegatron-3.9B ensemble 95.8 / 90.5 91.7 / 89.0 90.9 (93.1 / 90.0)\nFigure 7. Training loss for BERT model using the original architec-\nture (a) and the rearranged architecture (b). Left ﬁgure shows the\ntraining loss for 336M and 752M BERT model. While the original\narchitecture performs well on the 336M model, the modiﬁcations\nin (b) enable stable training with lower training loss.\nUsing the architecture change in Figure 7(b), we consider\nthree different cases as detailed in Table 4. The 336M model\nhas the same size as BERT-large. The 1.3B is the same as\nthe BERT-xlarge conﬁguration that was previously shown\nto get worse results than the 336M BERT-large model (Lan\net al., 2019). We further scale the BERT model using both\nlarger hidden size as well as more layers to arrive at the 3.9B\nparameter case. In all cases, the hidden size per attention\nhead is kept constant at 64. 336M and 1.3B models are\ntrained for 2 million iterations while the 3.9B model is\ntrained for 1.5 million iterations and is still training.\nOn a 3% held-out set, 336M, 1.3B, and 3.9B models achieve\nvalidation set perplexity ']","RoBERTa introduces several key modifications, including training with larger mini-batches, eliminating the next sentence prediction (NSP) loss, training on longer sequences, dynamically changing the masking pattern, and pretraining over more data. These improvements collectively enable RoBERTa to match or exceed the performance of all post-BERT models, achieving state-of-the-art results on benchmarks like GLUE, RACE, and SQuAD.","RoBERTa introduces dynamic masking, full-sentences without NSP loss, large mini-batches, and a larger byte-level BPE in the pretraining process. These modifications collectively enhance model performance by improving the pretraining process, allowing for better representation learning and ultimately leading to better fine-tuning results.",0.8,1.0,0.8337115049362183
"What role does the novel dataset CC-NEWS play in RoBERTa's pretraining, and how does it compare to other datasets used?","['0.98\nGradient Clipping 0.0 0.0\nTable 9: Hyperparameters for pretraining RoBERTa LARGE and RoBERTa BASE.\nHyperparam RACE SQuAD GLUE\nLearning Rate 1e-5 1.5e-5 {1e-5, 2e-5, 3e-5 }\nBatch Size 16 48 {16, 32}\nWeight Decay 0.1 0.01 0.1\nMax Epochs 4 2 10\nLearning Rate Decay Linear Linear Linear\nWarmup ratio 0.06 0.06 0.06\nTable 10: Hyperparameters for ﬁnetuning RoBERTa LARGE on RACE, SQuAD and GLUE.', 'RoBERTa is trained with dynamic mask-\ning (Section 4.1),FULL -SENTENCES without NSP\nloss (Section 4.2), large mini-batches (Section 4.3)\nand a larger byte-level BPE (Section 4.4).\nAdditionally, we investigate two other impor-\ntant factors that have been under-emphasized in\nprevious work: (1) the data used for pretraining,\nand (2) the number of training passes through the\ndata. For example, the recently proposed XLNet\narchitecture ( Yang et al. ,2019 ) is pretrained us-\ning nearly 10 times more data than the original\nBERT ( Devlin et al. ,2019 ). It is also trained with\na batch size eight times larger for half as many op-\ntimization steps, thus seeing four times as many\nsequences in pretraining compared to BERT.\nTo help disentangle the importance of these fac-\ntors from other modeling choices (e.g., the pre-\ntraining objective), we begin by training RoBERTa\nfollowing the BERT LARGE architecture ( L= 24 ,\nH= 1024 ,A= 16 , 355M parameters). We\npretrain for 100K steps over a comparable B OOK -\nCORPUS plus W IKIPEDIA dataset as was used in\x0cModel data bsz stepsSQuADMNLI-m SST-2(v1.1/2.0)\nRoBERTa\nwith B OOKS + W IKI 16GB 8K 100K 93.6/87.3 89.0 95.3\n+ additional data ( §3.2) 160GB 8K 100K 94.0/87.7 89.3 95.6\n+ pretrain longer 160GB 8K 300K 94.4/88.7 90.0 96.1\n+ pretrain even longer 160GB 8K 500K 94.6/89.4 90.2 96.4\nBERT LARGE\nwith B OOKS + W IKI 13GB 256 1M 90.9/81.8 86.6 93.7\nXLNet LARGE\nwith B OOKS + W IKI 13GB 256 1M 94.0/87.8 88.4 94.4\n+ additional data 126GB 2K 500K 94.5/88.8 89.8 95.6\nTable 4: Development set results for RoBERTa as we pretrain o ver more data (16GB →160GB of text) and pretrain\nfor longer (100K →300K→500K steps). Each row ', 'and\nYejin Choi. 2019. Defending against neural fake\nnews. arXiv preprint arXiv:1905.12616 .\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watch-\ning movies and reading books. In arXiv preprint\narXiv:1506.06724 .\nAppendix for “RoBERTa: A Robustly\nOptimized BERT Pretraining Approach”\nA Full results on GLUE\nIn Table 8we present the full set of development\nset results for RoBERTa. We present results for\naLARGE conﬁguration that follows BERT LARGE ,\nas well as a BASE conﬁguration that follows\nBERT BASE.B Pretraining Hyperparameters\nTable 9describes the hyperparameters for pre-\ntraining of RoBERTa LARGE and RoBERTa BASE\nC Finetuning Hyperparameters\nFinetuning hyperparameters for RACE, SQuAD\nand GLUE are given in Table 10. We select the\nbest hyperparameter values based on the median\nof 5 random seeds for each task.\x0cMNLI QNLI QQP RTE SST MRPC CoLA STS\nRoBERTa BASE\n+ all data + 500k steps 87.6 92.8 91.9 78.7 94.8 90.2 63.6 91.2\nRoBERTa LARGE\nwith B OOKS + W IKI 89.0 93.9 91.9 84.5 95.3 90.2 66.3 91.6\n+ additional data ( §3.2) 89.3 94.0 92.0 82.7 95.6 91.4 66.1 92.2\n+ pretrain longer 300k 90.0 94.5 92.2 83.3 96.1 91.1 67.4 92.3\n+ pretrain longer 500k 90.2 94.7 92.2 86.6 96.4 90.9 68.0 92.4\nTable 8: Development set results on GLUE tasks for various co nﬁgurations of RoBERTa.\nHyperparam RoBERTa LARGE RoBERTa BASE\nNumber of Layers 24 12\nHidden size 1024 768\nFFN inner hidden size 4096 3072\nAttention heads 16 12\nAttention head size 64 64\nDropout 0.1 0.1\nAttention Dropout 0.1 0.1\nWarmup Steps 30k 24k\nPeak Learning Rate 4e-4 6e-4\nBatch Size 8k 8k\nWeight Decay 0.01 0.01\nMax Steps 500k 500k\nLearning Rate Decay Linear Linear\nAdamǫ 1e-6 1e-6\nAdamβ1 0.9 0.9\nAdamβ2 0.98 ']","CC-NEWS, a large dataset collected from English news articles, provides a significant source of diverse and recent textual content for RoBERTa's pretraining, complementing other datasets and ensuring a broad coverage of language use and topics. This dataset helps control for training set size effects and contributes to the robustness and generalization capability of the model.","The novel dataset CC-NEWS does not play a role in RoBERTa's pretraining. Instead, RoBERTa is pretrained with a comparable BOOK-CORPUS plus WIKIPEDIA dataset, which is the same dataset used in BERT's pretraining. The dataset CC-NEWS is not mentioned in the context.",1.0,1.0,0.7390231490135193
Describe the process and purpose of the 'Next Sentence Prediction' task in BERT's pre-training.,"['and replacement is performed once in the be-\nginning and saved for the duration of training, al-\nthough in practice, data is duplicated so the mask\nis not always the same for every training sentence\n(see Section 4.1).\nNext Sentence Prediction (NSP) NSP is a bi-\nnary classiﬁcation loss for predicting whether two\nsegments follow each other in the original text.\nPositive examples are created by taking consecu-\ntive sentences from the text corpus. Negative ex-\namples are created by pairing segments from dif-\nferent documents. Positive and negative examples\nare sampled with equal probability.\nThe NSP objective was designed to improve\nperformance on downstream tasks, such as Natural\nLanguage Inference ( Bowman et al. ,2015 ), which\nrequire reasoning about the relationships between\npairs of sentences.\n2.4 Optimization\nBERT is optimized with Adam ( Kingma and Ba ,\n2015 ) using the following parameters: β1= 0.9,\nβ2= 0.999,ǫ=1e-6 and L2weight de-\ncay of0.01. The learning rate is warmed up\nover the ﬁrst 10,000 steps to a peak value of\n1e-4, and then linearly decayed. BERT trains\nwith a dropout of 0.1 on all layers and at-\ntention weights, and a GELU activation func-\ntion ( Hendrycks and Gimpel ,2016 ). Models are\npretrained for S=1,000,000 updates, with mini-\nbatches containing B=256 sequences of maxi-\nmum length T=512 tokens.\n2.5 Data\nBERT is trained on a combination of B OOK COR-\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\nwhich totals 16GB of uncompressed text.3\n3 Experimental Setup\nIn this section, we describe the experimental setup\nfor our replication study of BERT.\n3.1 Implementation\nWe reimplement BERT in FAIRSEQ (Ott et al. ,\n2019 ). We primarily follow the original BERT\n3Yang et al. (2019 ) use the same dataset but report having\nonly 13GB ', 'token is chosen, we replace\nthei-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\nTiwill be used to predict the original token with\ncross entropy loss. We compare variations of this\nprocedure in Appendix C.2.\nTask #2: Next Sentence Prediction (NSP)\nMany important downstream tasks such as Ques-\ntion Answering (QA) and Natural Language Infer-\nence (NLI) are based on understanding the rela-\ntionship between two sentences, which is not di-\nrectly captured by language modeling. In order\nto train a model that understands sentence rela-\ntionships, we pre-train for a binarized next sen-\ntence prediction task that can be trivially gener-\nated from any monolingual corpus. Speciﬁcally,\nwhen choosing the sentences AandBfor each pre-\ntraining example, 50% of the time Bis the actual\nnext sentence that follows A(labeled as IsNext ),\nand 50% of the time it is a random sentence from\nthe corpus (labeled as NotNext ). As we show\nin Figure 1, Cis used for next sentence predic-\ntion (NSP).5Despite its simplicity, we demon-\nstrate in Section 5.1 that pre-training towards this\ntask is very beneﬁcial to both QA and NLI.6\n5The ﬁnal model achieves 97%-98% accuracy on NSP.\n6The vector Cis not a meaningful sentence representation\nwithout ﬁne-tuning, since it was trained with NSP.\x0c[CLS] helikesplay## ing[SEP] mydogiscute[SEP]Input E[CLS] Ehe Elikes Eplay E## ing E[SEP] Emy Edog Eis Ecute E[SEP] Token Embeddings EA EB EB EB EB EB EA EA EA EA EASegment Embeddings E0 E6 E7 E8 E9 E10 E1 E2 E3 E4 E5Position Embeddings Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, ', 'SST-2. Reported results are\nmedians over 5 random initializations (seeds). Refer-\nence results are from Yang et al. (2019 ).\nResults Table 1compares the published\nBERT BASE results from Devlin et al. (2019 ) to our\nreimplementation with either static or dynamic\nmasking. We ﬁnd that our reimplementation\nwith static masking performs similar to the\noriginal BERT model, and dynamic masking is\ncomparable or slightly better than static masking.\nGiven these results and the additional efﬁciency\nbeneﬁts of dynamic masking, we use dynamic\nmasking in the remainder of the experiments.\n4.2 Model Input Format and Next Sentence\nPrediction\nIn the original BERT pretraining procedure, the\nmodel observes two concatenated document seg-\nments, which are either sampled contiguously\nfrom the same document (with p= 0.5) or from\ndistinct documents. In addition to the masked lan-\nguage modeling objective, the model is trained to\npredict whether the observed document segments\ncome from the same or distinct documents via an\nauxiliary Next Sentence Prediction (NSP) loss.\nThe NSP loss was hypothesized to be an impor-\ntant factor in training the original BERT model.\nDevlin et al. (2019 ) observe that removing NSP\nhurts performance, with signiﬁcant performance\ndegradation on QNLI, MNLI, and SQuAD 1.1.\nHowever, some recent work has questioned the\nnecessity of the NSP loss ( Lample and Conneau ,\n2019 ;Yang et al. ,2019 ;Joshi et al. ,2019 ).\nTo better understand this discrepancy, we com-\npare several alternative training formats:\n•SEGMENT -PAIR +NSP: This follows the original\ninput format used in BERT ( Devlin et al. ,2019 ),\nwith the NSP loss. Each input has a pair of seg-\nments, which can each contain multiple natural\nsentences, but the total combined length must\nbe less than 512 tokens.\x0cModel SQuAD 1.1/2.0 MNLI-m SST-2 RACE\nOur reimplementation (with ']","NSP involves predicting whether a sentence B is the actual next sentence that follows sentence A, facilitating understanding of sentence relationships.","The Next Sentence Prediction (NSP) task in BERT's pre-training is a binary classification task that aims to predict whether two segments of text follow each other in the original text. Positive examples are generated by taking consecutive sentences from the text corpus, while negative examples are created by pairing segments from different documents. The purpose of the NSP task is to improve the model's performance on downstream tasks, such as Natural Language Inference, by training it to reason about the relationships between pairs of sentences.",1.0,1.0,0.6753357648849487
"What performance improvements does LLaMA-13B show over GPT-3, and how does LLaMA-65B stand in comparison to Chinchilla-70B and PaLM-540B?","['and conﬁgurations listed in Table 2. The 355M\nmodel is equivalent in size and conﬁguration of BERT-Large\nmodel (Devlin et al., 2018). The 2.5B model is bigger than\nthe previous largest GPT-2 model, and the 8.3B model is\nlarger than any left-to-right transformer language model\never trained, to the best of our knowledge. To train and eval-\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 2. Model conﬁgurations used for GPT-2.\nHidden Time\nParameter Layers Hidden Attn Size Total per\nCount Size Heads per GPUs Epoch\nHead (days)\n355M 24 1024 16 64 64 0.86\n2.5B 54 1920 20 96 128 2.27\n8.3B 72 3072 24 128 512 2.10\nTable 3. Zero-shot results. SOTA are from (Khandelwal et al.,\n2019) for Wikitext103 and (Radford et al., 2019) for LAMBADA.\nModel Wikitext103 LAMBADA\nPerplexity↓ Accuracy↑\n355M 19.31 45.18%\n2.5B 12.76 61.73%\n8.3B 10.81 66.51%\nPrevious SOTA 15.79 63.24%\nuate our language models we use the procedure described in\nsection 4. Table 2 also lists the time it takes to advance one\nepoch which is equivalent to 68,507 iterations. For example,\nfor the 8.3B model on 512 GPUs, each epoch takes around\ntwo days. Compared to the conﬁgurations used for our scal-\ning studies in Table 1, the 2.5B model is the same, the 8.3B\nmodel has 24 attention heads instead of 32, and the 355M is\nmuch smaller than any seen previously while still using 64\nGPUs to train, leading to the much lower time per epoch.\nFigure 6 shows validation perpelixity as a function of num-\nber of iterations. As the model size increases, the validation\nperpelixity decreases and reaches a validation perplexity of\n9.27 for the 8.3B model. We report the zero-shot evaluation\nof the trained models on the ', 'times, but halfway through also\nsay quack” ).5. Results\n5.1. Math Results\nTable 1 shows the following results: across all tasks,\nPALusing Codex sets a new few-shot state-of-the-art top-\n1 decoding across all datasets, outperforming COTCodex,\nCOTPaLM-540 B, and COTMinerva 540B which was ﬁne-tuned\non explicit mathematical content.\nInterestingly, COTalso beneﬁts from Codex over PaLM-\n540 Bin some of the datasets such as ASDIV , but performs\nworse than PaLM-540 Bin others such as SVAMP . Yet,\nusing PALfurther improves the solve rate across all datasets.\nGSM-HARD On GSM -HARD (Table 1), the accuracy of\nDIRECT drops dramatically from 19.7% to 5.0% (a relative\ndrop of 74%), the accuracy of COTdrops from 65.6% to\n20.1% (a relative drop of almost 70%), while PALremains\nstable at 61.5%, dropping by only 14.3%. The results of\nCOTonGSM-HARD did not improve even when we replaced\nits prompts with prompts that include large numbers (Ap-\npendix B). This shows how PALprovides not only better\x0cPAL: Program-aided Language Models 6\nCOLORED OBJECT PENGUINS DATE REPEAT COPY OBJECT COUNTING\nDIRECT Codex 75.7 71.1 49.9 81.3 37.6\nCOTLaMDA-137B - - 26.8 - -\nCOTPaLM-540 B - 65.1 65.3 - -\nCOTCodex 86.3 79.2 64.8 68.8 73.0\nPALCodex 95.1 93.3 76.2 90.6 96.7\nTable 2: Solve rate on three symbolic reasoning datasets and two algorithmic datasets, In all datasets, PALachieves a much\nhigher accuracy than chain-of-thought. Results with closed models LaMDA-137B and PaLM-540B are included if available\nto public (Wei et al., 2022; Suzgun et al., 2022).\nresults on the standard benchmarks, but is also much more\nrobust . In fact, since PALofﬂoads the computation to the\nPython interpreter, any complex computation can be per-\nformed accurately given the correctly generated program.\nLarge Numbers or Incorrect Reasoning? Are the ', 'so too is\nend-to-end ﬁnetuning. Freezing BERT-Base and\nadding an LSTM on top lowers its overall perfor-\nmance 4.3%. This may help explain why mod-\nels such as ESIM +ELMo struggled on SWAG, as\nELMo isn’t updated during ﬁnetuning.\nWhile BERT is the best model, it still struggles\nonHellaSwag , and especially so on zero-shot cat-\n9For ELMo and BERT-Base, the model learns scalar\nweights to combine each internal layer of the encoder.\n10This model is trained with binary cross entropy loss.\n6\x0cOverall LSMDC ActivityNet30405060708090100 BERT-Large Accuracy (%)86.7%85.5%88.0%\n71.4%69.0%74.2%Evaluated on SWAG\nOverall WikiHow ActivityNet34.6%\n28.0%48.4%46.4%\n42.9%53.7%Evaluated on HellaSwag\nTrained on...\nSWAG\nHellaSwagFigure 9: Transfer experiments from SWAG to Hella-\nSwag and vice versa, evaluated on the validation sets.\nOverall, a BERT-Large that is trained on SWAG hardly\ngeneralizes to HellaSwag : it scores 34.6%.\negories. Performance drops roughly 5% on the\ntest fold, which suggests that the ﬁnetuning is not\nenough for BERT to learn to generalize to novel\nactivities or how-to categories.\nLast, we see that WikiHow is a much harder do-\nmain that ActivityNet for machines: 45% Bert-\nLarge performance, versus 96.5% for humans.\nCuriously, it is on this source dataset that we see\nthe smallest gap between OpenAI GPT and BERT.\nIn fact, OpenAI GPT outperforms BERT on Wiki-\nHow, but the reverse is true for ActivityNet. One\npossibility is that the left-to-right structure of GPT\nis the right inductive bias for WikiHow - perhaps\nreasoning bidirectionally over long contexts is too\nmuch for a 12-layer transformer to learn.\n5.1 SWAG to HellaSwag transfer\nGiven the shared goals and partial domains of\nSWAG and HellaSwag , it is natural to ask to\nwhat extent models can transfer between the two\ndatasets. In Figure 9 we show the results from\ntransfer experiments: models are trained on ']","LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with Chinchilla-70B and PaLM-540B, showing that it can achieve comparable or better performance.",None.,1.0,0.5,0.12306821346282959
How does LLaMA's training data preprocessing and mixture differ from other large language models?,"['fastest to train but the\nfastest at inference, and although it may be cheaper\nto train a large model to reach a certain level of\n∗Equal contribution. Correspondence: {htouvron,\nthibautlav,gizacard,egrave,glample}@meta.com\n1https://github.com/facebookresearch/llamaperformance, a smaller one trained longer will\nultimately be cheaper at inference. For instance,\nalthough Hoffmann et al. (2022) recommends\ntraining a 10B model on 200B tokens, we ﬁnd\nthat the performance of a 7B model continues to\nimprove even after 1T tokens.\nThe focus of this work is to train a series of\nlanguage models that achieve the best possible per-\nformance at various inference budgets, by training\non more tokens than what is typically used. The\nresulting models, called LLaMA , ranges from 7B\nto 65B parameters with competitive performance\ncompared to the best existing LLMs. For instance,\nLLaMA-13B outperforms GPT-3 on most bench-\nmarks, despite being 10 ×smaller. We believe that\nthis model will help democratize the access and\nstudy of LLMs, since it can be run on a single GPU.\nAt the higher-end of the scale, our 65B-parameter\nmodel is also competitive with the best large lan-\nguage models such as Chinchilla or PaLM-540B.\nUnlike Chinchilla, PaLM, or GPT-3, we only\nuse publicly available data, making our work com-\npatible with open-sourcing, while most existing\nmodels rely on data which is either not publicly\navailable or undocumented (e.g. “Books – 2TB” or\n“Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022) and GLM (Zeng et al., 2022), but none\nthat are competitive with PaLM-62B or Chinchilla.\nIn the rest of this paper, we present an overview\nof the modiﬁcations we made to the transformer\narchitecture (Vaswani et al., 2017), as well as our\ntraining method. We ', '(Radford et al., 2019),\n10B-scale language models (Raffel et al., 2020; Shoeybi et al., 2019; Black et al., 2022), to 100B-\nscale GPT-3 (Brown et al., 2020). Later, despite many 100B-scale LLMs (Lieber et al., 2021; Thop-\npilan et al., 2022; Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022; Wu et al., 2021; Zeng\net al., 2021; Wang et al., 2021) in both English and Chinese, they are not available to public or only\naccessible via limited APIs. The closeness of LLMs severely stymies its development. GLM-130B’s\nefforts, along with recent ElutherAI, OPT-175B (Zhang et al., 2022), and BLOOM-176B (Scao et al.,\n2022), aim to offer high-quality open-sourced LLMs to our community.\nTransferring. Though fine-tuning has been a de facto way for transfer learning, the evaluation for\nLLMs has been focused on prompting and in-context learning due to their tremendous sizes (Brown\net al., 2020; Liu et al., 2021a). Nevertheless, some recent attempts has been on parameter-efficient\nlearning on language models (Houlsby et al., 2019) and prompt tuning (i.e., P-tuning, Li & Liang\n(2021); Liu et al. (2021b); Lester et al. (2021); Liu et al. (2022)). For now we do not focus on them\nand will leave the comprehensive testing of them on GLM-130B in future study.\nInference. Most public-accessible LLMs nowadays are providing their services via limited APIs.In\nthis work, an important part of our endeavor has been on LLMs’ efficient and fast inference. Related\nwork may include distillation (Sanh et al., 2019; Jiao et al., 2020; Wang et al., 2020), quantiza-\ntion (Zafrir et al., 2019; Shen et al., 2020; Tao et al., 2022), and pruning (Michel ', 'LLaMA: Open and Efﬁcient Foundation Language Models\nHugo Touvron∗, Thibaut Lavril∗, Gautier Izacard∗, Xavier Martinet\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\nEdouard Grave∗, Guillaume Lample∗\nMeta AI\nAbstract\nWe introduce LLaMA, a collection of founda-\ntion language models ranging from 7B to 65B\nparameters. We train our models on trillions\nof tokens, and show that it is possible to train\nstate-of-the-art models using publicly avail-\nable datasets exclusively, without resorting\nto proprietary and inaccessible datasets. In\nparticular, LLaMA-13B outperforms GPT-3\n(175B) on most benchmarks, and LLaMA-\n65B is competitive with the best models,\nChinchilla-70B and PaLM-540B. We release\nall our models to the research community1.\n1 Introduction\nLarge Languages Models (LLMs) trained on mas-\nsive corpora of texts have shown their ability to per-\nform new tasks from textual instructions or from a\nfew examples (Brown et al., 2020). These few-shot\nproperties ﬁrst appeared when scaling models to a\nsufﬁcient size (Kaplan et al., 2020), resulting in a\nline of work that focuses on further scaling these\nmodels (Chowdhery et al., 2022; Rae et al., 2021).\nThese efforts are based on the assumption that\nmore parameters will lead to better performance.\nHowever, recent work from Hoffmann et al. (2022)\nshows that, for a given compute budget, the best\nperformances are not achieved by the largest mod-\nels, but by smaller models trained on more data.\nThe objective of the scaling laws from Hoff-\nmann et al. (2022) is to determine how to best\nscale the dataset and model sizes for a particular\ntraining compute budget. However, this objective\ndisregards the inference budget, which becomes\ncritical when serving a language model at scale.\nIn this context, given a target level of performance,\nthe preferred model is not the ']","LLaMA's training data includes a mixture of sources like CommonCrawl, C4, GitHub, Wikipedia, and others, preprocessed with techniques like deduplication, language identification, and quality filtering, emphasizing the use of publicly available data.","LLaMA's training data preprocessing and mixture differ from other large language models because they train their models on trillions of tokens using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. This allows LLaMA to achieve competitive performance with models like GPT-3 and Chinchilla, even though they are smaller in size. Additionally, LLaMA emphasizes training on more data rather than increasing model size, which ultimately leads to better performance.",1.0,1.0,0.7543127536773682
How does the accuracy of the largest GPT-3 model compare to human professionals across the 57 tasks?,"['et al., 2014). Finally, we list the best known\nresult on each task as of May 2019, except on tasks which we recast (WSC), resplit (CB), or achieve\n6For ReCoRD, we predict the entity that has the highest F1 with the other entity options.\n8\x0cthe best known result (WiC). The outside results for COPA, MultiRC, and RTE are from Sap et al.\n(2019), Trivedi et al. (2019), and Liu et al. (2019d) respectively.\nHuman Performance Pilehvar and Camacho-Collados (2019), Khashabi et al. (2018), Nangia and\nBowman (2019), and Zhang et al. (2018) respectively provide estimates for human performance\non WiC, MultiRC, RTE, and ReCoRD. For the remaining tasks, including the diagnostic set, we\nestimate human performance by hiring crowdworker annotators through Amazon’s Mechanical Turk\nplatform to reannotate a sample of each test set. We follow a two step procedure where a crowd\nworker completes a short training phase before proceeding to the annotation phase, modeled after the\nmethod used by Nangia and Bowman (2019) for GLUE. See Appendix C for details.\n5.2 Results\nTable 3 shows results for all baselines. The most frequent class and CBOW baselines do not perform\nwell overall, achieving near chance performance for several of the tasks. Using BERT increases\nthe average SuperGLUE score by 25 points, attaining signiﬁcant gains on all of the benchmark\ntasks, particularly MultiRC, ReCoRD, and RTE. On WSC, BERT actually performs worse than\nthe simple baselines, likely due to the small size of the dataset and the lack of data augmentation.\nUsing MultiNLI as an additional source of supervision for BoolQ, CB, and RTE leads to a 2-5 point\nimprovement on all tasks. Using SWAG as ', 'many were mistakes that a human might make. For example,\none question it got wrong was “How many chromosomes do all human somatic cells contain?” The\ncorrect answer is 46, while few-shot GPT-3 predicted 23with conﬁdence 97.5%. This answer would\nhave been correct if the question asked about the number of pairs of chromosomes. Similarly, many\nof its other high conﬁdence mistakes were also correct answers to slightly different questions.\nA.3 F ORMAT SENSITIVITY\nWhile different question formatting choices often lead to similar GPT-3 accuracies, we ﬁnd that\nUniﬁedQA is more sensitive. UniﬁedQA’s input format is of the form\nQUESTION1 \\\\n (A) CHOICE1 (B) CHOICE2 (C) CHOICE3 (D) CHOICE4</s>\nwhere questions and choices are normalized and made lowercase. If we remove the </s> from the\ninput, accuracy declines by several percentage points.\n12\x0cPublished as a conference paper at ICLR 2021\n0-Shot 1-Shot 2-Shot 3-Shot 4-Shot 5-Shot\nNumber of Examples in Context3035404550Accuracy (%)\nGPT-3 Multitask Accuracy vs.\nNumber of Examples in Context\nFigure 10: As the number of few-shot instruction\nexamples increases, the accuracy monotonically\nincreases. Notably, zero-shot performance is only\nsomewhat lower than 5-shot accuracy.\n20 30 40 50 60 70\nConfidence (%)203040506070Accuracy (%)\nGPT-3 Few-Shot CalibrationFigure 11: While models are more calibrated in\na few-shot setting than a zero-shot setting, they\nare still miscalibrated, with gap between accuracy\nand conﬁdence reaching up to 14%. Here the\ncorrelation between conﬁdence and accuracy is\nr= 0.81, compared to r= 0.63in the zero-shot\nsetting.\nB T ESTDETAILS\nB.1 T ASK DESCRIPTIONS AND EXAMPLES\nWe provide analysis of question length and difﬁculty in Figure 12. We list all tasks and the topics\nthey test in Table 2. We also provide an example for each task starting with Figure 14.\n0 500 1000 ', '(1 1B) T0 (1 1B) T0++ (1 1B)0.50.550.60.650.70.750.80.850.9\ninstructive irrelevant mis-moderate mis-extreme null\nFigure 6: 16-shot accuracy of four large models on\nRTE. For GPT-3, there is no practical difference be-\ntween any template categories except null (not plotted\nbecause they are below 0.5). For T5, there is no prac-\ntical difference between instructive and irrelevant. For\nT0, there is no practical difference between instructive\nand irrelevant nor between instructive and misleading-\nmoderate. For T0++, there is no practical difference be-\ntween instructive and irrelevant nor between instructive\nand misleading-extreme.\n175B) perform only marginally above random, ex-\ncept the instruction-tuned T0. Thus, for our analysis\nof zero shot performance, we focus on T0. Figure 5\nshows that there is no practical difference between\nthe performance of T0 3B given instructive tem-\nplates and either category of misleading templates.\nT0 11B performs better, although it also shows no\npractical difference between misleading-moderate\nand instructive templates. Lastly, T0++ (trained on\nmore datasets than other T0 variants), is the onlymodel in this paper that shows statistically signiﬁ-\ncantly different performance across all categories\nof prompts. However, there remains the caveat that\nit still performs arguably too well in absolute terms\nwith pathological prompts, which we discuss in the\nnext section.\n4.3 Discussion\nRecall that a common assumption in the literature\nis that prompts require experts to clearly and cor-\nrectly describe the task at hand (§1). In contrast,\nTable 2 summarizes that, with the exception of\nT0++ at zero shots, all models perform essentially\nas well with some pathological prompts as they do\nwith proper prompts. Notably, despite being much\nlarger than its competitors, GPT-3 shows the same\npatterns of behaviors, suggesting that mere scaling\ndoes not address this issue. Meanwhile, the evi-\ndence from instruction tuning ']","he largest GPT-3 model achieves 43.9% accuracy on average, significantly below the expert-level performance, indicating that even the most advanced models require substantial improvements to reach human professional accuracy levels across these tasks.",None,1.0,0.0,0.10794641077518463
What can you say about the calibration of models like GPT-3 in relation to their confidence and accuracy on the multitask test?,"['many were mistakes that a human might make. For example,\none question it got wrong was “How many chromosomes do all human somatic cells contain?” The\ncorrect answer is 46, while few-shot GPT-3 predicted 23with conﬁdence 97.5%. This answer would\nhave been correct if the question asked about the number of pairs of chromosomes. Similarly, many\nof its other high conﬁdence mistakes were also correct answers to slightly different questions.\nA.3 F ORMAT SENSITIVITY\nWhile different question formatting choices often lead to similar GPT-3 accuracies, we ﬁnd that\nUniﬁedQA is more sensitive. UniﬁedQA’s input format is of the form\nQUESTION1 \\\\n (A) CHOICE1 (B) CHOICE2 (C) CHOICE3 (D) CHOICE4</s>\nwhere questions and choices are normalized and made lowercase. If we remove the </s> from the\ninput, accuracy declines by several percentage points.\n12\x0cPublished as a conference paper at ICLR 2021\n0-Shot 1-Shot 2-Shot 3-Shot 4-Shot 5-Shot\nNumber of Examples in Context3035404550Accuracy (%)\nGPT-3 Multitask Accuracy vs.\nNumber of Examples in Context\nFigure 10: As the number of few-shot instruction\nexamples increases, the accuracy monotonically\nincreases. Notably, zero-shot performance is only\nsomewhat lower than 5-shot accuracy.\n20 30 40 50 60 70\nConfidence (%)203040506070Accuracy (%)\nGPT-3 Few-Shot CalibrationFigure 11: While models are more calibrated in\na few-shot setting than a zero-shot setting, they\nare still miscalibrated, with gap between accuracy\nand conﬁdence reaching up to 14%. Here the\ncorrelation between conﬁdence and accuracy is\nr= 0.81, compared to r= 0.63in the zero-shot\nsetting.\nB T ESTDETAILS\nB.1 T ASK DESCRIPTIONS AND EXAMPLES\nWe provide analysis of question length and difﬁculty in Figure 12. We list all tasks and the topics\nthey test in Table 2. We also provide an example for each task starting with Figure 14.\n0 500 1000 ', '0.58 0.53 0.53 0.50 0.55 0.56 0.57 0.54 0.32 0.37 0.28 0.32 0.32 0.32\nDetectGPT 0.99 0.98 1.00 0.98 0.97 0.98 0.99 0.98 0.98 0.90 0.82* 0.94 1.00 0.99 0.99 0.97* 0.93 0.98\nDiff 0.04 0.04 0.04 0.05 0.08 0.05 0.01 0.02 0.04 0.00 -0.01 0.02 0.01 0.01 0.01 -0.01 -0.05 0.00\nTable 4. Nucleus (top- p) sampling evaluation with p= 0.96. AUROC for detecting samples from the given model on the given dataset for\nDetectGPT and four previously proposed criteria. Nucleus sampling generally makes detection easier for all methods, but DetectGPT still\nprovides the highest average AUROC. For WritingPrompts, however, the LogRank baseline performs as well as DetectGPT.\nXSum SQuAD WritingPrompts\nMethod GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg. GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg. GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg.\nlogp(x) 0.89 0.89 0.89 0.84 0.81 0.87 0.93 0.90 0.88 0.82 0.74 0.85 0.97 0.95 0.97 0.96 0.95* 0.96\nRank 0.79 0.77 0.77 0.75 0.73 0.76 0.84 0.82 0.80 0.80 0.75 0.80 0.87 0.84 0.83 0.82 0.81 0.83\nLogRank 0.92* 0.91* 0.93* 0.89* 0.85* 0.90* 0.96* 0.94* 0.92* 0.87* 0.79* 0.90* 0.98* 0.97* 0.98* 0.97 0.96 0.97\nEntropy 0.58 0.49 0.55 0.56 0.59 0.55 0.55 0.52 0.56 0.56 0.58 0.56 0.35 0.41 0.30 0.34 0.37 0.35\nDetectGPT 0.99 0.97 0.99 0.96 0.96 0.98 0.99 0.98 0.98 0.89 0.80 0.93 0.99 0.98 0.99 0.97 0.93 0.97\nDiff 0.07 0.06 0.06 0.07 0.11 0.08 0.03 0.04 0.06 0.02 0.01 0.03 0.01 0.01 0.01 0.00 -0.03 0.00\nTable 5. Top-ksampling evaluation with k= 40 . DetectGPT generally provides the most accurate performance (highest AUROC),\nalthough the gap is narrowed comparing to direct sampling, ', '57tasks. On the right are UniﬁedQA\ntransfer accuracies for all of the 57tasks. For both models, capabilities are lopsided.\nA.1 A NALYSIS WITH MORE FINE-TUNED MODELS\nWe primarily analyzed models with more than 10billion parameters in the main body of the paper.\nFor this section, we analyze smaller models including RoBERTa-base (125 million parameters) (Liu\n11\x0cPublished as a conference paper at ICLR 2021\net al., 2019), ALBERT-xxlarge (223 million parameters) (Lan et al., 2020), and GPT-2 (1,558 million\nparameters) (Radford et al., 2019). Models are ﬁne-tuned to predict one of four classes using the\nUniﬁedQA MCQ questions and using our dev+val set. We test on our multitask test set.\nWe observe that these smaller models can attain better-than-random accuracy. RoBERTa-base attains\nan overall accuracy of 27.9%, with 27.9%accuracy for the humanities, 28.8%for social sciences,\n27.0%for STEM, and 27.7%for other. ALBERT-xxlarge attains an accuracy of 27.1%, with 27.2%\naccuracy for the humanities, 25.7%for the social sciences, 27.7%for STEM, and 27.9%for other.\nGPT-2 attains an accuracy of 32.4%, with 32.8%accuracy for the humanities, 33.3%for the social\nsciences, 30.2%for STEM, and 33.1%for other.\nCompare this to UniﬁedQA’s smallest variant, which has just 60million parameters and approximately\n29.3%accuracy. It obtains higher accuracy than RoBERTa and ALBERT, even though it has fewer\nparameters. This suggests that its larger pretraining dataset enables higher accuracy. Likewise,\nUniﬁedQA with 3billion parameters attains 43.7%, while the similarly sized GPT-2 model with 1.5\nbillion parameters attains 32.4%accuracy. This again suggests that T5’s larger pretraining dataset\nsize (and therefore UniﬁedQA’s pretraining dataset size) can increase accuracy.\nA.2 E RROR ANALYSIS\nWe qualitatively analyze when GPT-3 makes high conﬁdence mistakes. We ﬁnd that while many of\nthese mistakes were clearly wrong, ']","Models like GPT-3 are poorly calibrated, with their confidence often not accurately reflecting their actual performance across different subjects, indicating a need for improvement in model calibration techniques.","The calibration of models like GPT-3 in relation to their confidence and accuracy on the multitask test shows that while models are more calibrated in a few-shot setting compared to a zero-shot setting, they are still miscalibrated. There is a gap between accuracy and confidence reaching up to 14%, with a correlation between confidence and accuracy of r=0.81.",1.0,1.0,0.7677217721939087
"Without needing training on a separate classifier, how does DetectGPT determine if a passage was generated by an LLM?","['both top- k\nand nucleus sampling make detection easier, on average.\nAveraging across domains, DetectGPT provides the clearest\nsignal for zero-shot detection.\nDetection when the source model is unknown. While\nour experiments have focused on the white-box setting\nfor machine-generated text detection, in this section, we\nGPT-J GPT-Neo GPT-2Scoring ModelGPT-J GPT-Neo GPT-2Base Model0.92\n(0.02)0.83\n(0.04)0.79\n(0.02)\n0.64\n(0.06)0.97\n(0.01)0.83\n(0.02)\n0.60\n(0.09)0.85\n(0.05)0.99\n(0.00)0.85\n0.81\n0.81\n0.72 0.88 0.87\nFigure 6. DetectGPT performs\nbest when scoring samples\nwith the same model that gen-\nerated them (diagonal), but\nthe column means suggest that\nsome models (GPT-Neo, GPT-\n2) may be better ‘scorers’ than\nothers (GPT-J). White values\nshow mean (standard error)\nAUROC over XSum, SQuAD,\nand WritingPrompts; black\nshows row/column mean.explore the effect of using\na different model to score a\ncandidate passage (and per-\nturbed texts) than the model\nthat generated the passage.\nIn other words, we aim\nto classify between human-\ngenerated text and text from\nmodel A, but without ac-\ncess to model Ato com-\npute log probabilities. In-\nstead, we use log probabil-\nities computed by a surro-\ngate model B. We con-\nsider three models, GPT-J,\nGPT-Neo-2.7, and GPT-2,\nevaluating all possible com-\nbinations of source model\nand surrogate model (9 to-\ntal). We average the perfor-\nmance across 200 samples\nfrom XSum, SQuAD, and\n7\x0cZero-Shot Machine-Generated Text Detection using Probability Curvature\n60M 220M 770M 2.7B0.50.60.70.80.91.0Detection AUROC\n5 perturbations\n60M 220M 770M 2.7B\n25 perturbations\nRandom\nGPT2-sm\nGPT2-md\nGPT2-lg\nGPT2-xl\nMask filling model size (# parameters)\nFigure 7. There is a clear association between capacity of mask-\nfilling model and detection performance, across source model\nscales. Random mask filling (uniform sampling from mask filling\nmodel vocabulary) performs poorly, reinforcing the idea that the\nperturbation function should produce samples on the data manifold.\nCurves show AUROC scores on 200 SQuAD contexts.\nWritingPrompts. The results are presented in Figure 6,\nshowing that when the surrogate model is different from the\nsource model, detection performance is reduced, indicating\nthat DetectGPT ', 'them easier to detect. Our\nwork does not assume text is generated with the goal of easy\ndetection; DetectGPT detects text generated from publicly\navailable LLMs using standard LLM sampling strategies.\nThe widespread use of LLMs has led to much other con-\ntemporaneous work on detecting LLM output. Sadasivan\net al. (2023) show that the detection AUROC of the an de-\ntector is upper bounded by a function of the TV distance\nbetween the model and human text. However, we find that\nAUROC of DetectGPT is high even for the largest publicly-\navailable models (Table 2), suggesting that TV distance may\nnot correlate strongly with model scale and capability. This\ndisconnect may be exacerbated by new training objectives\nother than maximum likelihood, e.g., reinforcement learn-\ning with human feedback (Christiano et al., 2017; Ziegler\net al., 2020). Both Sadasivan et al. (2023) and Krishna et al.\n(2023) show the effectiveness of paraphrasing as a tool for\nevading detection, suggesting an important area of study\nfor future work. Liang et al. (2023) show that multi-lingual\ndetection is difficult, with non-DetectGPT detectors show-\ning bias against non-native speakers; this result highlights\nthe advantage of zero-shot detectors like DetectGPT, which\ngeneralize well to any data generated by the original gener-\nating model. Mireshghallah et al. (2023) study which proxy\nscoring models produce the most useful log probabilities\nfor detection when the generating model is not known (a\nlarge-scale version of our Figure 6). Surprisingly (but con-\nsistent with our findings), they find that smaller models are\nin fact better proxy models for performing detection with\nperturbation-based methods like DetectGPT.\nThe problem of machine-generated text detection echoes ear-\nlier work on detecting deepfakes, artificial images or videos\ngenerated by deep nets, which ', 'al.,\n2023) may further improve the effectiveness of methods\nsuch as DetectGPT, even as LLMs continue to improve.\nLimitations. One limitation of probability-based methods\nfor zero-shot machine-generated text detection (like Detect-\nGPT) is the white-box assumption that we can evaluate log\nprobabilities of the model(s) in question. For models be-\nhind APIs that do provide probabilities (such as GPT-3),\nevaluating probabilities nonetheless costs money. Another\nassumption of DetectGPT is access to a reasonable pertur-\nbation function. While in this work, we use off-the-shelf\nmask-filling models such as T5 and mT5 (for non-English\nlanguages), some domains may see reduced performance\nif existing mask-filling models do not well represent the\nspace of meaningful rephrases, reducing the quality of the\ncurvature estimate. While DetectGPT provides the best\navailable detection performance for PubMedQA, its drop\nin performance compared to other datasets may be a result\nAverage length0.9850.9900.995AUROC\ngpt-2\nAverage length0.960.970.980.99AUROC\nopt-2.7\nXSum\nSQuAD\nWritingPrompts\n130 140 150 160 170\nAverage length0.8750.9000.9250.9500.975AUROC\nEleutherAI/gpt-j-6b\n130 140 150 160 170\nAverage length0.70.80.9AUROC\nEleutherAI/gpt-neox-20bFigure 10. DetectGPT AUROC vs passage length. The relation-\nship between detection performance and passage length generally\ndepends on the dataset and model (or tokenizer). Decreases in\ndetection quality with increasing length may be due to T5 failing\nto track many (20+) masks to fill at once; this problem may be\nmitigated by applying mask-fills in a sequence of smaller batches.\nof lower quality perturbations. Finally, DetectGPT is more\ncompute-intensive than other methods for detection, as it\nrequires sampling and scoring the set of perturbations for\neach candidate passage, rather than just the candidate pas-\nsage; a better tuned perturbation function or more efficient\ncurvature approximation may help mitigate these costs.\nFuture Work. While the methods in this work make no\nassumptions about the models generating the samples, fu-\nture work may explore how watermarking ']",DetectGPT uses the log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model.,"DetectGPT determines if a passage was generated by an LLM by using log probabilities computed by a surrogate model to classify between human-generated text and text from model A, without access to model A to compute log probabilities.",1.0,1.0,0.7801974415779114
"What role do random perturbations play in DetectGPT's methodology, and how are they applied?","['0.58 0.53 0.53 0.50 0.55 0.56 0.57 0.54 0.32 0.37 0.28 0.32 0.32 0.32\nDetectGPT 0.99 0.98 1.00 0.98 0.97 0.98 0.99 0.98 0.98 0.90 0.82* 0.94 1.00 0.99 0.99 0.97* 0.93 0.98\nDiff 0.04 0.04 0.04 0.05 0.08 0.05 0.01 0.02 0.04 0.00 -0.01 0.02 0.01 0.01 0.01 -0.01 -0.05 0.00\nTable 4. Nucleus (top- p) sampling evaluation with p= 0.96. AUROC for detecting samples from the given model on the given dataset for\nDetectGPT and four previously proposed criteria. Nucleus sampling generally makes detection easier for all methods, but DetectGPT still\nprovides the highest average AUROC. For WritingPrompts, however, the LogRank baseline performs as well as DetectGPT.\nXSum SQuAD WritingPrompts\nMethod GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg. GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg. GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX Avg.\nlogp(x) 0.89 0.89 0.89 0.84 0.81 0.87 0.93 0.90 0.88 0.82 0.74 0.85 0.97 0.95 0.97 0.96 0.95* 0.96\nRank 0.79 0.77 0.77 0.75 0.73 0.76 0.84 0.82 0.80 0.80 0.75 0.80 0.87 0.84 0.83 0.82 0.81 0.83\nLogRank 0.92* 0.91* 0.93* 0.89* 0.85* 0.90* 0.96* 0.94* 0.92* 0.87* 0.79* 0.90* 0.98* 0.97* 0.98* 0.97 0.96 0.97\nEntropy 0.58 0.49 0.55 0.56 0.59 0.55 0.55 0.52 0.56 0.56 0.58 0.56 0.35 0.41 0.30 0.34 0.37 0.35\nDetectGPT 0.99 0.97 0.99 0.96 0.96 0.98 0.99 0.98 0.98 0.89 0.80 0.93 0.99 0.98 0.99 0.97 0.93 0.97\nDiff 0.07 0.06 0.06 0.07 0.11 0.08 0.03 0.04 0.06 0.02 0.01 0.03 0.01 0.01 0.01 0.00 -0.03 0.00\nTable 5. Top-ksampling evaluation with k= 40 . DetectGPT generally provides the most accurate performance (highest AUROC),\nalthough the gap is narrowed comparing to direct sampling, ', '8. Impact of varying the number of perturbations (samples\nof mask and mask-fill) used by DetectGPT on AUROC for GPT-2\n(left) and GPT-J ( right ) to estimate the perturbation discrepancy\non detection. Averaging up to 100 perturbations greatly increases\nDetectGPT’s reliability. Perturbations sampled from T5-large.\nlarly how the domain impacts the threshold separating the\nperturbation discrepancy distributions of model-generated\nand human texts as well as the impact of passage length on\ndetection. Figure 9 shows the perturbation discrepancy dis-\ntributions for model-generated and human texts across four\ndata distributions, using GPT-Neo-2.7B to generate sam-\nples. A threshold of slightly below 0.1 separates human and\nmodel texts across data distributions, which is important for\npractical scenarios in which a passage may be analyzed with-\nout knowing its domain a priori. Finally, Figure 10 shows an\nanalysis of DetectGPT’s performance as a function of pas-\nsage length. We bin the paired human- and model-generated\nsequences by their average length into three bins of equal\nsize (bottom/middle/top third), and plot the AUROC within\neach bin. The relationship between detection performance\nand passage length generally depends on the dataset and\nmodel (or tokenizer). For very long sequences, DetectGPT\nmay see reduced performance because our implementation\nof DetectGPT applies all T5 mask-filling perturbations at\nonce, and T5 may fail to track many mask tokens at once.\nBy applying perturbations in multiple sequential rounds of\nsmaller numbers of masks, this effect may be mitigated.\n6. Discussion\nAs large language models continue to improve, they will\nbecome increasingly attractive tools for replacing human\nwriters in a variety of contexts, such as education, jour-\nnalism, and art. While legitimate uses of language model\ntechnologies exist in all of these settings, teachers, readers,\nand consumers are likely ', 'deviation of the ob-\nserved values used to estimate E˜x∼q(·|x)logpθ(˜x)provides\na slightly better signal for detection, typically increasing\n1We later show in Figure 8 that varying the number of samples\nused to estimate the expectation effectively allows for trading off\nbetween accuracy and speed.AUROC by around 0.020, so we use this normalized version\nof the perturbation discrepancy in our experiments. The\nresulting method, DetectGPT, is summarized in Alg. 1. Hav-\ning described an application of the perturbation discrepancy\nto machine-generated text detection, we next provide an\ninterpretation of this quantity.\nInterpretation of perturbation discrepancy as curvature\nWhile Figure 3 suggests that the perturbation discrepancy\nmay be useful, it is not immediately obvious what it mea-\nsures. In this section, we show that the perturbation dis-\ncrepancy approximates a measure of the local curvature\nof the log probability function near the candidate passage,\nmore specifically, that it is proportional to the negative trace\nof the Hessian of the log probability function.2To han-\ndle the non-differentiability of discrete data, we consider\ncandidate passages in a latent semantic space, where small\ndisplacements correspond to valid edits that retain similar\nmeaning to the original. Because our perturbation function\n(T5) models natural text, we expect our perturbations to\nroughly capture such meaningful variations of the original\npassage, rather than arbitrary edits.\nWe first invoke Hutchinson’s trace estimator (Hutchinson,\n1990), giving an unbiased estimate of the trace of matrix A:\ntr(A) =Ezz⊤Az (2)\nprovided that the elements of z∼qzare IID with E[zi] = 0\nandVar(zi) = 1 . To use Equation 2 to estimate the trace\nof the Hessian of fatx, we must therefore compute the\nexpectation of the directional second derivative z⊤Hf(x)z.\nWe approximate this expression with finite differences:\nz⊤Hf(x)z≈f(x+hz) +f(x−hz)−2f(x)\nh2(3)\nCombining Equations 2 and 3 ']","Random perturbations are used to slightly modify the text, keeping its meaning similar. DetectGPT evaluates the change in log probability caused by these perturbations to detect machine-generated text.","Random perturbations play a crucial role in DetectGPT's methodology by estimating the perturbation discrepancy on detection. These perturbations are used to approximate a measure of the local curvature of the log probability function near the candidate passage. In the methodology, perturbations are sampled from T5-large and applied by applying all T5 mask-filling perturbations at once. To mitigate any issues with very long sequences where T5 may fail to track many mask tokens at once, perturbations can also be applied in multiple sequential rounds of smaller numbers of masks.",1.0,1.0,0.5356681942939758
What specific architectural changes were made to develop DistilBERT from BERT?,"['/ 87.4 86.5 (89.0 / 85.5)\nXLNet (Yang et al., 2019) 2 90.8 / 90.8 92.3 95.1 / 89.7 90.6 / 87.9 85.4 (88.6 / 84.0)\nMegatron-336M 1 89.7 / 90.0 92.3 94.2 / 88.0 88.1 / 84.8 83.0 (86.9 / 81.5)\nMegatron-1.3B 1 90.9 / 91.0 92.6 94.9 / 89.1 90.2 / 87.1 87.3 (90.4 / 86.1)\nMegatron-3.9B 1 91.4 / 91.4 92.7 95.5 / 90.0 91.2 / 88.5 89.5 (91.8 / 88.6)\nALBERT ensemble (Lan et al., 2019) 95.5 / 90.1 91.4 / 88.9 89.4 (91.2 / 88.6)\nMegatron-3.9B ensemble 95.8 / 90.5 91.7 / 89.0 90.9 (93.1 / 90.0)\nFigure 7. Training loss for BERT model using the original architec-\nture (a) and the rearranged architecture (b). Left ﬁgure shows the\ntraining loss for 336M and 752M BERT model. While the original\narchitecture performs well on the 336M model, the modiﬁcations\nin (b) enable stable training with lower training loss.\nUsing the architecture change in Figure 7(b), we consider\nthree different cases as detailed in Table 4. The 336M model\nhas the same size as BERT-large. The 1.3B is the same as\nthe BERT-xlarge conﬁguration that was previously shown\nto get worse results than the 336M BERT-large model (Lan\net al., 2019). We further scale the BERT model using both\nlarger hidden size as well as more layers to arrive at the 3.9B\nparameter case. In all cases, the hidden size per attention\nhead is kept constant at 64. 336M and 1.3B models are\ntrained for 2 million iterations while the 3.9B model is\ntrained for 1.5 million iterations and is still training.\nOn a 3% held-out set, 336M, 1.3B, and 3.9B models achieve\nvalidation set perplexity ', 'yields to comparable\nperformance on downstream tasks. Com-\nparison on downstream tasks: IMDb (test ac-\ncuracy) and SQuAD 1.1 (EM/F1 on dev set).\nD: with a second step of distillation during\nﬁne-tuning.\nModel IMDb SQuAD\n(acc.) (EM/F1)\nBERT-base 93.46 81.2/88.5\nDistilBERT 92.82 77.7/85.8\nDistilBERT (D) - 79.1/86.9Table 3: DistilBERT is signiﬁcantly smaller\nwhile being constantly faster. Inference\ntime of a full pass of GLUE task STS-B (sen-\ntiment analysis) on CPU with a batch size of\n1.\nModel # param. Inf. time\n(Millions) (seconds)\nELMo 180 895\nBERT-base 110 668\nDistilBERT 66 410\nDistillation We applied best practices for training BERT model recently proposed in Liu et al. [2019].\nAs such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K\nexamples per batch) using dynamic masking and without the next sentence prediction objective.\nData and compute power We train DistilBERT on the same corpus as the original BERT model:\na concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT\nwas trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the\nRoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100.\n4 Experiments\nGeneral Language Understanding We assess the language understanding and generalization ca-\npabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems.\nWe report scores on the development sets for each task by ﬁne-tuning DistilBERT without the use\nof ensembling or multi-tasking scheme for ﬁne-tuning (which are mostly orthogonal to the present\nwork). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters\net al. [2018]) encoder followed ', 'inference speed\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number\nof parameters of each model along with the inference time needed to do a full pass on the STS-\nB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1.\nDistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT.\nOn device computation We studied whether DistilBERT could be used for on-the-edge applications\nby building a mobile application for question answering. We compare the average inference time on\na recent smartphone (iPhone 7 Plus) against our previously trained question answering model based\non BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole\nmodel weighs 207 MB (which could be further reduced with quantization). Our code is available5.\n4.2 Ablation study\nIn this section, we investigate the inﬂuence of various components of the triple loss and the student\ninitialization on the performances of the distilled model. We report the macro-score on GLUE. Table 4\npresents the deltas with the full triple loss: removing the Masked Language Modeling loss has little\nimpact while the two distillation losses account for a large portion of the performance.\n5 Related work\nTask-speciﬁc distillation Most of the prior works focus on building task-speciﬁc distillation se-\ntups. Tang et al. [2019] transfer ﬁne-tune classiﬁcation model BERT to an LSTM-based classiﬁer.\nChatterjee [2019] distill BERT model ﬁne-tuned on SQuAD in a smaller Transformer model previ-\nously initialized from BERT. In the present work, we found it beneﬁcial to use a general-purpose\npre-training distillation rather than a task-speciﬁc distillation. Turc et al. [2019] use ']","DistilBERT omits token-type embeddings and the pooler layer, and reduces the number of layers by half compared to BERT.","The specific architectural change made to develop DistilBERT from BERT was using a distilled general-purpose pre-training distillation rather than a task-specific distillation, as mentioned in the related work section.",1.0,1.0,0.7026130557060242
What core challenge does HellaSwag aim to address in the context of state-of-the-art models' capabilities in commonsense natural language inference (NLI)?,"['several works (Kocijan et al., 2019; Liu et al., 2019d) have made rapid\nprogress via a hueristic data augmentation scheme, raising machine performance to 90.4% accuracy.\nGiven estimated human performance of ∼96%, there is still a gap between machine and human\nperformance, which we expect will be relatively difﬁcult to close. We therefore include a version of\nWSC cast as binary classiﬁcation, where each example consists of a sentence with a marked pronoun\nand noun, and the task is to determine if the pronoun refers to that noun. The training and validation\nexamples are drawn from the original WSC data (Levesque et al., 2012), as well as those distributed\nby the afﬁliated organization Commonsense Reasoning .3The test examples are derived from ﬁction\nbooks and have been shared with us by the authors of the original dataset. We evaluate using accuracy.\n3.3 Scoring\nAs with GLUE, we seek to give a sense of aggregate system performance over all tasks by averaging\nscores of all tasks. Lacking a fair criterion with which to weight the contributions of each task to\nthe overall score, we opt for the simple approach of weighing each task equally, and for tasks with\nmultiple metrics, ﬁrst averaging those metrics to get a task score.\n3.4 Tools for Model Analysis\nAnalyzing Linguistic and World Knowledge in Models GLUE includes an expert-constructed,\ndiagnostic dataset that automatically tests models for a broad range of linguistic, commonsense, and\nworld knowledge. Each example in this broad-coverage diagnostic is a sentence pair labeled with\na three-way entailment relation ( entailment ,neutral , orcontradiction ) and tagged with labels that\nindicate the phenomena that characterize the relationship between the ', 'HellaSwag : Can a Machine Really Finish Your Sentence?\nRowan Zellers♠Ari Holtzman♠Yonatan Bisk♠Ali Farhadi♠♥Yejin Choi♠♥\n♠Paul G. Allen School of Computer Science & Engineering, University of Washington\n♥Allen Institute for Artiﬁcial Intelligence\nhttps://rowanzellers.com/hellaswag\nAbstract\nRecent work by Zellers et al. (2018) intro-\nduced a new task of commonsense natural lan-\nguage inference : given an event description\nsuch as “A woman sits at a piano,” a machine\nmust select the most likely followup: “She\nsets her ﬁngers on the keys.” With the intro-\nduction of BERT (Devlin et al., 2018), near\nhuman-level performance was reached. Does\nthis mean that machines can perform human\nlevel commonsense inference?\nIn this paper, we show that commonsense in-\nference still proves di ﬃcult for even state-\nof-the-art models, by presenting HellaSwag ,\na new challenge dataset. Though its ques-\ntions are trivial for humans ( ą95% accuracy),\nstate-of-the-art models struggle ( ă48%). We\nachieve this via Adversarial Filtering (AF), a\ndata collection paradigm wherein a series of\ndiscriminators iteratively select an adversarial\nset of machine-generated wrong answers. AF\nproves to be surprisingly robust. The key in-\nsight is to scale up the length and complex-\nity of the dataset examples towards a critical\n‘Goldilocks’ zone wherein generated text is\nridiculous to humans, yet often misclassiﬁed\nby state-of-the-art models.\nOur construction of HellaSwag , and its result-\ning di ﬃculty, sheds light on the inner work-\nings of deep pretrained models. More broadly,\nit suggests a new path forward for NLP re-\nsearch, in which benchmarks co-evolve with\nthe evolving state-of-the-art in an adversarial\nway, so as to present ever-harder challenges.\n1 Introduction\nImagine a woman chasing a dog around outside,\ntrying to give it a bath. What might happen next?\nHumans can read a narrative like this, shown in\nFigure 1, and connect ', 'the wrong\nendings are on-topic, with words that relate to the con-\ntext, humans consistently judge their meanings to be\neither incorrect or implausible. For example, option A\nof the WikiHow passage suggests that a driver should\nstop at a red light for no more than two seconds .\nplausible next event is option C—that she’ll get\nthe dog wet and it will run away again.\nWhen the SWAG dataset was ﬁrst announced\n(Zellers et al., 2018), this new task of common-\nsense natural language inference seemed trivial\nfor humans (88%) and yet challenging for then-\nstate-of-the-art models ( ă60%), including ELMo\n(Peters et al., 2018). However, BERT (Devlin\net al., 2018) soon reached over 86%, almost\nhuman-level performance. One news article on\nthis development was headlined “ ﬁnally, a ma-\nchine that can ﬁnish your sentence. ”1\nIn this paper, we investigate the following ques-\ntion: How well do deep pretrained models, like\n1A New York Times article at https: //nyti.ms /2DycutY.\n1arXiv:1905.07830v1 [cs.CL] 19 May 2019\x0cBERT, perform at commonsense natural language\ninference (NLI)? Our surprising conclusion is\nthat the underlying task remains unsolved. In-\ndeed, we ﬁnd that deep models such as BERT do\nnot demonstrate robust commonsense reasonining\nability by themselves. Instead, they operate more\nlikerapid surface learners for a particular dataset.\nTheir strong performance on SWAG is dependent\non the ﬁnetuning process, wherein they largely\nlearn to pick up on dataset-speciﬁc distributional\nbiases. When the distribution of language shifts\nslightly, performance drops drastically – even if\nthe domain remains identical.\nWe study this question by introducing Hella-\nSwag ,2a new benchmark for commonsense\nNLI. We use Adversarial Filtering (AF), a data-\ncollection paradigm in which a series of discrim-\ninators is used to select a challenging set of gen-\nerated wrong ']","HellaSwag challenges the notion that state-of-the-art models have solved commonsense NLI by introducing adversarially crafted dataset examples that are easy for humans but difficult for machines, highlighting the gap in commonsense reasoning.","HellaSwag aims to address the challenge of robust commonsense reasoning ability in deep pretrained models, showing that they operate more like rapid surface learners for a specific dataset rather than demonstrating robust capabilities for commonsense NLI.",1.0,1.0,0.8394746780395508
"How does the dynamic masking strategy in RoBERTa differ from BERT's static masking, and what advantage does it offer?","['RoBERTa is trained with dynamic mask-\ning (Section 4.1),FULL -SENTENCES without NSP\nloss (Section 4.2), large mini-batches (Section 4.3)\nand a larger byte-level BPE (Section 4.4).\nAdditionally, we investigate two other impor-\ntant factors that have been under-emphasized in\nprevious work: (1) the data used for pretraining,\nand (2) the number of training passes through the\ndata. For example, the recently proposed XLNet\narchitecture ( Yang et al. ,2019 ) is pretrained us-\ning nearly 10 times more data than the original\nBERT ( Devlin et al. ,2019 ). It is also trained with\na batch size eight times larger for half as many op-\ntimization steps, thus seeing four times as many\nsequences in pretraining compared to BERT.\nTo help disentangle the importance of these fac-\ntors from other modeling choices (e.g., the pre-\ntraining objective), we begin by training RoBERTa\nfollowing the BERT LARGE architecture ( L= 24 ,\nH= 1024 ,A= 16 , 355M parameters). We\npretrain for 100K steps over a comparable B OOK -\nCORPUS plus W IKIPEDIA dataset as was used in\x0cModel data bsz stepsSQuADMNLI-m SST-2(v1.1/2.0)\nRoBERTa\nwith B OOKS + W IKI 16GB 8K 100K 93.6/87.3 89.0 95.3\n+ additional data ( §3.2) 160GB 8K 100K 94.0/87.7 89.3 95.6\n+ pretrain longer 160GB 8K 300K 94.4/88.7 90.0 96.1\n+ pretrain even longer 160GB 8K 500K 94.6/89.4 90.2 96.4\nBERT LARGE\nwith B OOKS + W IKI 13GB 256 1M 90.9/81.8 86.6 93.7\nXLNet LARGE\nwith B OOKS + W IKI 13GB 256 1M 94.0/87.8 88.4 94.4\n+ additional data 126GB 2K 500K 94.5/88.8 89.8 95.6\nTable 4: Development set results for RoBERTa as we pretrain o ver more data (16GB →160GB of text) and pretrain\nfor longer (100K →300K→500K steps). Each row ', 'NER,\nwe report both ﬁne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npliﬁed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\nMasking Rates Dev Set Results\nMASK SAME RND MNLI NER\nFine-tune Fine-tune Feature-based\n80% 10% 10% 84.2 95.4 94.9\n100% 0% 0% 84.3 94.9 94.0\n80% 0% 20% 84.1 95.2 94.6\n80% 20% 0% 84.4 95.2 94.7\n0% 20% 80% 83.7 94.8 94.6\n0% 0% 100% 83.6 94.9 94.6\nTable 8: Ablation over different masking strategies.\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe[MASK] symbol for MLM; S AME means that\nwe keep the target token as is; R NDmeans that\nwe replace the target token with another random\ntoken.\nThe numbers in the left part of the table repre-\nsent the probabilities of the speciﬁc strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\nFrom the table it can be seen that ﬁne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the M ASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe R NDstrategy performs much worse than our\nstrategy as well.', 'SST-2. Reported results are\nmedians over 5 random initializations (seeds). Refer-\nence results are from Yang et al. (2019 ).\nResults Table 1compares the published\nBERT BASE results from Devlin et al. (2019 ) to our\nreimplementation with either static or dynamic\nmasking. We ﬁnd that our reimplementation\nwith static masking performs similar to the\noriginal BERT model, and dynamic masking is\ncomparable or slightly better than static masking.\nGiven these results and the additional efﬁciency\nbeneﬁts of dynamic masking, we use dynamic\nmasking in the remainder of the experiments.\n4.2 Model Input Format and Next Sentence\nPrediction\nIn the original BERT pretraining procedure, the\nmodel observes two concatenated document seg-\nments, which are either sampled contiguously\nfrom the same document (with p= 0.5) or from\ndistinct documents. In addition to the masked lan-\nguage modeling objective, the model is trained to\npredict whether the observed document segments\ncome from the same or distinct documents via an\nauxiliary Next Sentence Prediction (NSP) loss.\nThe NSP loss was hypothesized to be an impor-\ntant factor in training the original BERT model.\nDevlin et al. (2019 ) observe that removing NSP\nhurts performance, with signiﬁcant performance\ndegradation on QNLI, MNLI, and SQuAD 1.1.\nHowever, some recent work has questioned the\nnecessity of the NSP loss ( Lample and Conneau ,\n2019 ;Yang et al. ,2019 ;Joshi et al. ,2019 ).\nTo better understand this discrepancy, we com-\npare several alternative training formats:\n•SEGMENT -PAIR +NSP: This follows the original\ninput format used in BERT ( Devlin et al. ,2019 ),\nwith the NSP loss. Each input has a pair of seg-\nments, which can each contain multiple natural\nsentences, but the total combined length must\nbe less than 512 tokens.\x0cModel SQuAD 1.1/2.0 MNLI-m SST-2 RACE\nOur reimplementation (with ']","Unlike BERT's static masking, which applies the same mask to each training instance throughout epochs, RoBERTa employs dynamic masking, where the mask is generated anew for each instance every time it is fed into the model. This strategy ensures a richer variety of training examples and prevents the model from overfitting to a specific masked pattern.",The dynamic masking strategy in RoBERTa differs from BERT's static masking by being comparable or slightly better than static masking. Dynamic masking offers additional efficiency benefits compared to static masking.,1.0,1.0,0.841602087020874
"How does the performance of RoBERTa compare to BERT and other models on the GLUE benchmark, particularly in terms of state-of-the-art achievements?","['56.0 75.1\nBERT BASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\nBERT LARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\nTable 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard ).\nThe number below each task denotes the number of training examples. The “Average” column is slightly different\nthan the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8BERT and OpenAI GPT are single-\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\nWe use a batch size of 32 and ﬁne-tune for 3\nepochs over the data for all GLUE tasks. For each\ntask, we selected the best ﬁne-tuning learning rate\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\nAdditionally, for BERT LARGE we found that ﬁne-\ntuning was sometimes unstable on small datasets,\nso we ran several random restarts and selected the\nbest model on the Dev set. With random restarts,\nwe use the same pre-trained checkpoint but per-\nform different ﬁne-tuning data shufﬂing and clas-\nsiﬁer layer initialization.9\nResults are presented in Table 1. Both\nBERT BASE and BERT LARGE outperform all sys-\ntems on all tasks by a substantial margin, obtaining\n4.5% and 7.0% respective average accuracy im-\nprovement over the prior state of the art. Note that\nBERT BASE and OpenAI GPT are nearly identical\nin terms of model architecture apart from the at-\ntention masking. For the largest and most widely\nreported GLUE task, MNLI, BERT obtains a 4.6%\nabsolute accuracy improvement. On the ofﬁcial\nGLUE leaderboard10, BERT LARGE obtains a score\nof ', 'accumulates improvements from the row s above. RoBERTa\nmatches the architecture and training objective of BERT LARGE . Results for BERT LARGE and XLNet LARGE are from\nDevlin et al. (2019 ) and Yang et al. (2019 ), respectively. Complete results on all GLUE tasks can be fo und in the\nAppendix.\nDevlin et al. (2019 ). We pretrain our model using\n1024 V100 GPUs for approximately one day.\nResults We present our results in Table 4. When\ncontrolling for training data, we observe that\nRoBERTa provides a large improvement over the\noriginally reported BERT LARGE results, reafﬁrming\nthe importance of the design choices we explored\nin Section 4.\nNext, we combine this data with the three ad-\nditional datasets described in Section 3.2. We\ntrain RoBERTa over the combined data with the\nsame number of training steps as before (100K).\nIn total, we pretrain over 160GB of text. We ob-\nserve further improvements in performance across\nall downstream tasks, validating the importance of\ndata size and diversity in pretraining.9\nFinally, we pretrain RoBERTa for signiﬁcantly\nlonger, increasing the number of pretraining steps\nfrom 100K to 300K, and then further to 500K. We\nagain observe signiﬁcant gains in downstream task\nperformance, and the 300K and 500K step mod-\nels outperform XLNet LARGE across most tasks. We\nnote that even our longest-trained model does not\nappear to overﬁt our data and would likely beneﬁt\nfrom additional training.\nIn the rest of the paper, we evaluate our best\nRoBERTa model on the three different bench-\nmarks: GLUE, SQuaD and RACE. Speciﬁcally\n9Our experiments conﬂate increases in data size and di-\nversity. We leave a more careful analysis of these two dimen-\nsions to future work.we consider RoBERTa trained for 500K steps over\nall ', '65.1 0.0 100.0/ 50.0\nCBoW 44.3 62.1 49.0/71.2 51.6 0.0 / 0.4 14.0/13.6 49.7 53.0 65.1 -0.4 100.0/ 50.0\nBERT 69.0 77.4 75.7/83.6 70.6 70.0 / 24.0 72.0/71.3 71.6 69.5 64.3 23.0 97.8 / 51.7\nBERT++ 71.5 79.0 84.7/90.4 73.8 70.0 / 24.1 72.0/71.3 79.0 69.5 64.3 38.0 99.4 / 51.4\nOutside Best - 80.4 - / - 84.4 70.4 */24.5*74.8/73.0 82.7 - - - - / -\nHuman (est.) 89.8 89.0 95.8/98.9 100.0 81.8*/51.9* 91.7/91.3 93.6 80.0 100.0 77.0 99.3 / 99.7\n5 Experiments\n5.1 Baselines\nBERT Our main baselines are built around BERT, variants of which are among the most successful\napproach on GLUE at the time of writing. Speciﬁcally, we use the bert-large-cased variant.\nFollowing the practice recommended in Devlin et al. (2019), for each task, we use the simplest\npossible architecture on top of BERT. We ﬁne-tune a copy of the pretrained BERT model separately\nfor each task, and leave the development of multi-task learning models to future work. For training,\nwe use the procedure speciﬁed in Devlin et al. (2019): We use Adam (Kingma and Ba, 2014) with an\ninitial learning rate of 10−5and ﬁne-tune for a maximum of 10 epochs.\nFor classiﬁcation tasks with sentence-pair inputs (BoolQ, CB, RTE, WiC), we concatenate the\nsentences with a [SEP]token, feed the fused input to BERT, and use a logistic regression classiﬁer\nthat sees the representation corresponding to [CLS]. For WiC, we also concatenate the representation\nof the marked word. For COPA, MultiRC, and ReCoRD, for each answer choice, we similarly\nconcatenate the context with that answer choice and feed the resulting sequence into BERT to produce\nan answer representation. For COPA, we ']","RoBERTa surpasses BERT and establishes new state-of-the-art performance on the GLUE benchmark, particularly excelling in tasks like MNLI, QNLI, RTE, and STS-B. This achievement underscores the effectiveness of RoBERTa's optimizations and its capacity to better understand and process natural language.","RoBERTa provides a large improvement over the originally reported BERT LARGE results, reaffirming the importance of the design choices explored. It outperforms BERT and other models on the GLUE benchmark, particularly in terms of state-of-the-art achievements.",1.0,1.0,0.7718908786773682
How should future language model benchmarks be structured to ensure a holistic assessment of models' capabilities and knowledge breadth?,"['0.669 0.698 0.771\nphilpapers 0.741 0.723 0.766\ngutenberg_pg_19 0.890 1.160 0.821\narxiv 0.680 0.838 0.570\nstackexchange 0.655 0.773 0.611\nnih_exporter 0.590 0.612 0.614\npubmed_abstracts 0.587 0.625 0.610\nuspto_backgrounds 0.537 0.566 0.537\npubmed_central 0.579 0.690 0.510\nfreelaw 0.514 0.612 0.499\ngithub 0.358 0.645 0.329\nenron_emails 0.621 0.958 0.604\nyoutube_subtitles 0.825 0.815 0.746\nWeighted Avg. 0.650 0.742 0.634Pile evalution (Gao et al., 2020) is a comprehen-\nsive language modeling benchmark which origi-\nnally includes 22 different text datasets from di-\nverse domains. We report our results over a part of\n18 datasets with previously reported baseline re-\nsults (Lieber et al., 2021). Different from tradi-\ntional language modeling benchmarks, Pile evalu-\nation report the BPB (bits-per-byte) perplexity to\navoid the mismatch comparison between models\nwith different vocabularies. Because in general,\nlanguage models with a larger vocabulary will be\nfavored in perplexity comparison if not restricted.\nIn the evaluation, we strictly follow the setting\nin (Gao et al., 2020), leveraging [gMASK] and\na context-length of 1,024 with bidirectional atten-\ntion, and the rest 1024 tokens to calculate BPB in\nan autoregressive manner. The weighted average\nBPB are calculated based on each shared dataset’s\nratio in Pile training-set (Gao et al., 2020).\nThe detailed metrics on Pile test-set are reported in Table 13. We observe that compared to GPT-\n3, GLM-130B has a noticeable weaker performance on phil_papers and pile_cc, which is likely\nbecause of GLM-130B’s bilingual natural and lack of more diverse and high-quality private collected\ncorpora.\n10https://github.com/google/BIG-bench\n11https://docs.google.com/spreadsheets/d/1CI8Q9RCblLRzUOPJ6ViqBmo284-8oj\nluQ-CmaEuhuv0\n12https://huggingface.co/datasets/bigscience/evaluation-results/tree/ma\nin/bloom/bloomzeval/transformers/evaluation_val\n39\x0cPublished as a conference paper at ICLR 2023\nC.5 BIG- BENCH -LITE EVALUATION\n10810910101011\nEffective Parameter Count5\n0510152025303540Aggregate Normalized Performance\nGLM-130B 0-shot\nGLM-130B 1-shot\nGLM-130B 3-shot\nGPT-3 0-shot\nGPT-3 1-shot\nGPT-3 3-shot\nPaLM 0-shot\nPaLM 1-shot\nFigure 16: A full scope of BIG-bench-\nlite (24 tasks) evaluation.Recent works (Wei et al., 2022c; Wang et al., 2022c) re-\nveal that LLMs are capable to ', 'shows that by performing such an analysis, one can\nhelp elucidate the successes and failures of existing models,\nas well as help to identify possible paths forward to improve\ntoday’s systems.\nAcknowledgements\nWe thank Sewon Min, Sameer Singh, Katherine Lee, and\nthe members of UNC NLP for their valuable feedback. Eric\nWallace is supported by the Apple Scholars in AI/ML Fel-\nlowship. This work was supported by NSF-AI Engage Insti-\ntute DRL-2112635.\nReferences\nAky¨urek, E., Bolukbasi, T., Liu, F., Xiong, B., Tenney, I.,\nAndreas, J., and Guu, K. Tracing knowledge in language\nmodels back to the training data. In Findings of EMNLP ,\n2022.\nBlack, S., Leo, G., Wang, P., Leahy, C., and Biderman, S.\nGPT-Neo: Large Scale Autoregressive Language Model-\ning with Mesh-Tensorflow, 2021.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., et al. GPT-Neox-20B: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745 , 2022.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nInNeurIPS , 2020.\nCarlini, N., Liu, C., Erlingsson, ´U., Kos, J., and Song,\nD. The secret sharer: Evaluating and testing unintended\nmemorization in neural networks. In USENIX Security\nSymposium , 2019.\nCarlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-\nV oss, A., Lee, K., Roberts, A., Brown, T., Song, D.,\nErlingsson, U., Oprea, A., and Raffel, C. Extracting\ntraining data from large language models. In USENIX\nSecurity Symposium , 2021.\nDe Cao, N., Izacard, G., Riedel, S., and Petroni, F. Autore-\ngressive entity retrieval. In ICLR , 2021.Elazar, Y ., Kassner, N., Ravfogel, S., Feder, A., Ravichan-\nder, A., Mosbach, M., Belinkov, Y ., Sch ¨utze, ', '(Radford et al., 2019),\n10B-scale language models (Raffel et al., 2020; Shoeybi et al., 2019; Black et al., 2022), to 100B-\nscale GPT-3 (Brown et al., 2020). Later, despite many 100B-scale LLMs (Lieber et al., 2021; Thop-\npilan et al., 2022; Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022; Wu et al., 2021; Zeng\net al., 2021; Wang et al., 2021) in both English and Chinese, they are not available to public or only\naccessible via limited APIs. The closeness of LLMs severely stymies its development. GLM-130B’s\nefforts, along with recent ElutherAI, OPT-175B (Zhang et al., 2022), and BLOOM-176B (Scao et al.,\n2022), aim to offer high-quality open-sourced LLMs to our community.\nTransferring. Though fine-tuning has been a de facto way for transfer learning, the evaluation for\nLLMs has been focused on prompting and in-context learning due to their tremendous sizes (Brown\net al., 2020; Liu et al., 2021a). Nevertheless, some recent attempts has been on parameter-efficient\nlearning on language models (Houlsby et al., 2019) and prompt tuning (i.e., P-tuning, Li & Liang\n(2021); Liu et al. (2021b); Lester et al. (2021); Liu et al. (2022)). For now we do not focus on them\nand will leave the comprehensive testing of them on GLM-130B in future study.\nInference. Most public-accessible LLMs nowadays are providing their services via limited APIs.In\nthis work, an important part of our endeavor has been on LLMs’ efficient and fast inference. Related\nwork may include distillation (Sanh et al., 2019; Jiao et al., 2020; Wang et al., 2020), quantiza-\ntion (Zafrir et al., 2019; Shen et al., 2020; Tao et al., 2022), and pruning (Michel ']","Future benchmarks should integrate a broader spectrum of subjects and cognitive skills, emphasizing the inclusion of tasks that test models' ethical reasoning, understanding of human values, and ability to perform complex problem-solving, beyond the mere scale of data and parameters.","Future language model benchmarks should be structured to include a wide range of diverse text datasets from various domains, similar to the Pile evaluation benchmark. Additionally, the benchmarks should incorporate metrics such as bits-per-byte perplexity to standardize comparisons between models with different vocabularies. It is important to have a strict evaluation setting, leveraging techniques like [gMASK] and bidirectional attention, to ensure fair comparison. Furthermore, the benchmarks should consider the shared dataset ratio in the training set to calculate weighted average metrics. This approach will help provide a comprehensive assessment of models' capabilities and knowledge breadth.",1.0,1.0,0.31704699993133545
How does DetectGPT's approach to machine-generated text detection differ from previous zero-shot methods?,"['both top- k\nand nucleus sampling make detection easier, on average.\nAveraging across domains, DetectGPT provides the clearest\nsignal for zero-shot detection.\nDetection when the source model is unknown. While\nour experiments have focused on the white-box setting\nfor machine-generated text detection, in this section, we\nGPT-J GPT-Neo GPT-2Scoring ModelGPT-J GPT-Neo GPT-2Base Model0.92\n(0.02)0.83\n(0.04)0.79\n(0.02)\n0.64\n(0.06)0.97\n(0.01)0.83\n(0.02)\n0.60\n(0.09)0.85\n(0.05)0.99\n(0.00)0.85\n0.81\n0.81\n0.72 0.88 0.87\nFigure 6. DetectGPT performs\nbest when scoring samples\nwith the same model that gen-\nerated them (diagonal), but\nthe column means suggest that\nsome models (GPT-Neo, GPT-\n2) may be better ‘scorers’ than\nothers (GPT-J). White values\nshow mean (standard error)\nAUROC over XSum, SQuAD,\nand WritingPrompts; black\nshows row/column mean.explore the effect of using\na different model to score a\ncandidate passage (and per-\nturbed texts) than the model\nthat generated the passage.\nIn other words, we aim\nto classify between human-\ngenerated text and text from\nmodel A, but without ac-\ncess to model Ato com-\npute log probabilities. In-\nstead, we use log probabil-\nities computed by a surro-\ngate model B. We con-\nsider three models, GPT-J,\nGPT-Neo-2.7, and GPT-2,\nevaluating all possible com-\nbinations of source model\nand surrogate model (9 to-\ntal). We average the perfor-\nmance across 200 samples\nfrom XSum, SQuAD, and\n7\x0cZero-Shot Machine-Generated Text Detection using Probability Curvature\n60M 220M 770M 2.7B0.50.60.70.80.91.0Detection AUROC\n5 perturbations\n60M 220M 770M 2.7B\n25 perturbations\nRandom\nGPT2-sm\nGPT2-md\nGPT2-lg\nGPT2-xl\nMask filling model size (# parameters)\nFigure 7. There is a clear association between capacity of mask-\nfilling model and detection performance, across source model\nscales. Random mask filling (uniform sampling from mask filling\nmodel vocabulary) performs poorly, reinforcing the idea that the\nperturbation function should produce samples on the data manifold.\nCurves show AUROC scores on 200 SQuAD contexts.\nWritingPrompts. The results are presented in Figure 6,\nshowing that when the surrogate model is different from the\nsource model, detection performance is reduced, indicating\nthat DetectGPT ', 'samples used for evaluation). From 1.5B parameter GPT-2 to 20B parameter GPT-NeoX, DetectGPT consistently provides the most\naccurate detections. Bold shows the best AUROC within each column (model-dataset combination); asterisk (*) denotes the second-best\nAUROC. Values in the final row show DetectGPT’s AUROC over the strongest baseline method in that column.\ncompact semantic space. Since the mask-filling model sam-\nples sentences similar to xwith minimal changes to seman-\ntic meaning, we can think of the mask-filling model as first\nsampling a similar semantic embedding ( ˜z∼qz) and then\nmapping this to a token sequence ( ˜z7→˜x). Sampling in\nsemantic space ensures that all samples stay near the data\nmanifold, which is useful because we would expect the log\nprobability to always drop if we randomly perturb tokens.\nWe can therefore interpret our objective as approximating\nthe curvature restricted to the data manifold.\n5. Experiments\nWe conduct experiments to better understand multiple facets\nof machine-generated text detection; we study the effective-\nness of DetectGPT for zero-shot machine-generated text de-\ntection compared to prior zero-shot approaches, the impact\nof distribution shift on zero-shot and supervised detectors,\nand detection accuracy for the largest publicly-available\nmodels. To further characterize factors that impact detec-\ntion accuracy, we also study the robustness of zero-shot\nmethods to machine-generated text that has been partially\nrevised, the impact of alternative decoding strategies on\ndetection accuracy, and a black-box variant of the detec-\ntion task. Finally, we analyze more closely DetectGPT’s\nbehavior as the choice of perturbation function, the number\nof samples used to estimate d(x, pθ, q), the length of the\npassage, and the data distribution is varied.\nComparisons. We compare DetectGPT with various exist-\ning zero-shot methods for machine-generated text detection\nthat also leverage the ', 'the slash).\nand the average margin of improvement is narrow.4For 14\nof the 15 combinations of dataset and model, DetectGPT\nprovides the most accurate detection performance, with a\n0.06 AUROC improvement on average. Log-rank threshold-\ning proves to be a consistently stronger baseline than log\nprobability thresholding, although it requires slightly more\ninformation (full predicted logits), which are not always\navailable in public APIs.\nComparison with supervised detectors. While our experi-\nments generally focus on zero-shot detection, some works\nhave evaluated the detection performance of supervised\nmethods (typically fine-tuned transformers) for detecting\nmachine-generated text. In this section, we explore several\ndomains to better understand the relative strengths of super-\nvised and zero-shot detectors. The results are presented in\nFigure 4, using 200 samples from each dataset for evalua-\ntion. We find that supervised detectors can provide similar\ndetection performance to DetectGPT on in-distribution data\nlike English news, but perform significantly worse than zero-\nshot methods in the case of English scientific writing and\nfail altogether for German writing. This finding echoes past\nwork showing that language models trained for machine-\ngenerated text detection overfit to their training data (source\nmodel, decoding strategy, topic, language, etc.; Uchendu\net al. (2020); Ippolito et al. (2020); Jawahar et al. (2020)).\nIn contrast, zero-shot methods generalize relatively easily\nto new languages and domains; DetectGPT’s performance\nin particular is mostly unaffected by the change in language\nfrom English to German.\nWhile our experiments have shown that DetectGPT is ef-\nfective on a variety of domains and models, it is natural to\nwonder if it is effective for the largest publicly-available\nLMs. Therefore, we also evaluate multiple zero-shot and su-\npervised methods on two 175B parameter models, OpenAI’s\nGPT-3 and AI21 Labs’ Jurassic-2 Jumbo. Because neither\nAPI provides ']","Previous zero-shot methods mainly relied on evaluating the average per-token log probability of generated text, whereas DetectGPT assesses the local structure around a passage through log probability curvature.",DetectGPT's approach to machine-generated text detection differs from previous zero-shot methods by using a surrogate model to score candidate passages and perturbed texts instead of relying on the model that generated the passage. This approach helps in classifying between human-generated text and text from model A without directly accessing model A's log probabilities.,1.0,1.0,0.7868260145187378
"What percentage of BERT's language understanding capabilities does DistilBERT retain, and what is the size reduction achieved?","['by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of\nindividual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo\nbaseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to\nBERT, retaining 97% of the performance with 40% fewer parameters.\n4.1 Downstream task benchmark\nDownstream tasks We further study the performances of DistilBERT on several downstream tasks\nunder efﬁcient inference constraints: a classiﬁcation task (IMDb sentiment classiﬁcation - Maas et al.\n[2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]).\nAs shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb\nbenchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT.\nWe also studied whether we could add another step of distillation during the adaptation phase by\nﬁne-tuning DistilBERT on SQuAD using a BERT model previously ﬁne-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\n3\x0cTable 4: Ablation study. Variations are relative to the model trained with triple loss and teacher\nweights initialization.\nAblation Variation on GLUE macro-score\n∅-Lcos-Lmlm -2.96\nLce-∅-Lmlm -1.46\nLce-Lcos-∅ -0.31\nTriple loss + random weights initialization -3.69\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two\nsuccessive steps of distillation, one during the pre-training phase and one during the adaptation phase.\nIn this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and\n70.4 EM, i.e. within 3 points of the full model.\nSize and ', 'yields to comparable\nperformance on downstream tasks. Com-\nparison on downstream tasks: IMDb (test ac-\ncuracy) and SQuAD 1.1 (EM/F1 on dev set).\nD: with a second step of distillation during\nﬁne-tuning.\nModel IMDb SQuAD\n(acc.) (EM/F1)\nBERT-base 93.46 81.2/88.5\nDistilBERT 92.82 77.7/85.8\nDistilBERT (D) - 79.1/86.9Table 3: DistilBERT is signiﬁcantly smaller\nwhile being constantly faster. Inference\ntime of a full pass of GLUE task STS-B (sen-\ntiment analysis) on CPU with a batch size of\n1.\nModel # param. Inf. time\n(Millions) (seconds)\nELMo 180 895\nBERT-base 110 668\nDistilBERT 66 410\nDistillation We applied best practices for training BERT model recently proposed in Liu et al. [2019].\nAs such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K\nexamples per batch) using dynamic masking and without the next sentence prediction objective.\nData and compute power We train DistilBERT on the same corpus as the original BERT model:\na concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT\nwas trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the\nRoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100.\n4 Experiments\nGeneral Language Understanding We assess the language understanding and generalization ca-\npabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems.\nWe report scores on the development sets for each task by ﬁne-tuning DistilBERT without the use\nof ensembling or multi-tasking scheme for ﬁne-tuning (which are mostly orthogonal to the present\nwork). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters\net al. [2018]) encoder followed ', 'inference speed\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number\nof parameters of each model along with the inference time needed to do a full pass on the STS-\nB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1.\nDistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT.\nOn device computation We studied whether DistilBERT could be used for on-the-edge applications\nby building a mobile application for question answering. We compare the average inference time on\na recent smartphone (iPhone 7 Plus) against our previously trained question answering model based\non BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole\nmodel weighs 207 MB (which could be further reduced with quantization). Our code is available5.\n4.2 Ablation study\nIn this section, we investigate the inﬂuence of various components of the triple loss and the student\ninitialization on the performances of the distilled model. We report the macro-score on GLUE. Table 4\npresents the deltas with the full triple loss: removing the Masked Language Modeling loss has little\nimpact while the two distillation losses account for a large portion of the performance.\n5 Related work\nTask-speciﬁc distillation Most of the prior works focus on building task-speciﬁc distillation se-\ntups. Tang et al. [2019] transfer ﬁne-tune classiﬁcation model BERT to an LSTM-based classiﬁer.\nChatterjee [2019] distill BERT model ﬁne-tuned on SQuAD in a smaller Transformer model previ-\nously initialized from BERT. In the present work, we found it beneﬁcial to use a general-purpose\npre-training distillation rather than a task-speciﬁc distillation. Turc et al. [2019] use ']",DistilBERT retains 97% of BERT's language understanding capabilities with a 40% reduction in size.,DistilBERT retains 97% of BERT's performance with 40% fewer parameters.,1.0,1.0,0.8076728582382202
"What datasets and computational resources were used to train DistilBERT, and how do they compare to the original BERT training setup?","['yields to comparable\nperformance on downstream tasks. Com-\nparison on downstream tasks: IMDb (test ac-\ncuracy) and SQuAD 1.1 (EM/F1 on dev set).\nD: with a second step of distillation during\nﬁne-tuning.\nModel IMDb SQuAD\n(acc.) (EM/F1)\nBERT-base 93.46 81.2/88.5\nDistilBERT 92.82 77.7/85.8\nDistilBERT (D) - 79.1/86.9Table 3: DistilBERT is signiﬁcantly smaller\nwhile being constantly faster. Inference\ntime of a full pass of GLUE task STS-B (sen-\ntiment analysis) on CPU with a batch size of\n1.\nModel # param. Inf. time\n(Millions) (seconds)\nELMo 180 895\nBERT-base 110 668\nDistilBERT 66 410\nDistillation We applied best practices for training BERT model recently proposed in Liu et al. [2019].\nAs such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K\nexamples per batch) using dynamic masking and without the next sentence prediction objective.\nData and compute power We train DistilBERT on the same corpus as the original BERT model:\na concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT\nwas trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the\nRoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100.\n4 Experiments\nGeneral Language Understanding We assess the language understanding and generalization ca-\npabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems.\nWe report scores on the development sets for each task by ﬁne-tuning DistilBERT without the use\nof ensembling or multi-tasking scheme for ﬁne-tuning (which are mostly orthogonal to the present\nwork). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters\net al. [2018]) encoder followed ', 'by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of\nindividual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo\nbaseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to\nBERT, retaining 97% of the performance with 40% fewer parameters.\n4.1 Downstream task benchmark\nDownstream tasks We further study the performances of DistilBERT on several downstream tasks\nunder efﬁcient inference constraints: a classiﬁcation task (IMDb sentiment classiﬁcation - Maas et al.\n[2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]).\nAs shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb\nbenchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT.\nWe also studied whether we could add another step of distillation during the adaptation phase by\nﬁne-tuning DistilBERT on SQuAD using a BERT model previously ﬁne-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\n3\x0cTable 4: Ablation study. Variations are relative to the model trained with triple loss and teacher\nweights initialization.\nAblation Variation on GLUE macro-score\n∅-Lcos-Lmlm -2.96\nLce-∅-Lmlm -1.46\nLce-Lcos-∅ -0.31\nTriple loss + random weights initialization -3.69\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two\nsuccessive steps of distillation, one during the pre-training phase and one during the adaptation phase.\nIn this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and\n70.4 EM, i.e. within 3 points of the full model.\nSize and ', '/ 87.4 86.5 (89.0 / 85.5)\nXLNet (Yang et al., 2019) 2 90.8 / 90.8 92.3 95.1 / 89.7 90.6 / 87.9 85.4 (88.6 / 84.0)\nMegatron-336M 1 89.7 / 90.0 92.3 94.2 / 88.0 88.1 / 84.8 83.0 (86.9 / 81.5)\nMegatron-1.3B 1 90.9 / 91.0 92.6 94.9 / 89.1 90.2 / 87.1 87.3 (90.4 / 86.1)\nMegatron-3.9B 1 91.4 / 91.4 92.7 95.5 / 90.0 91.2 / 88.5 89.5 (91.8 / 88.6)\nALBERT ensemble (Lan et al., 2019) 95.5 / 90.1 91.4 / 88.9 89.4 (91.2 / 88.6)\nMegatron-3.9B ensemble 95.8 / 90.5 91.7 / 89.0 90.9 (93.1 / 90.0)\nFigure 7. Training loss for BERT model using the original architec-\nture (a) and the rearranged architecture (b). Left ﬁgure shows the\ntraining loss for 336M and 752M BERT model. While the original\narchitecture performs well on the 336M model, the modiﬁcations\nin (b) enable stable training with lower training loss.\nUsing the architecture change in Figure 7(b), we consider\nthree different cases as detailed in Table 4. The 336M model\nhas the same size as BERT-large. The 1.3B is the same as\nthe BERT-xlarge conﬁguration that was previously shown\nto get worse results than the 336M BERT-large model (Lan\net al., 2019). We further scale the BERT model using both\nlarger hidden size as well as more layers to arrive at the 3.9B\nparameter case. In all cases, the hidden size per attention\nhead is kept constant at 64. 336M and 1.3B models are\ntrained for 2 million iterations while the 3.9B model is\ntrained for 1.5 million iterations and is still training.\nOn a 3% held-out set, 336M, 1.3B, and 3.9B models achieve\nvalidation set perplexity ']","DistilBERT was trained on the same English Wikipedia and Toronto Book Corpus as BERT, using 8 NVIDIA V100 GPUs for approximately 90 hours, showcasing efficient resource use compared to larger models.","DistilBERT was trained on the same corpus as the original BERT model, which is a concatenation of English Wikipedia and Toronto Book Corpus. It was trained on 8 16GB V100 GPUs for approximately 90 hours. In comparison, the RoBERTa model required 1 day of training on 1024 32GB V100.",0.8,1.0,0.8759269118309021
"What findings were revealed about model performance on HellaSwag when evaluated in zero-shot scenarios, and what implications does this have for future model development?","['blue, incorrect model\npredictions are red. The right answers are bolded .\n13\x0cFigure 13: Examples on the in-domain validation set of HellaSwag , grouped by category label. Our evaluation\nsetup equally weights performance on categories seen during training as well as out-of-domain.\n14', 'GPU memory\nrequirement, we were only able to complete one\nnumber of shots.)\nE Additional Figures Discussed in the\nMain Text\n4 8 16 32 64 128 2560.550.60.650.70.750.80.85 instructive\nirrelevant\nNumber of ShotsRTE V alidation Accuracy\nFigure 14: ALBERT on RTE. Models trained with irrel-\nevant templates actually slightly outperform the instruc-\ntive templates, albeit without statistical signiﬁcance at\nany number of shots.\n4 8 16 32 64 128 2560.50.550.60.650.70.750.80.85 instructive\nmisleading-moderate\nmisleading-extreme\nNumber of ShotsRTE V alidation Accuracy\nFigure 15: ALBERT on RTE. There is no statistical sig-\nniﬁcance between misleading-extreme and instructive\nat any number of shots. In contrast, models trained with\nmisleading-moderate templates are signiﬁcantly worse\nthan the instructive ones from 16 to 64 shots.\nyes-no yes-no-like arbitrary reversed0.450.50.550.60.650.70.75Template Category\ninstructive\nirrelevant\nmisleading-moderate\nmisleading-extreme\nnull\nLM T arget CategoryRTE V alidation AccuracyFigure 16: Median accuracies of all template-target\ncombinations at 32 shots. In general, the choice of tar-\nget words (x-axis groups) matters much more than the\nchoice of templates (colors).\n18\x0cF All Prompts\nF.1 Main Experiment Templates\ncategory template adapted from\ninstructive{premise} Using only the above description and what you know about the world,\n""{hypothesis}"" is deﬁnitely correct. Yes or no?Williams et al. (2018, p. 3)\ninstructive {premise} \\nquestion: {hypothesis}Yes or no? \\nanswer: Brown et al. (2020, p. 59)\ninstructive {premise} Are we justiﬁed in saying that ""{hypothesis}""?\ninstructive Given {premise} Should we assume that ""{hypothesis}"" is true?\ninstructive {premise} Based on the previous passage, is it true that ""{hypothesis}""?\ninstructive Given {premise} Is it guaranteed true that ""{hypothesis}""?\ninstructive Suppose {premise} Can we infer that ""{hypothesis}""?\ninstructive Given that {premise} Does it follow that ""{hypothesis}""?\ninstructive {premise} Question: Does this imply that ""{hypothesis}""?\ninstructive Given that {premise} Therefore, it must be true that ""{hypothesis}""?\nmisleading-moderate {premise} Do most of the above words appear in the following ', 'commonsense knowledge. OPT’s results are not included due to the reason described in\nAppendix C.3.\nC.12 F IXED LABEL DATASETS : A C ASE STUDY IN NATURAL LANGUAGE INFERENCE\nAs is discussed in Section 5, we adopt a rather strict criterion for selecting datasets for zero/few-shot\nlearning in GLM-130B’s evaluation due to the use of MIP. Nevertheless, the criterion significantly\nreduces the dataset we could currently evaluate, and especially some readers have doubted whether\nthe restriction of not evaluating on MIP-seen fixed-label datasets is necessary (e.g., natural language\ninference (NLI)), and suggest that we may report them in an independent section to avoid confusion.\nFrankly speaking, in such a setting GLM-130B’s zero/few-shot learning could be quite advanta-\ngeous. Below, we take NLI as a typical example to show GLM-130B’s outperformance in the\nscenarios. We include 6 widely-used NLI datasets–which are not incorporated in GLM-130B’s MIP\ntraining, as the benchmarks. The results are presented in Table 20, which shows that GLM-130B’s\n“zero-shot” performance could be much better due to the seen task type.\nTable 20: “Zero-shot” results of GLM-130B on 6 typical natural language inference (NLI) datasets.\n∗DISCLAIMER: Despite the datasets are never seen, some other NLI datasets have been in-\ncluded in GLM-130B’s MIP, making it different from the existing standard zero-shot setting.\nBLOOM 176B OPT 175B GLM-130B∗\nqnli (valid, median of 5 prompts) 50.9 55.4 86.7\nmnli (valid, median of 15 prompts) 35.5 36.0 85.7\nmnli_mismatched (valid, median of 15 prompts) 35.5 36.0 84.6\nwnli (valid, median of 5 prompts) 57.7 53.5 67.6\nglue/cola (valid, median of 5 prompts) 39.0 44.4 57.6\nglue/mrpc (valid, median of 5 prompts) 31.6 44.6 87.3\nC.13 S UPER GLUE\nWe also report our ']","Models showed a decline in performance in zero-shot scenarios, highlighting the need for future models to improve generalization capabilities and adapt to unseen contexts or domains.",None,0.8,0.0,0.16318365931510925
Describe RoBERTa's approach to training with large mini-batches and its effect on model optimization and performance.,"['0.98\nGradient Clipping 0.0 0.0\nTable 9: Hyperparameters for pretraining RoBERTa LARGE and RoBERTa BASE.\nHyperparam RACE SQuAD GLUE\nLearning Rate 1e-5 1.5e-5 {1e-5, 2e-5, 3e-5 }\nBatch Size 16 48 {16, 32}\nWeight Decay 0.1 0.01 0.1\nMax Epochs 4 2 10\nLearning Rate Decay Linear Linear Linear\nWarmup ratio 0.06 0.06 0.06\nTable 10: Hyperparameters for ﬁnetuning RoBERTa LARGE on RACE, SQuAD and GLUE.', 'data parallelism and handling random number\ngeneration.\nB.1. Hybrid Model and Data Parallelism\nModel parallelism is orthogonal to data parallelism, and so\nwe can use both simultaneously to train large models in a\nreasonable amount of time. Figure 8 shows a grouping of\nGPUs for hybrid model and data parallelism. Two or more\nGPUs within the same server form model parallel groups\n(for example GPUs 1 to 8 in Figure 8), and contain oneTable 6. Hyperparameters for ﬁnetuning BERT model on down-\nstream tasks.\nTask Model Batch Learning Training\nsize rate epochs\n336M\nMNLI 1.3B 128 1e-5 10\n3.8B\n336M 128 5e-5\nQQP 1.3B 128 3e-5 12\n3.8B 256 4e-5\n336M 64 3e-5\nSQUAD 1.1 1.3B 48 3e-5 2\n3.8B 48 1e-5\n336M 48 3e-5\nSQUAD 2.0 1.3B 64 3e-5 2\n3.8B 48 1e-5\n336M 32 2e-5\nRACE 1.3B 16 1e-5 3\n3.8B 32 2e-5\ninstance of the model distributed across these GPUs. The\nremaining GPUs, which could be within the same server but\nmore typically are located in other servers, run additional\nmodel parallel groups. GPUs with the same position in each\nof the model parallel groups (for example GPUs 1, 9, ...,\n505 in Figure 8) form data parallel groups so that all GPUs\nwithin a data parallel group hold the same model param-\neters. During back propagation we run multiple gradient\nall-reduce operations in parallel to reduce weight gradients\nwithin each distinct data parallel group. The total number\nof required GPUs is the product of the number of model\nand data parallel groups. For example, for the 8.3 billion\nparameter model we use 8 GPUs per model parallel group\nand 64-way data parallelism, for a total of 512 GPUs. All\ncommunication is implemented in PyTorch by Python calls\nto NCCL. GPUs within each model parallel ', 'RoBERTa is trained with dynamic mask-\ning (Section 4.1),FULL -SENTENCES without NSP\nloss (Section 4.2), large mini-batches (Section 4.3)\nand a larger byte-level BPE (Section 4.4).\nAdditionally, we investigate two other impor-\ntant factors that have been under-emphasized in\nprevious work: (1) the data used for pretraining,\nand (2) the number of training passes through the\ndata. For example, the recently proposed XLNet\narchitecture ( Yang et al. ,2019 ) is pretrained us-\ning nearly 10 times more data than the original\nBERT ( Devlin et al. ,2019 ). It is also trained with\na batch size eight times larger for half as many op-\ntimization steps, thus seeing four times as many\nsequences in pretraining compared to BERT.\nTo help disentangle the importance of these fac-\ntors from other modeling choices (e.g., the pre-\ntraining objective), we begin by training RoBERTa\nfollowing the BERT LARGE architecture ( L= 24 ,\nH= 1024 ,A= 16 , 355M parameters). We\npretrain for 100K steps over a comparable B OOK -\nCORPUS plus W IKIPEDIA dataset as was used in\x0cModel data bsz stepsSQuADMNLI-m SST-2(v1.1/2.0)\nRoBERTa\nwith B OOKS + W IKI 16GB 8K 100K 93.6/87.3 89.0 95.3\n+ additional data ( §3.2) 160GB 8K 100K 94.0/87.7 89.3 95.6\n+ pretrain longer 160GB 8K 300K 94.4/88.7 90.0 96.1\n+ pretrain even longer 160GB 8K 500K 94.6/89.4 90.2 96.4\nBERT LARGE\nwith B OOKS + W IKI 13GB 256 1M 90.9/81.8 86.6 93.7\nXLNet LARGE\nwith B OOKS + W IKI 13GB 256 1M 94.0/87.8 88.4 94.4\n+ additional data 126GB 2K 500K 94.5/88.8 89.8 95.6\nTable 4: Development set results for RoBERTa as we pretrain o ver more data (16GB →160GB of text) and pretrain\nfor longer (100K →300K→500K steps). Each row ']","RoBERTa's training with large mini-batches, significantly larger than those used for BERT, improves both optimization speed and end-task performance. This approach, facilitated by appropriate scaling of the learning rate, enables more efficient training and better generalization across tasks.","RoBERTa's approach to training with large mini-batches involves using dynamic masking, full sentences without the Next Sentence Prediction (NSP) loss, and a larger byte-level Byte Pair Encoding (BPE). This approach allows RoBERTa to efficiently train with large mini-batches, resulting in improved model optimization and performance. It has been shown that training with large mini-batches can lead to better generalization and faster convergence during optimization. By utilizing dynamic masking, full sentences, and a larger BPE, RoBERTa is able to effectively handle the challenges of training with large mini-batches, ultimately enhancing the model's overall performance.",1.0,0.875,0.7555577158927917
What findings does RoBERTa reveal about the efficacy of masked language model (MLM) pretraining under its optimized design choices?,"['to\nhairy ), our masking procedure can be further il-\nlustrated by\n• 80% of the time: Replace the word with the\n[MASK] token, e.g., my dog is hairy →\nmy dog is [MASK]\n• 10% of the time: Replace the word with a\nrandom word, e.g., my dog is hairy →my\ndog is apple\n• 10% of the time: Keep the word un-\nchanged, e.g., my dog is hairy →my dog\nis hairy . The purpose of this is to bias the\nrepresentation towards the actual observed\nword.\nThe advantage of this procedure is that the\nTransformer encoder does not know which words\nit will be asked to predict or which have been re-\nplaced by random words, so it is forced to keep\na distributional contextual representation of ev-\neryinput token. Additionally, because random\nreplacement only occurs for 1.5% of all tokens\n(i.e., 10% of 15%), this does not seem to harm\nthe model’s language understanding capability. In\nSection C.2, we evaluate the impact this proce-\ndure.\nCompared to standard langauge model training,\nthe masked LM only make predictions on 15% of\ntokens in each batch, which suggests that more\npre-training steps may be required for the model\x0cBERT (Ours) Trm Trm Trm\nTrm Trm Trm...\n...Trm Trm Trm\nTrm Trm Trm...\n...OpenAI GPT Lstm ELMo Lstm Lstm Lstm Lstm Lstm Lstm Lstm Lstm Lstm Lstm Lstm T1 T2 TN...\n...\n......\n... E1 E2 EN... T1 T2TN... E1 E2 EN ... T1 T2 TN... E1 E2 EN...Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\nconditioned on ', 'that we only report single-task ﬁne-tuning results\nin this paper. A multitask ﬁne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n15https://gluebenchmark.com/faq\x0cjority class.\nC Additional Ablation Studies\nC.1 Effect of Number of Training Steps\nFigure 5 presents MNLI Dev accuracy after ﬁne-\ntuning from a checkpoint that has been pre-trained\nforksteps. This allows us to answer the following\nquestions:\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh ﬁne-tuning accuracy?\nAnswer: Yes, BERT BASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\nC.2 Ablation for Different Masking\nProcedures\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n200 400 600 800 1,0007678808284\nPre-training Steps (Thousands)MNLI Dev Accuracy\nBERT BASE (Masked LM)\nBERT BASE (Left-to-Right)\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after ﬁne-tuning, starting\nfrom model parameters that have been pre-trained for\nksteps. The x-axis is the value of k.Note that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand ﬁne-tuning, as the [MASK] symbol never ap-\npears during the ﬁne-tuning stage. We report the\nDev results for both MNLI and NER. For ', 'NER,\nwe report both ﬁne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npliﬁed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\nMasking Rates Dev Set Results\nMASK SAME RND MNLI NER\nFine-tune Fine-tune Feature-based\n80% 10% 10% 84.2 95.4 94.9\n100% 0% 0% 84.3 94.9 94.0\n80% 0% 20% 84.1 95.2 94.6\n80% 20% 0% 84.4 95.2 94.7\n0% 20% 80% 83.7 94.8 94.6\n0% 0% 100% 83.6 94.9 94.6\nTable 8: Ablation over different masking strategies.\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe[MASK] symbol for MLM; S AME means that\nwe keep the target token as is; R NDmeans that\nwe replace the target token with another random\ntoken.\nThe numbers in the left part of the table repre-\nsent the probabilities of the speciﬁc strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\nFrom the table it can be seen that ﬁne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the M ASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe R NDstrategy performs much worse than our\nstrategy as well.']","RoBERTa's findings reaffirm the efficacy of MLM pretraining, suggesting that with optimized design choices such as dynamic masking and training on extended sequences, MLM pretraining remains highly competitive with, or superior to, other recently proposed pretraining objectives.","RoBERTa reveals that fine-tuning is surprisingly robust to different masking strategies, but using only the [MASK] strategy was problematic when applying the feature-based approach to Named Entity Recognition (NER). Additionally, using only the random replacement strategy performs much worse than the optimized strategy as well.",1.0,1.0,0.3912748098373413
Describe the triple loss used in DistilBERT's training and its components.,"['of models.\nIn supervised learning, a classiﬁcation model is generally trained to predict an instance class by\nmaximizing the estimated probability of gold labels. A standard training objective thus involves\nminimizing the cross-entropy between the model’s predicted distribution and the one-hot empirical\ndistribution of training labels. A model performing well on the training set will predict an output\ndistribution with high probability on the correct class and with near-zero probabilities on other\nclasses. But some of these ""near-zero"" probabilities are larger than others and reﬂect, in part, the\ngeneralization capabilities of the model and how well it will perform on the test set3.\nTraining loss The student is trained with a distillation loss over the soft target probabilities of\nthe teacher: Lce=∑\niti∗log(si)where ti(resp. si) is a probability estimated by the teacher\n(resp. the student). This objective results in a rich training signal by leveraging the full teacher\ndistribution. Following Hinton et al. [2015] we used a softmax-temperature :pi=exp(zi/T)∑\njexp(zj/T)\nwhere Tcontrols the smoothness of the output distribution and ziis the model score for the class i.\nThe same temperature Tis applied to the student and the teacher at training time, while at inference,\nTis set to 1 to recover a standard softmax .\nThe ﬁnal training objective is a linear combination of the distillation loss Lcewith the supervised\ntraining loss, in our case the masked language modeling lossLmlm [Devlin et al., 2018]. We found it\nbeneﬁcial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student\nand teacher hidden states vectors.\n3 DistilBERT: a distilled version of BERT\nStudent architecture In the present work, the student - DistilBERT - has ', 'than the standard deﬁnition, as it\ndepends on the loss in a whole neighborhood of ˆwof size∝Λ, rather than from the derivatives of the loss at a point.\nTo further make the estimation more robust, and to reduce the number of parameters, we constrain Λto be diagonal, and\nconstrain weights wijbelonging to the same ﬁlter to have the same precision Λij. Optimization of this loss can be performed\neasily using Stochastic Gradient Variational Bayes, and in particular using the local reparametrization trick of [18].\nThe prior precision λ2should be picked according to the scale of the weights of each layer. In practice, since the weights\nof each layer have a different scale, we found it useful to select a different λ2for each layer, and train it together with Λ,\nC. Details of the experiments\nC.1. Training of experts and classiﬁers\nGiven a task, we train an expert on it by ﬁne-tuning an off-the-shelf ResNet-34 pretrained on ImageNet1. Fine-tuning is\nperformed by ﬁrst ﬁxing the weights of the network and retraining from scratch only the ﬁnal classiﬁer for 10 epochs using\nAdam, and then ﬁne-tuning all the network together with SGD for 60 epochs with weight decay 5e-4, starting from learning\nrate 0.001 and decreasing it by a factor 0.1 at epochs 40.\nGiven an expert, we train a classiﬁer on top of it by replacing the ﬁnal classiﬁcation layer and training it with Adam for\n16 epochs. We use weight decay 5e-4 and learning rate 1e-4.\nThe tasks we train on generally have different number of samples and unbalanced classes. To limit the impact of this\nimbalance on the training procedure, regardless ', 'use the RMSNorm normalizing func-\ntion, introduced by Zhang and Sennrich (2019).\nSwiGLU activation function [PaLM]. We re-\nplace the ReLU non-linearity by the SwiGLU ac-\ntivation function, introduced by Shazeer (2020) to\nimprove the performance. We use a dimension of\n2\n34dinstead of 4das in PaLM.\nRotary Embeddings [GPTNeo]. We remove the\nabsolute positional embeddings, and instead, add\nrotary positional embeddings (RoPE), introduced\nby Su et al. (2021), at each layer of the network.\nThe details of the hyper-parameters for our dif-\nferent models are given in Table 2.\n2.3 Optimizer\nOur models are trained using the AdamW opti-\nmizer (Loshchilov and Hutter, 2017), with the fol-\nlowing hyper-parameters: β1= 0.9,β2= 0.95.\nWe use a cosine learning rate schedule, such that\nthe ﬁnal learning rate is equal to 10% of the maxi-\nmal learning rate. We use a weight decay of 0.1and\ngradient clipping of 1.0. We use 2,000warmup\n0 200 400 600 800 1000 1200 1400\nBillion of tokens1.51.61.71.81.92.02.12.2Training lossLLaMA 7B\nLLaMA 13B\nLLaMA 33B\nLLaMA 65BFigure 1: Training loss over train tokens for the 7B,\n13B, 33B, and 65 models. LLaMA-33B and LLaMA-\n65B were trained on 1.4T tokens. The smaller models\nwere trained on 1.0T tokens. All models are trained\nwith a batch size of 4M tokens.\nsteps, and vary the learning rate and batch size with\nthe size of the model (see Table 2 for details).\n2.4 Efﬁcient implementation\nWe make several optimizations to improve the train-\ning speed of our models. First, we use an efﬁcient\nimplementation of the causal multi-head attention\nto reduce memory usage and runtime. This imple-\nmentation, available in the xformers library,2is\ninspired by Rabe and Staats (2021) and uses the\nbackward from Dao et al. (2022). This is achieved\nby not storing the attention weights and ']","The triple loss in DistilBERT's training combines distillation loss (Lce), masked language modeling loss (Lmlm), and cosine embedding loss (Lcos), aiming to retain performance while reducing model size.",None,1.0,0.0,0.0734001100063324
What advantages does DistilBERT present for on-device computations and mobile applications?,"['inference speed\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number\nof parameters of each model along with the inference time needed to do a full pass on the STS-\nB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1.\nDistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT.\nOn device computation We studied whether DistilBERT could be used for on-the-edge applications\nby building a mobile application for question answering. We compare the average inference time on\na recent smartphone (iPhone 7 Plus) against our previously trained question answering model based\non BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole\nmodel weighs 207 MB (which could be further reduced with quantization). Our code is available5.\n4.2 Ablation study\nIn this section, we investigate the inﬂuence of various components of the triple loss and the student\ninitialization on the performances of the distilled model. We report the macro-score on GLUE. Table 4\npresents the deltas with the full triple loss: removing the Masked Language Modeling loss has little\nimpact while the two distillation losses account for a large portion of the performance.\n5 Related work\nTask-speciﬁc distillation Most of the prior works focus on building task-speciﬁc distillation se-\ntups. Tang et al. [2019] transfer ﬁne-tune classiﬁcation model BERT to an LSTM-based classiﬁer.\nChatterjee [2019] distill BERT model ﬁne-tuned on SQuAD in a smaller Transformer model previ-\nously initialized from BERT. In the present work, we found it beneﬁcial to use a general-purpose\npre-training distillation rather than a task-speciﬁc distillation. Turc et al. [2019] use ', 'scaling these models’ computational requirements as mentioned\nin Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device\nin real-time has the potential to enable novel and interesting language processing applications, the\ngrowing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM ( https://nv-adlr.github.io/MegatronLM )\nEMC^2: 5th Edition Co-located with NeurIPS’19arXiv:1910.01108v4 [cs.CL] 1 Mar 2020\x0cIn this paper, we show that it is possible to reach similar performances on many downstream-tasks\nusing much smaller language models pre-trained with knowledge distillation, resulting in models\nthat are lighter and faster at inference time, while also requiring a smaller computational training\nbudget. Our general-purpose pre-trained models can be ﬁne-tuned with good performances on several\ndownstream tasks, keeping the ﬂexibility of larger models. We also show that our compressed models\nare small enough to run on the edge, e.g. on mobile devices.\nUsing a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained\nthrough distillation via the supervision of a bigger Transformer language model can achieve similar\nperformance on a variety of downstream tasks, while being 60% faster at inference time. Further\nablation studies indicate that all the components of the triple loss are important for best performances.\nWe have made the trained weights available along with the training code in the Transformers2\nlibrary from HuggingFace [Wolf et al., 2019].\n2 Knowledge distillation\nKnowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which\na compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher -\nor an ensemble ', 'studies of LLMs’ theory, capacity, and flaws. Researchers can also modify the\nmodel architecture and weights, to validate the proposed algorithms to improve LLMs Zhu et al.\n(2020); Cao et al. (2021); Hase et al. (2021); Mitchell et al. (2022).\nWith INT4 quantization, GLM-130B can perform inference on popularized GPUs such as 4 ×RTX\n3090 or 8 ×RTX 2080 Ti, which can be easily accessed from cloud service. As a result, researchers\nwho cannot afford powerful data-center GPU servers like DGX-A100 can also utilize GLM-130B.\nG.2 I MPACT ON INDIVIDUAL DEVELOPERS AND SMALL COMPANIES\nCurrently, individual developers and small companies who want to integrate LLMs into their busi-\nness can only choose paid inference APIs. The increased cost can hinder their attempts. Instead,\nGLM-130B can be deployed on popularized hardware that they own or can access via cloud service\nto reduce the cost. Furthermore, they can utilize distillation techniques Sanh et al. (2019); Jiao et al.\n(2020) to obtain smaller models that preserve comparable performance on their specific tasks. While\nsome developers may lack the ability to complete deployment and distillation on their own, we be-\nlieve with GLM-130B and more open LLMs in the future, the corresponding toolkits and service\nproviders will become more available.\nWe also note that currently most applications of LLMs are based on prompt engineering, partly\ndue to the limitation of inference APIs. In downstream scenarios such as online customer service,\nthe companies accumulate huge amounts of human-generated data that contain domain knowledge.\nWith the open-source weights and code, developers can finetune GLM-130B on their own data to\nmitigate the gap of domain knowledge.\nG.3 S OCIAL IMPACT\nLarge language models, together ']","DistilBERT's reduced size and faster inference capabilities make it highly suitable for on-device computations, demonstrated through a mobile application for question answering.","DistilBERT is advantageous for on-device computations and mobile applications because it is 60% faster than BERT and is 71% faster than BERT on a recent smartphone like iPhone 7 Plus. Additionally, the whole model of DistilBERT weighs 207 MB, which can be further reduced with quantization.",1.0,1.0,0.741051971912384
"In what ways does HellaSwag expand upon its predecessor, SWAG, to offer a more rigorous test of AI commonsense reasoning?","['HellaSwag : Can a Machine Really Finish Your Sentence?\nRowan Zellers♠Ari Holtzman♠Yonatan Bisk♠Ali Farhadi♠♥Yejin Choi♠♥\n♠Paul G. Allen School of Computer Science & Engineering, University of Washington\n♥Allen Institute for Artiﬁcial Intelligence\nhttps://rowanzellers.com/hellaswag\nAbstract\nRecent work by Zellers et al. (2018) intro-\nduced a new task of commonsense natural lan-\nguage inference : given an event description\nsuch as “A woman sits at a piano,” a machine\nmust select the most likely followup: “She\nsets her ﬁngers on the keys.” With the intro-\nduction of BERT (Devlin et al., 2018), near\nhuman-level performance was reached. Does\nthis mean that machines can perform human\nlevel commonsense inference?\nIn this paper, we show that commonsense in-\nference still proves di ﬃcult for even state-\nof-the-art models, by presenting HellaSwag ,\na new challenge dataset. Though its ques-\ntions are trivial for humans ( ą95% accuracy),\nstate-of-the-art models struggle ( ă48%). We\nachieve this via Adversarial Filtering (AF), a\ndata collection paradigm wherein a series of\ndiscriminators iteratively select an adversarial\nset of machine-generated wrong answers. AF\nproves to be surprisingly robust. The key in-\nsight is to scale up the length and complex-\nity of the dataset examples towards a critical\n‘Goldilocks’ zone wherein generated text is\nridiculous to humans, yet often misclassiﬁed\nby state-of-the-art models.\nOur construction of HellaSwag , and its result-\ning di ﬃculty, sheds light on the inner work-\nings of deep pretrained models. More broadly,\nit suggests a new path forward for NLP re-\nsearch, in which benchmarks co-evolve with\nthe evolving state-of-the-art in an adversarial\nway, so as to present ever-harder challenges.\n1 Introduction\nImagine a woman chasing a dog around outside,\ntrying to give it a bath. What might happen next?\nHumans can read a narrative like this, shown in\nFigure 1, and connect ', 'helpful comments: Oyvind Tafjord, Jan Leike, David\nKrueger, Alex Tamkin, Girish Sastry, and Henry Zhu. DH is supported by the NSF GRFP Fellowship\nand an Open Philanthropy Project Fellowship. This research was also supported by the NSF Frontier\nAward 1804794.\nREFERENCES\nM. G. Bellemare, Y . Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An\nevaluation platform for general agents (extended abstract). J. Artif. Intell. Res. , 47:253–279, 2013.\nY . Bisk, R. Zellers, R. L. Bras, J. Gao, and Y . Choi. Piqa: Reasoning about physical commonsense in\nnatural language, 2019.\nY . Bisk, A. Holtzman, J. Thomason, J. Andreas, Y . Bengio, J. Chai, M. Lapata, A. Lazaridou, J. May,\nA. Nisnevich, N. Pinto, and J. Turian. Experience grounds language, 2020.\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\nJ. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models\nare few-shot learners, 2020.\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have\nsolved question answering? try arc, the ai2 reasoning challenge. ArXiv , abs/1803.05457, 2018.\nP. Clark, O. Etzioni, D. Khashabi, T. Khot, B. D. Mishra, K. Richardson, A. Sabharwal, C. Schoenick,\nO. Tafjord, N. Tandon, S. Bhakthavatsalam, D. Groeneveld, M. Guerquin, and M. Schmitz. From ’f’\nto ’a’ on the n.y. regents science exams: An overview of the aristo project. ArXiv , ', 'As we have included TriviaQA (Joshi et al., 2017) and WebQuestions (Berant et al.,\n2013) in the MIP training, here we choose Natural Questions (Kwiatkowski et al., 2019) and Strat-\negyQA (Geva et al., 2021) as the evaluation datasets for CBQA.\nThe results are presented in Table 18. GLM-130B performs relatively poorer on Natural Questions\nand performs well on StrategyQA. GLM-130B’s underperformance on Natural Questions, we spec-\nulate, potentially derives from the insufficiency fitting on English corpora, as it roughly only viewed\n43\x0cPublished as a conference paper at ICLR 2023\n200B English tokens and thus does not memorize the detailed knowledge very well. Since CBQA\nseems to be a task that especially stresses memorization, as is indicated by Chinchilla (Hoffmann\net al., 2022)’s a strong performance, we think with sufficient training later, GLM-130B can perform\nbetter.\nC.11 C OMMONSENSE REASONING\nHere we evaluate GLM-130B and some other LLMs on commonsense reasoning abilities. As we\nhave included PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), and OpenbookQA (Mihaylov\net al., 2018) in the MIP training, we select another two widely adopted commonsense reasoning\ndatasets in our evaluation: Commonsense QA (Talmor et al., 2019) and Multiple-choice Temporal\nCommonsense (MC-TACO, Zhou et al. (2019)). For Commonsense QA, we test the GPT-3 via\nOpenAI Davinci API, BLOOM-176B via its Huggingface Implementation, and GLM-130B using\nthe prompt “answer_given_question_without_options” from promptsource (Bach et al., 2022). For\nStrategyQA, we follow the EM computation method provided in (Zhou et al., 2019).\nThe results are shown in Table 19. As we can see, GLM-130B performs the best on both Com-\nmonsense QA and MC-TACO across evaluated LLMs, demonstrating that GLM-130B has a good\ngrasp of ']","HellaSwag expands on SWAG by incorporating longer and more diverse contexts from WikiHow articles, introducing more complex and varied scenarios for testing commonsense reasoning.","HellaSwag uses Adversarial Filtering (AF) to collect a dataset with examples that are more difficult for state-of-the-art models to classify, making it a more challenging test of AI commonsense reasoning compared to SWAG.",0.75,1.0,0.6643863320350647
How does RoBERTa's use of a byte-level BPE (Byte Pair Encoding) vocabulary contribute to its model architecture and performance?,"['diverse cor-\npora, such as the ones considered in this work.\nRadford et al. (2019 ) introduce a clever imple-\nmentation of BPE that uses bytes instead of uni-\ncode characters as the base subword units. Using\nbytes makes it possible to learn a subword vocab-\nulary of a modest size (50K units) that can still en-\ncode any input text without introducing any “un-\nknown” tokens.\n8Large batch training can improve training efﬁciency even\nwithout large scale parallel hardware through gradient ac-\ncumulation , whereby gradients from multiple mini-batches\nare accumulated locally before each optimization step. Thi s\nfunctionality is supported natively in FAIRSEQ (Ott et al. ,\n2019 ).The original BERT implementa-\ntion ( Devlin et al. ,2019 ) uses a character-level\nBPE vocabulary of size 30K, which is learned\nafter preprocessing the input with heuristic tok-\nenization rules. Following Radford et al. (2019 ),\nwe instead consider training BERT with a larger\nbyte-level BPE vocabulary containing 50K sub-\nword units, without any additional preprocessing\nor tokenization of the input. This adds approxi-\nmately 15M and 20M additional parameters for\nBERT BASEand BERT LARGE , respectively.\nEarly experiments revealed only slight dif-\nferences between these encodings, with the\nRadford et al. (2019 ) BPE achieving slightly\nworse end-task performance on some tasks. Nev-\nertheless, we believe the advantages of a univer-\nsal encoding scheme outweighs the minor degre-\ndation in performance and use this encoding in\nthe remainder of our experiments. A more de-\ntailed comparison of these encodings is left to fu-\nture work.\n5 RoBERTa\nIn the previous section we propose modiﬁcations\nto the BERT pretraining procedure that improve\nend-task performance. We now aggregate these\nimprovements and evaluate their combined im-\npact. We call this conﬁguration RoBERTa for\nRobustly optimized BERT approach. Speciﬁ-\ncally, ', 'equivalent in computa-\ntional cost, via gradient accumulation, to training\nfor 125K steps with a batch size of 2K sequences,\nor for 31K steps with a batch size of 8K.\nIn Table 3we compare perplexity and end-\x0cbsz steps lr ppl MNLI-m SST-2\n256 1M 1e-4 3.99 84.7 92.7\n2K 125K 7e-4 3.68 85.2 92.9\n8K 31K 1e-3 3.77 84.6 92.8\nTable 3: Perplexity on held-out training data ( ppl) and\ndevelopment set accuracy for base models trained over\nBOOK CORPUS and W IKIPEDIA with varying batch\nsizes ( bsz). We tune the learning rate ( lr) for each set-\nting. Models make the same number of passes over the\ndata (epochs) and have the same computational cost.\ntask performance of BERT BASE as we increase the\nbatch size, controlling for the number of passes\nthrough the training data. We observe that train-\ning with large batches improves perplexity for the\nmasked language modeling objective, as well as\nend-task accuracy. Large batches are also easier to\nparallelize via distributed data parallel training,8\nand in later experiments we train with batches of\n8K sequences.\nNotably You et al. (2019 ) train BERT with even\nlarger batche sizes, up to 32K sequences. We leave\nfurther exploration of the limits of large batch\ntraining to future work.\n4.4 Text Encoding\nByte-Pair Encoding (BPE) ( Sennrich et al. ,2016 )\nis a hybrid between character- and word-level rep-\nresentations that allows handling the large vocab-\nularies common in natural language corpora. In-\nstead of full words, BPE relies on subwords units,\nwhich are extracted by performing statistical anal-\nysis of the training corpus.\nBPE vocabulary sizes typically range from\n10K-100K subword units. However, unicode char-\nacters can account for a sizeable portion of this\nvocabulary when modeling large and ', 'RoBERTa is trained with dynamic mask-\ning (Section 4.1),FULL -SENTENCES without NSP\nloss (Section 4.2), large mini-batches (Section 4.3)\nand a larger byte-level BPE (Section 4.4).\nAdditionally, we investigate two other impor-\ntant factors that have been under-emphasized in\nprevious work: (1) the data used for pretraining,\nand (2) the number of training passes through the\ndata. For example, the recently proposed XLNet\narchitecture ( Yang et al. ,2019 ) is pretrained us-\ning nearly 10 times more data than the original\nBERT ( Devlin et al. ,2019 ). It is also trained with\na batch size eight times larger for half as many op-\ntimization steps, thus seeing four times as many\nsequences in pretraining compared to BERT.\nTo help disentangle the importance of these fac-\ntors from other modeling choices (e.g., the pre-\ntraining objective), we begin by training RoBERTa\nfollowing the BERT LARGE architecture ( L= 24 ,\nH= 1024 ,A= 16 , 355M parameters). We\npretrain for 100K steps over a comparable B OOK -\nCORPUS plus W IKIPEDIA dataset as was used in\x0cModel data bsz stepsSQuADMNLI-m SST-2(v1.1/2.0)\nRoBERTa\nwith B OOKS + W IKI 16GB 8K 100K 93.6/87.3 89.0 95.3\n+ additional data ( §3.2) 160GB 8K 100K 94.0/87.7 89.3 95.6\n+ pretrain longer 160GB 8K 300K 94.4/88.7 90.0 96.1\n+ pretrain even longer 160GB 8K 500K 94.6/89.4 90.2 96.4\nBERT LARGE\nwith B OOKS + W IKI 13GB 256 1M 90.9/81.8 86.6 93.7\nXLNet LARGE\nwith B OOKS + W IKI 13GB 256 1M 94.0/87.8 88.4 94.4\n+ additional data 126GB 2K 500K 94.5/88.8 89.8 95.6\nTable 4: Development set results for RoBERTa as we pretrain o ver more data (16GB →160GB of text) and pretrain\nfor longer (100K →300K→500K steps). Each row ']","RoBERTa adopts a byte-level BPE vocabulary, allowing for more efficient handling of large and diverse corpora without the need for additional preprocessing or tokenization. This encoding scheme supports a modest-sized vocabulary that can encode any input text, potentially enhancing the model's linguistic flexibility and performance on various NLP tasks.","RoBERTa's use of a byte-level BPE vocabulary contributes to its model architecture and performance by allowing it to learn a subword vocabulary of a modest size (50K units) that can encode any input text without introducing unknown tokens. This byte-level BPE vocabulary adds approximately 15M and 20M additional parameters for BERT BASE and BERT LARGE, respectively. Despite experiencing slightly worse end-task performance on some tasks compared to other encodings, the advantages of a universal encoding scheme outweigh the minor degradation in performance. This encoding scheme helps improve training efficiency and allows RoBERTa to handle large vocabularies common in natural language corpora.",0.6666666666666666,1.0,0.855593740940094
Discuss RoBERTa's contributions to the understanding of effective pretraining strategies in NLP.,"['RoBERTa is trained with dynamic mask-\ning (Section 4.1),FULL -SENTENCES without NSP\nloss (Section 4.2), large mini-batches (Section 4.3)\nand a larger byte-level BPE (Section 4.4).\nAdditionally, we investigate two other impor-\ntant factors that have been under-emphasized in\nprevious work: (1) the data used for pretraining,\nand (2) the number of training passes through the\ndata. For example, the recently proposed XLNet\narchitecture ( Yang et al. ,2019 ) is pretrained us-\ning nearly 10 times more data than the original\nBERT ( Devlin et al. ,2019 ). It is also trained with\na batch size eight times larger for half as many op-\ntimization steps, thus seeing four times as many\nsequences in pretraining compared to BERT.\nTo help disentangle the importance of these fac-\ntors from other modeling choices (e.g., the pre-\ntraining objective), we begin by training RoBERTa\nfollowing the BERT LARGE architecture ( L= 24 ,\nH= 1024 ,A= 16 , 355M parameters). We\npretrain for 100K steps over a comparable B OOK -\nCORPUS plus W IKIPEDIA dataset as was used in\x0cModel data bsz stepsSQuADMNLI-m SST-2(v1.1/2.0)\nRoBERTa\nwith B OOKS + W IKI 16GB 8K 100K 93.6/87.3 89.0 95.3\n+ additional data ( §3.2) 160GB 8K 100K 94.0/87.7 89.3 95.6\n+ pretrain longer 160GB 8K 300K 94.4/88.7 90.0 96.1\n+ pretrain even longer 160GB 8K 500K 94.6/89.4 90.2 96.4\nBERT LARGE\nwith B OOKS + W IKI 13GB 256 1M 90.9/81.8 86.6 93.7\nXLNet LARGE\nwith B OOKS + W IKI 13GB 256 1M 94.0/87.8 88.4 94.4\n+ additional data 126GB 2K 500K 94.5/88.8 89.8 95.6\nTable 4: Development set results for RoBERTa as we pretrain o ver more data (16GB →160GB of text) and pretrain\nfor longer (100K →300K→500K steps). Each row ', 'American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies , pages 2627–2636, On-\nline. Association for Computational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efﬁcient prompt\ntuning. In EMNLP .\n11\x0cHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nThirteenth international conference on the princi-\nples of knowledge representation and reasoning .\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n4582–4597, Online. Association for Computational\nLinguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv preprint , abs/1907.11692.\nRobert L Logan IV , Ivana Balaževi ´c, Eric Wallace,\nFabio Petroni, Sameer Singh, and Sebastian Riedel.\n2021. Cutting down on prompts and parameters:\nSimple few-shot learning with language models.\nArXiv preprint , abs/2106.13353.\nCharles Lovering, Rohan Jha, Tal Linzen, and Ellie\nPavlick. 2021. Predicting inductive biases of pre-\ntrained models. In International Conference on\nLearning Representations .\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 3428–\n3448, Florence, Italy. Association for Computational\nLinguistics.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2021. Metaicl: Learning to learn\nin context. CoRR , abs/2110.15943.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? ', ""4: BERT validation accuracy when trained and\nevaluated under several versions of SWAG, with the\nnew dataset HellaSwag as comparison. We compare:\nEnding Only No context is provided; just the endings.\nShuffled Endings that are indidivually tokenized,\nshuﬄed, and then detokenized.\nShuffled+\nEnding OnlyNo context is provided andeach ending is\nshuﬄed.\nknown approach at the time of writing.4Core to\nour analysis is investigating how a model trained\non Wikipedia and books can be so e ﬀectively ﬁne-\ntuned for SWAG, a dataset from video captions.\n3.1 How much innate knowledge does BERT\nhave about SWAG?\nWe investigate this question by measuring BERT’s\nperformance on SWAG while varying the size of\nthe training dataset; results are shown in Fig-\nure 3. While the best known ELMo NLI model\n(ESIM +ELMo; Chen et al., 2017) requires the en-\ntire training set to reach 59%, BERT outperforms\nthis given only 64 examples. However, BERT still\nneeds upwards of 16k examples to approach hu-\nman performance, around which it plateaus.\n3.2 What is learned during ﬁnetuning?\nFigure 4 compares BERT’s performance when\ntrained and evaluated on variants of SWAG.\nContext: BERT’s performance only slips 11.9\npoints (86.7%Ñ74.8%) when context is omitted\n(Ending Only ), suggesting a bias exists in the\nendings themselves.5If a followup event seems\nunreasonable absent of context , then there must be\nsomething markedly di ﬀerent between the space\nof human-written and machine-generated endings.\nStructure: To distinguish word usage from\n4See the appendix for a discussion of the BERT architec-\nture and hyperparameter settings we used in our experiments.\n5These biases are similar to those in NLI datasets, as\nfound by Gururangan et al. (2018); Poliak et al. (2018).\n3\x0c0 10 20 30 40 50\nActivitynet Adversarial Filtering iteration0255075100BERT accuracy (4-way) Zellers' LM GPT\n0 10 20 30 ""]","RoBERTa's comprehensive evaluation of pretraining strategies, including the effects of dynamic masking, data size, batch size, and the removal of the NSP objective, contributes valuable insights into the design and optimization of language models. Its success offers guidance for future model development, emphasizing the importance of these previously under-explored factors in achieving high performance in NLP tasks.","RoBERTa's contributions to the understanding of effective pretraining strategies in NLP include utilizing dynamic masking, training with full sentences without NSP loss, using large mini-batches, and incorporating a larger byte-level BPE. Additionally, RoBERTa investigates the importance of the data used for pretraining and the number of training passes through the data. By training RoBERTa following the BERT LARGE architecture and pretraining over a comparable dataset, RoBERTa demonstrates the impact of scaling up both the amount of data and the duration of pretraining on model performance. These findings highlight the significance of these factors in improving the effectiveness of pretraining strategies in NLP.",0.4,1.0,0.8431122303009033
"How does Adversarial Filtering (AF) contribute to the creation of HellaSwag, and what unique characteristic does it bring to the dataset?","['(rather than 10%). This helps the\nmodel adapt to SWAG more gradually, with-\nout diverging early on.\nb. For the Adversarial Filtering experiments (for\nboth WikiHow and ActivityNet), we random-\nize some of the hyperaparmeters on each it-\neration. We sample a learning rate between\n1e-5 and4e-5 , using a log-uniform distribu-\ntion. These outer ranges were recommended\nfrom the original BERT paper. Additionally,\nwith probability 0 .5 we use the cased model\n(where the input isn’t originally lowercased\nbefore tokenization), rather than the uncased\nmodel.\nc. During adversarial ﬁltering, we used 3 epochs.\nHowever, we found that adding more epochs\n13The only exception is for the plots where we vary the\nnumber of training examples. In this case, we don’t want\nto disadvantage the trials without much training data (since\nthis would allow for fewer parameter updates). To remedy\nthis, we continue training for 10 epochs and report the best\nvalidation performance over the entire training history.\n11\x0chelped the model during ﬁne-tuning on the ﬁ-\nnal dataset HellaSwag . Our best conﬁguration\nuses 10 epochs.\nd. While ﬁne-tuning on HellaSwag we used a\nlearning rate of 2e-5 .\nE Human validation\nWe performed human validation using the same\nsetup as (Zellers et al., 2018). Humans get six an-\nswers to choose from, of which exactly one is the\ntrue ending and the other ﬁve are from AF. We\nfound that multiple rounds of human validation\nwere especially helpful on ActivityNet. However,\nit helps to do the human validation in an intelli-\ngent way: if the ﬁrst worker is confused, the an-\nswer should be replaced before it goes to the next\nworker. This is a hard problem, so we adopt the\nfollowing approach:\na. We use best practices on mechanical turk, pay-\ning workers fairly ', 'adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) .\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watch-\ning movies and reading books. In arXiv preprint\narXiv:1506.06724 .\n10\x0cSupplemental Material\nA Adversarial Filtering Setup\nIn this subsection, we provide some more details\nregarding the Adversarial Filtering experiments.\nOur version of Adversarial Filtering is mostly\nthe same as Zellers et al. (2018). Details:\na. On each iteration, we split the dataset up into\n80% training and 20% testing. We don’t do\nanything special for this split (like looking at\nthe video /article IDs).\nb. For ActivityNet, we use k“9 assigned in-\ndices for every example. (This corresponds to\nthe number of red columns in Figure 2). For\nWikiHow, we used k“5, since we found\nthat there were fewer good endings produced\nby the generators after scaling up the sequence\nlength.\nc. Similarly to Zellers et al. (2018), we train the\nAF models in a multi-way fashion. Since\nwe use BERT-Large as the discriminator, this\nmatches Devlin et al. (2018)’s model for\nSWAG: on each training example, the model\nis given exactly one positive ending and sev-\neral negative endings, and the model com-\nputes probability distribution over the endings\nthrough a softmax. However, we also wanted\nto always report 4-way probability for simplic-\nity. To do this, we train in a 4-way setting (the\ntraining set is constructed by subsampling 3\nwrong answers from the set of kthat are cur-\nrently assigned to each example). The accu-\nracy values that are reported are done so using\nthe ﬁrst 3 assigned negatives in dataset Dtest.\nd. Sometimes, BERT never converges (accuracy\naround ', 'versa.\nThe last example comes from WikiHow, which\nappears to be incredibly challenging for BERT.\nBERT picks answer d, which has more words that\nmatch the context of technology (planes, tra ﬃc,\nlaptop), but is incoherent.12\n12Among other issues, why would someone suddenly be\naware that they are ‘ﬂying at high speed on a plane...?’\n7\x0cStylistic\nEnsembleELMo+\nLSTMGPT BERTBase BERTLarge30405060708090100 Accuracy (%)\n48.2%53.7%64.8%71.4%83.0%\n28.0% 28.2% 28.4%32.0%41.1%78.5%77.4%\n71.3%\n63.0%\n41.1%Accuracy of the filtering model before AF\nAccuracy of the filtering model after AF\nBERT-Large accuracy after AFFigure 11: Performance on the WikiHow subset of al-\nternative variations of HellaSwag , where di ﬀerent Ad-\nversarial Filters are used (but without human valida-\ntion). We consider the shallow stylistic adversaries\nused by Zellers et al. (2018) (Stylistic Ensemble),\nas well as an LSTM with ELMo embeddings, GPT,\nBERT-Base, and BERT-Large. For each adversarial ﬁl-\ntering model, we record the accuracy of that model be-\nfore and after AF is used. We also evaluate each al-\nternative dataset using BERT-Large. The results sug-\ngest that using a a stronger model at test time (over the\nmodel used for AF) improves performance, but is not\nenough to solve the task.\n6 Discussion\nOur results suggest that HellaSwag is a challenging\ntestbed for state-of-the-art NLI models, even those\nbuilt on extensive pretraining. The question still\nremains, though, of where will the ﬁeld go next?\n6.1 How easy might HellaSwag be for future\ndiscriminators?\nIn this paper, we showed the existence of a\nGoldilocks zone of text complexity – in which\ngenerations are nonsensical, but existing state-\nof-the-art NLP models cannot tell the di ﬀerence.\nHow hard will the dataset be for future, even more\npowerful, models?\nAnswering this question is challenging because\nthese models don’t exist (or are unavailable) at\nthe time of writing . However, ']","AF iteratively selects challenging machine-generated wrong answers by using a series of discriminators, creating a dataset that is robust against model biases and exhibits a 'Goldilocks' zone of complexity.","Adversarial Filtering (AF) contributes to the creation of HellaSwag by training the AF models in a multi-way fashion, where the model is given one positive ending and several negative endings, computing a probability distribution over the endings. It brings the unique characteristic of generating alternative variations of endings for the dataset, improving the overall performance of the model by filtering out incorrect or irrelevant answers.",1.0,1.0,0.5764979124069214
What is the impact of removing the NSP loss on RoBERTa's performance across various benchmarks compared to BERT?,"['/ 87.4 86.5 (89.0 / 85.5)\nXLNet (Yang et al., 2019) 2 90.8 / 90.8 92.3 95.1 / 89.7 90.6 / 87.9 85.4 (88.6 / 84.0)\nMegatron-336M 1 89.7 / 90.0 92.3 94.2 / 88.0 88.1 / 84.8 83.0 (86.9 / 81.5)\nMegatron-1.3B 1 90.9 / 91.0 92.6 94.9 / 89.1 90.2 / 87.1 87.3 (90.4 / 86.1)\nMegatron-3.9B 1 91.4 / 91.4 92.7 95.5 / 90.0 91.2 / 88.5 89.5 (91.8 / 88.6)\nALBERT ensemble (Lan et al., 2019) 95.5 / 90.1 91.4 / 88.9 89.4 (91.2 / 88.6)\nMegatron-3.9B ensemble 95.8 / 90.5 91.7 / 89.0 90.9 (93.1 / 90.0)\nFigure 7. Training loss for BERT model using the original architec-\nture (a) and the rearranged architecture (b). Left ﬁgure shows the\ntraining loss for 336M and 752M BERT model. While the original\narchitecture performs well on the 336M model, the modiﬁcations\nin (b) enable stable training with lower training loss.\nUsing the architecture change in Figure 7(b), we consider\nthree different cases as detailed in Table 4. The 336M model\nhas the same size as BERT-large. The 1.3B is the same as\nthe BERT-xlarge conﬁguration that was previously shown\nto get worse results than the 336M BERT-large model (Lan\net al., 2019). We further scale the BERT model using both\nlarger hidden size as well as more layers to arrive at the 3.9B\nparameter case. In all cases, the hidden size per attention\nhead is kept constant at 64. 336M and 1.3B models are\ntrained for 2 million iterations while the 3.9B model is\ntrained for 1.5 million iterations and is still training.\nOn a 3% held-out set, 336M, 1.3B, and 3.9B models achieve\nvalidation set perplexity ', 'RoBERTa is trained with dynamic mask-\ning (Section 4.1),FULL -SENTENCES without NSP\nloss (Section 4.2), large mini-batches (Section 4.3)\nand a larger byte-level BPE (Section 4.4).\nAdditionally, we investigate two other impor-\ntant factors that have been under-emphasized in\nprevious work: (1) the data used for pretraining,\nand (2) the number of training passes through the\ndata. For example, the recently proposed XLNet\narchitecture ( Yang et al. ,2019 ) is pretrained us-\ning nearly 10 times more data than the original\nBERT ( Devlin et al. ,2019 ). It is also trained with\na batch size eight times larger for half as many op-\ntimization steps, thus seeing four times as many\nsequences in pretraining compared to BERT.\nTo help disentangle the importance of these fac-\ntors from other modeling choices (e.g., the pre-\ntraining objective), we begin by training RoBERTa\nfollowing the BERT LARGE architecture ( L= 24 ,\nH= 1024 ,A= 16 , 355M parameters). We\npretrain for 100K steps over a comparable B OOK -\nCORPUS plus W IKIPEDIA dataset as was used in\x0cModel data bsz stepsSQuADMNLI-m SST-2(v1.1/2.0)\nRoBERTa\nwith B OOKS + W IKI 16GB 8K 100K 93.6/87.3 89.0 95.3\n+ additional data ( §3.2) 160GB 8K 100K 94.0/87.7 89.3 95.6\n+ pretrain longer 160GB 8K 300K 94.4/88.7 90.0 96.1\n+ pretrain even longer 160GB 8K 500K 94.6/89.4 90.2 96.4\nBERT LARGE\nwith B OOKS + W IKI 13GB 256 1M 90.9/81.8 86.6 93.7\nXLNet LARGE\nwith B OOKS + W IKI 13GB 256 1M 94.0/87.8 88.4 94.4\n+ additional data 126GB 2K 500K 94.5/88.8 89.8 95.6\nTable 4: Development set results for RoBERTa as we pretrain o ver more data (16GB →160GB of text) and pretrain\nfor longer (100K →300K→500K steps). Each row ', 'so too is\nend-to-end ﬁnetuning. Freezing BERT-Base and\nadding an LSTM on top lowers its overall perfor-\nmance 4.3%. This may help explain why mod-\nels such as ESIM +ELMo struggled on SWAG, as\nELMo isn’t updated during ﬁnetuning.\nWhile BERT is the best model, it still struggles\nonHellaSwag , and especially so on zero-shot cat-\n9For ELMo and BERT-Base, the model learns scalar\nweights to combine each internal layer of the encoder.\n10This model is trained with binary cross entropy loss.\n6\x0cOverall LSMDC ActivityNet30405060708090100 BERT-Large Accuracy (%)86.7%85.5%88.0%\n71.4%69.0%74.2%Evaluated on SWAG\nOverall WikiHow ActivityNet34.6%\n28.0%48.4%46.4%\n42.9%53.7%Evaluated on HellaSwag\nTrained on...\nSWAG\nHellaSwagFigure 9: Transfer experiments from SWAG to Hella-\nSwag and vice versa, evaluated on the validation sets.\nOverall, a BERT-Large that is trained on SWAG hardly\ngeneralizes to HellaSwag : it scores 34.6%.\negories. Performance drops roughly 5% on the\ntest fold, which suggests that the ﬁnetuning is not\nenough for BERT to learn to generalize to novel\nactivities or how-to categories.\nLast, we see that WikiHow is a much harder do-\nmain that ActivityNet for machines: 45% Bert-\nLarge performance, versus 96.5% for humans.\nCuriously, it is on this source dataset that we see\nthe smallest gap between OpenAI GPT and BERT.\nIn fact, OpenAI GPT outperforms BERT on Wiki-\nHow, but the reverse is true for ActivityNet. One\npossibility is that the left-to-right structure of GPT\nis the right inductive bias for WikiHow - perhaps\nreasoning bidirectionally over long contexts is too\nmuch for a 12-layer transformer to learn.\n5.1 SWAG to HellaSwag transfer\nGiven the shared goals and partial domains of\nSWAG and HellaSwag , it is natural to ask to\nwhat extent models can transfer between the two\ndatasets. In Figure 9 we show the results from\ntransfer experiments: models are trained on ']","Removing the NSP loss from RoBERTa's pretraining objective leads to significant improvements in downstream task performance. This finding challenges the initial hypothesis that NSP is crucial for learning sentence relationships, suggesting that masked language modeling alone, under optimal conditions, is sufficient for achieving superior performance.",RoBERTa's performance across various benchmarks is improved by removing the NSP loss compared to BERT.,1.0,1.0,0.6512536406517029
In what ways does RoBERTa's training process leverage data size and training duration for improved model performance?,"['RoBERTa is trained with dynamic mask-\ning (Section 4.1),FULL -SENTENCES without NSP\nloss (Section 4.2), large mini-batches (Section 4.3)\nand a larger byte-level BPE (Section 4.4).\nAdditionally, we investigate two other impor-\ntant factors that have been under-emphasized in\nprevious work: (1) the data used for pretraining,\nand (2) the number of training passes through the\ndata. For example, the recently proposed XLNet\narchitecture ( Yang et al. ,2019 ) is pretrained us-\ning nearly 10 times more data than the original\nBERT ( Devlin et al. ,2019 ). It is also trained with\na batch size eight times larger for half as many op-\ntimization steps, thus seeing four times as many\nsequences in pretraining compared to BERT.\nTo help disentangle the importance of these fac-\ntors from other modeling choices (e.g., the pre-\ntraining objective), we begin by training RoBERTa\nfollowing the BERT LARGE architecture ( L= 24 ,\nH= 1024 ,A= 16 , 355M parameters). We\npretrain for 100K steps over a comparable B OOK -\nCORPUS plus W IKIPEDIA dataset as was used in\x0cModel data bsz stepsSQuADMNLI-m SST-2(v1.1/2.0)\nRoBERTa\nwith B OOKS + W IKI 16GB 8K 100K 93.6/87.3 89.0 95.3\n+ additional data ( §3.2) 160GB 8K 100K 94.0/87.7 89.3 95.6\n+ pretrain longer 160GB 8K 300K 94.4/88.7 90.0 96.1\n+ pretrain even longer 160GB 8K 500K 94.6/89.4 90.2 96.4\nBERT LARGE\nwith B OOKS + W IKI 13GB 256 1M 90.9/81.8 86.6 93.7\nXLNet LARGE\nwith B OOKS + W IKI 13GB 256 1M 94.0/87.8 88.4 94.4\n+ additional data 126GB 2K 500K 94.5/88.8 89.8 95.6\nTable 4: Development set results for RoBERTa as we pretrain o ver more data (16GB →160GB of text) and pretrain\nfor longer (100K →300K→500K steps). Each row ', 'accumulates improvements from the row s above. RoBERTa\nmatches the architecture and training objective of BERT LARGE . Results for BERT LARGE and XLNet LARGE are from\nDevlin et al. (2019 ) and Yang et al. (2019 ), respectively. Complete results on all GLUE tasks can be fo und in the\nAppendix.\nDevlin et al. (2019 ). We pretrain our model using\n1024 V100 GPUs for approximately one day.\nResults We present our results in Table 4. When\ncontrolling for training data, we observe that\nRoBERTa provides a large improvement over the\noriginally reported BERT LARGE results, reafﬁrming\nthe importance of the design choices we explored\nin Section 4.\nNext, we combine this data with the three ad-\nditional datasets described in Section 3.2. We\ntrain RoBERTa over the combined data with the\nsame number of training steps as before (100K).\nIn total, we pretrain over 160GB of text. We ob-\nserve further improvements in performance across\nall downstream tasks, validating the importance of\ndata size and diversity in pretraining.9\nFinally, we pretrain RoBERTa for signiﬁcantly\nlonger, increasing the number of pretraining steps\nfrom 100K to 300K, and then further to 500K. We\nagain observe signiﬁcant gains in downstream task\nperformance, and the 300K and 500K step mod-\nels outperform XLNet LARGE across most tasks. We\nnote that even our longest-trained model does not\nappear to overﬁt our data and would likely beneﬁt\nfrom additional training.\nIn the rest of the paper, we evaluate our best\nRoBERTa model on the three different bench-\nmarks: GLUE, SQuaD and RACE. Speciﬁcally\n9Our experiments conﬂate increases in data size and di-\nversity. We leave a more careful analysis of these two dimen-\nsions to future work.we consider RoBERTa trained for 500K steps over\nall ', '0.98\nGradient Clipping 0.0 0.0\nTable 9: Hyperparameters for pretraining RoBERTa LARGE and RoBERTa BASE.\nHyperparam RACE SQuAD GLUE\nLearning Rate 1e-5 1.5e-5 {1e-5, 2e-5, 3e-5 }\nBatch Size 16 48 {16, 32}\nWeight Decay 0.1 0.01 0.1\nMax Epochs 4 2 10\nLearning Rate Decay Linear Linear Linear\nWarmup ratio 0.06 0.06 0.06\nTable 10: Hyperparameters for ﬁnetuning RoBERTa LARGE on RACE, SQuAD and GLUE.']","RoBERTa extensively explores the impact of both increased data size and prolonged training duration, demonstrating that both factors significantly contribute to enhanced model performance. Training over larger datasets and for more steps than BERT allows RoBERTa to better generalize and excel on downstream tasks.","RoBERTa leverages data size by pretraining over a larger amount of text, starting with 16GB and increasing to 160GB, resulting in improved performance across all downstream tasks. Additionally, RoBERTa leverages training duration by increasing the number of pretraining steps from 100K to 300K, and then further to 500K, which leads to significant gains in downstream task performance and outperforms XLNet LARGE across most tasks.",1.0,1.0,0.7816301584243774
What defines the Task2Vec embedding's ability to capture the complexity of a visual classification task and its semantic relationships?,"['embedding (distance from origin) and test error obtained on the task.\nnear the decision boundary since p(1−p)is maximized at\np= 1/2. Compare this to the covariance matrix of the data,\nC0, to which all data points contribute equally. Instead, in\nTASK 2VEC information on the domain is based on data near\nthe decision boundary (task-weighted domain embedding).\nEncoding useful features for the task: The FIM de-\npends on the curvature of the loss function with the diagonal\nentries capturing the sensitivity of the loss to model param-\neters. Speciﬁcally, in the two-layer model one can see that,\nif a given feature is uncorrelated with y, the correspond-\ning blocks of Fare zero. In contrast, a domain embedding\nbased on feature activations of the probe network (e.g., C1)\nonly reﬂects which features vary over the dataset without\nindication of whether they are relevant to the task.\n3. Similarity Measures on the Space of Tasks\nWhat metric should be used on the space of tasks? This\ndepends critically on the meta-task we are considering. As a\nmotivation, we concentrate on the meta-task of selecting the\npre-trained feature extractor from a set in order to obtain the\nbest performance on a new training task. There are several\nnatural metrics that may be considered for this meta-task.\nIn this work, we mainly consider:\nTaxonomic distance For some tasks, there is a natural no-\ntion of semantic similarity, for instance deﬁned by sets of\ncategories organized in a taxonomic hierarchy where each\ntask is classiﬁcation inside a subtree of the hierarchy ( e.g.,\nwe may say that classifying breeds of dogs is closer to clas-siﬁcation of cats than it is to classiﬁcation of species of\nplants). In this setting, ', 'TASK2VEC: Task Embedding for Meta-Learning\nAlessandro Achille\nUCLA and AWS\nachille@cs.ucla.eduMichael Lam\nAWS\nmichlam@amazon.comRahul Tewari\nAWS\ntewarir@amazon.comAvinash Ravichandran\nAWS\nravinash@amazon.com\nSubhransu Maji\nUMass and AWS\nsmmaji@amazon.comCharless Fowlkes\nUCI and AWS\nfowlkec@amazon.comStefano Soatto\nUCLA and AWS\nsoattos@amazon.comPietro Perona\nCaltech and AWS\nperonapp@amazon.com\nAbstract\nWe introduce a method to provide vectorial represen-\ntations of visual classiﬁcation tasks which can be used\nto reason about the nature of those tasks and their re-\nlations. Given a dataset with ground-truth labels and a\nloss function deﬁned over those labels, we process images\nthrough a “probe network” and compute an embedding\nbased on estimates of the Fisher information matrix asso-\nciated with the probe network parameters. This provides a\nﬁxed-dimensional embedding of the task that is independent\nof details such as the number of classes and does not require\nany understanding of the class label semantics. We demon-\nstrate that this embedding is capable of predicting task sim-\nilarities that match our intuition about semantic and tax-\nonomic relations between different visual tasks ( e.g., tasks\nbased on classifying different types of plants are similar).\nWe also demonstrate the practical value of this framework\nfor the meta-task of selecting a pre-trained feature extractor\nfor a new task. We present a simple meta-learning frame-\nwork for learning a metric on embeddings that is capable of\npredicting which feature extractors will perform well. Se-\nlecting a feature extractor with task embedding obtains a\nperformance close to the best available feature extractor,\nwhile costing substantially less than exhaustively training\nand evaluating on all available feature extractors.\n1. Introduction\nThe success of Deep Learning hinges in part on the fact\nthat models learned for one task can be used on other related\ntasks. Yet, no general framework exists to describe and\nlearn relations between tasks. We introduce the TASK 2VEC\nembedding, a technique to ', 'represent tasks as elements of a\nvector space based on the Fisher Information Matrix. The\nnorm of the embedding correlates with the complexity of\nthe task, while the distance between embeddings capturessemantic similarities between tasks (Fig. 1). When other\nnatural distances are available, such as the taxonomical dis-\ntance in biological classiﬁcation, we ﬁnd that the embed-\nding distance correlates positively with it (Fig. 2). More-\nover, we introduce an asymmetric distance on tasks which\ncorrelates with the transferability between tasks.\nComputation of the embedding leverages a duality be-\ntween network parameters (weights) and outputs (activa-\ntions) in a deep neural network (DNN): Just as the activa-\ntions of a DNN trained on a complex visual recognition task\nare a rich representation of the input images, we show that\nthe gradients of the weights relative to a task-speciﬁc loss\nare a rich representation of the task itself. Speciﬁcally, given\na task deﬁned by a dataset D={(xi,yi)}N\ni=1of labeled\nsamples, we feed the data through a pre-trained reference\nconvolutional neural network which we call “ probe net-\nwork ”, and compute the diagonal Fisher Information Ma-\ntrix (FIM) of the network ﬁlter parameters to capture the\nstructure of the task (Sect. 2). Since the architecture and\nweights of the probe network are ﬁxed, the FIM provides a\nﬁxed-dimensional representation of the task. We show this\nembedding encodes the “difﬁculty” of the task, character-\nistics of the input domain, and which features of the probe\nnetwork are useful to solve it (Sect. 2.1).\nOur task embedding can be used to reason about the\nspace of tasks and solve meta-tasks. As a motivating exam-\nple, we study the problem of selecting the best pre-trained\nfeature extractor to solve a new ']","Task2Vec embedding utilizes the Fisher Information Matrix (FIM) computed from a probe network's parameters, capturing the task's complexity and semantic relationships by representing tasks as elements in a vector space.","The Task2Vec embedding's ability to capture the complexity of a visual classification task and its semantic relationships is defined by the norm of the embedding correlating with the complexity of the task, while the distance between embeddings captures semantic similarities between tasks. Additionally, the embedding distance positively correlates with natural distances, such as taxonomical distance in biological classification, and an asymmetric distance on tasks which correlates with transferability between tasks.",1.0,1.0,0.7864362001419067
How does Task2Vec's embedding relate to the difficulty and domain characteristics of a task?,"['embedding (distance from origin) and test error obtained on the task.\nnear the decision boundary since p(1−p)is maximized at\np= 1/2. Compare this to the covariance matrix of the data,\nC0, to which all data points contribute equally. Instead, in\nTASK 2VEC information on the domain is based on data near\nthe decision boundary (task-weighted domain embedding).\nEncoding useful features for the task: The FIM de-\npends on the curvature of the loss function with the diagonal\nentries capturing the sensitivity of the loss to model param-\neters. Speciﬁcally, in the two-layer model one can see that,\nif a given feature is uncorrelated with y, the correspond-\ning blocks of Fare zero. In contrast, a domain embedding\nbased on feature activations of the probe network (e.g., C1)\nonly reﬂects which features vary over the dataset without\nindication of whether they are relevant to the task.\n3. Similarity Measures on the Space of Tasks\nWhat metric should be used on the space of tasks? This\ndepends critically on the meta-task we are considering. As a\nmotivation, we concentrate on the meta-task of selecting the\npre-trained feature extractor from a set in order to obtain the\nbest performance on a new training task. There are several\nnatural metrics that may be considered for this meta-task.\nIn this work, we mainly consider:\nTaxonomic distance For some tasks, there is a natural no-\ntion of semantic similarity, for instance deﬁned by sets of\ncategories organized in a taxonomic hierarchy where each\ntask is classiﬁcation inside a subtree of the hierarchy ( e.g.,\nwe may say that classifying breeds of dogs is closer to clas-siﬁcation of cats than it is to classiﬁcation of species of\nplants). In this setting, ', 'depends on the cross-entropy in a neighborhood\nofˆwof size Λ−1. As in the standard Fisher computation,\nwe estimate one parameter per ﬁlter, rather than per weight,\nwhich in practice means that we constrain Λii= Λjjwhen-\neverwiandwjbelongs to the same ﬁlter. In this case, opti-\nmization ofL( ˆw; Λ)can be done efﬁciently using the local\nreparametrization trick of [18].\n2.2. Properties of the TASK 2VECembedding\nThe task embedding we just deﬁned has a number of\nuseful properties. For illustrative purposes, consider a two-\nlayer sigmoidal network for which an analytic expression\ncan be derived (see Supplementary Materials). The FIM\nof the feature extractor parameters can be written using the\nKronecker product as\nF=Ex,y∼ˆp(x)pw(y|x)[(y−p)2·S⊗xxT]\nwherep=pw(y= 1|x)and the matrix S=wwT⊙zzT⊙\n(1−z)(1−z)Tis an element-wise product of classiﬁer\nweightswand ﬁrst layer feature activations z. It is informa-\ntive to compare this expression to an embedding based only\non the dataset domain statistics, such as the (non-centered)\ncovarianceC0=E[\nxxT]\nof the input data or the covari-\nanceC1=E[\nzzT]\nof the feature activations. One could\ntake such statistics as a representative domain embedding\nsince they only depend on the marginal distribution p(x)in\ncontrast to the FIM task embedding , which depends on the\njoint distribution p(x,y). These simple expressions high-\nlight some important (and more general) properties of the\nFisher embedding we now describe.\nInvariance to the label space: The task embedding does\nnot directly depend on the task labels, but only on the pre-\ndicted distribution pw(y|x)of the trained model. Infor-\nmation about the ground-truth labels yis encoded in the\nweightswwhich are a sufﬁcient statistic of the task [5]. In\nparticular, the task embedding is invariant to permutations\nof the labels y, and has ﬁxed dimension (number of ﬁlters\nof the feature extractor) regardless of the output space (e.g.,\nk-way classiﬁcation with ', 'we can deﬁne\nDtax(ta,tb) = min\ni∈Sa,j∈Sbd(i,j),\nwhereSa,Sbare the sets of categories in task ta,tband\nd(i,j)is an ultrametric or graph distance in the taxonomy\ntree. Notice that this is a proper distance, and in particular\nit is symmetric.\nTransfer distance. We deﬁne the transfer (or ﬁne-tuning)\ngain from a task tato a tasktb(which we improperly call\ndistance, but is not necessarily symmetric or positive) as\nthe difference in expected performance between a model\ntrained for task tbfrom a ﬁxed initialization (random or pre-\ntrained), and the performance of a model ﬁne-tuned for task\ntbstarting from a solution of task ta:\nDft(ta→tb) =E[ℓa→b]−E[ℓb]\nE[ℓb],\nwhere the expectations are taken over all trainings with the\nselected architecture, training procedure and network ini-\ntialization,ℓbis the ﬁnal test error obtained by training on\ntaskbfrom the chosen initialization, and ℓa→bis the error\nobtained instead when starting from a solution to task aand\nthen ﬁne-tuning (with the selected procedure) on task tb.\n3.1. Symmetric and asymmetric TASK 2VECmetrics\nBy construction, the Fisher embedding on which\nTASK 2VEC is based captures fundamental information\x0cabout the structure of the task. We may therefore expect\nthat the distance between two embeddings correlate posi-\ntively with natural metrics on the space of tasks. However,\nthere are two problems in using the Euclidean distance be-\ntween embeddings: the parameters of the network have dif-\nferent scales, and the norm of the embedding is affected by\ncomplexity of the task and the number of samples used to\ncompute the embedding.\nSymmetric TASK 2VEC distance To make the distance\ncomputation robust, we propose to use the cosine distance\nbetween normalized embeddings:\ndsym(Fa,Fb) =dcos(Fa\nFa+Fb,Fb\nFa+Fb)\n,\nwheredcosis the cosine distance, FaandFbare the two\ntask embeddings ( i.e., the diagonal of the Fisher Informa-\ntion computed on the same probe network), and ']","The embedding's norm correlates with task difficulty, while its orientation and distance from other embeddings capture domain characteristics and task similarities.","The Task2Vec embedding relates to the difficulty and domain characteristics of a task by capturing information about the structure of the task, including its sensitivity to model parameters and relevance to the task. It is based on the curvature of the loss function, sensitivity of the loss to model parameters, and feature activations near the decision boundary. This embedding is designed to be invariant to permutations of task labels and has a fixed dimension regardless of the output space, making it reflective of the task's domain characteristics and complexity.",1.0,1.0,0.7383545637130737
How does Task2Vec differentiate itself from traditional domain embeddings and other task representation methods?,"['embedding (distance from origin) and test error obtained on the task.\nnear the decision boundary since p(1−p)is maximized at\np= 1/2. Compare this to the covariance matrix of the data,\nC0, to which all data points contribute equally. Instead, in\nTASK 2VEC information on the domain is based on data near\nthe decision boundary (task-weighted domain embedding).\nEncoding useful features for the task: The FIM de-\npends on the curvature of the loss function with the diagonal\nentries capturing the sensitivity of the loss to model param-\neters. Speciﬁcally, in the two-layer model one can see that,\nif a given feature is uncorrelated with y, the correspond-\ning blocks of Fare zero. In contrast, a domain embedding\nbased on feature activations of the probe network (e.g., C1)\nonly reﬂects which features vary over the dataset without\nindication of whether they are relevant to the task.\n3. Similarity Measures on the Space of Tasks\nWhat metric should be used on the space of tasks? This\ndepends critically on the meta-task we are considering. As a\nmotivation, we concentrate on the meta-task of selecting the\npre-trained feature extractor from a set in order to obtain the\nbest performance on a new training task. There are several\nnatural metrics that may be considered for this meta-task.\nIn this work, we mainly consider:\nTaxonomic distance For some tasks, there is a natural no-\ntion of semantic similarity, for instance deﬁned by sets of\ncategories organized in a taxonomic hierarchy where each\ntask is classiﬁcation inside a subtree of the hierarchy ( e.g.,\nwe may say that classifying breeds of dogs is closer to clas-siﬁcation of cats than it is to classiﬁcation of species of\nplants). In this setting, ', 'we can deﬁne\nDtax(ta,tb) = min\ni∈Sa,j∈Sbd(i,j),\nwhereSa,Sbare the sets of categories in task ta,tband\nd(i,j)is an ultrametric or graph distance in the taxonomy\ntree. Notice that this is a proper distance, and in particular\nit is symmetric.\nTransfer distance. We deﬁne the transfer (or ﬁne-tuning)\ngain from a task tato a tasktb(which we improperly call\ndistance, but is not necessarily symmetric or positive) as\nthe difference in expected performance between a model\ntrained for task tbfrom a ﬁxed initialization (random or pre-\ntrained), and the performance of a model ﬁne-tuned for task\ntbstarting from a solution of task ta:\nDft(ta→tb) =E[ℓa→b]−E[ℓb]\nE[ℓb],\nwhere the expectations are taken over all trainings with the\nselected architecture, training procedure and network ini-\ntialization,ℓbis the ﬁnal test error obtained by training on\ntaskbfrom the chosen initialization, and ℓa→bis the error\nobtained instead when starting from a solution to task aand\nthen ﬁne-tuning (with the selected procedure) on task tb.\n3.1. Symmetric and asymmetric TASK 2VECmetrics\nBy construction, the Fisher embedding on which\nTASK 2VEC is based captures fundamental information\x0cabout the structure of the task. We may therefore expect\nthat the distance between two embeddings correlate posi-\ntively with natural metrics on the space of tasks. However,\nthere are two problems in using the Euclidean distance be-\ntween embeddings: the parameters of the network have dif-\nferent scales, and the norm of the embedding is affected by\ncomplexity of the task and the number of samples used to\ncompute the embedding.\nSymmetric TASK 2VEC distance To make the distance\ncomputation robust, we propose to use the cosine distance\nbetween normalized embeddings:\ndsym(Fa,Fb) =dcos(Fa\nFa+Fb,Fb\nFa+Fb)\n,\nwheredcosis the cosine distance, FaandFbare the two\ntask embeddings ( i.e., the diagonal of the Fisher Informa-\ntion computed on the same probe network), and ', 'depends on the cross-entropy in a neighborhood\nofˆwof size Λ−1. As in the standard Fisher computation,\nwe estimate one parameter per ﬁlter, rather than per weight,\nwhich in practice means that we constrain Λii= Λjjwhen-\neverwiandwjbelongs to the same ﬁlter. In this case, opti-\nmization ofL( ˆw; Λ)can be done efﬁciently using the local\nreparametrization trick of [18].\n2.2. Properties of the TASK 2VECembedding\nThe task embedding we just deﬁned has a number of\nuseful properties. For illustrative purposes, consider a two-\nlayer sigmoidal network for which an analytic expression\ncan be derived (see Supplementary Materials). The FIM\nof the feature extractor parameters can be written using the\nKronecker product as\nF=Ex,y∼ˆp(x)pw(y|x)[(y−p)2·S⊗xxT]\nwherep=pw(y= 1|x)and the matrix S=wwT⊙zzT⊙\n(1−z)(1−z)Tis an element-wise product of classiﬁer\nweightswand ﬁrst layer feature activations z. It is informa-\ntive to compare this expression to an embedding based only\non the dataset domain statistics, such as the (non-centered)\ncovarianceC0=E[\nxxT]\nof the input data or the covari-\nanceC1=E[\nzzT]\nof the feature activations. One could\ntake such statistics as a representative domain embedding\nsince they only depend on the marginal distribution p(x)in\ncontrast to the FIM task embedding , which depends on the\njoint distribution p(x,y). These simple expressions high-\nlight some important (and more general) properties of the\nFisher embedding we now describe.\nInvariance to the label space: The task embedding does\nnot directly depend on the task labels, but only on the pre-\ndicted distribution pw(y|x)of the trained model. Infor-\nmation about the ground-truth labels yis encoded in the\nweightswwhich are a sufﬁcient statistic of the task [5]. In\nparticular, the task embedding is invariant to permutations\nof the labels y, and has ﬁxed dimension (number of ﬁlters\nof the feature extractor) regardless of the output space (e.g.,\nk-way classiﬁcation with ']","Unlike domain embeddings that focus on input data distribution, Task2Vec uniquely represents tasks based on the joint input-output distribution's structure, offering a novel perspective on task similarity and complexity.","Task2Vec differentiates itself from traditional domain embeddings and other task representation methods by focusing on capturing fundamental information about the structure of the task through the Fisher embedding. Task2Vec takes into account the sensitivity of the loss function to the model parameters, incorporates transfer distance metrics, and uses symmetric TASK 2VEC distance computations, such as cosine distances between normalized embeddings. Additionally, Task2Vec is invariant to label space permutations, meaning it does not directly depend on task labels but on the predicted distribution of the trained model, making it a robust and versatile task representation method.",1.0,1.0,0.8564189672470093
How does Task2Vec ensure its task embeddings are invariant to the number of classes and label semantics within a dataset?,"['we can deﬁne\nDtax(ta,tb) = min\ni∈Sa,j∈Sbd(i,j),\nwhereSa,Sbare the sets of categories in task ta,tband\nd(i,j)is an ultrametric or graph distance in the taxonomy\ntree. Notice that this is a proper distance, and in particular\nit is symmetric.\nTransfer distance. We deﬁne the transfer (or ﬁne-tuning)\ngain from a task tato a tasktb(which we improperly call\ndistance, but is not necessarily symmetric or positive) as\nthe difference in expected performance between a model\ntrained for task tbfrom a ﬁxed initialization (random or pre-\ntrained), and the performance of a model ﬁne-tuned for task\ntbstarting from a solution of task ta:\nDft(ta→tb) =E[ℓa→b]−E[ℓb]\nE[ℓb],\nwhere the expectations are taken over all trainings with the\nselected architecture, training procedure and network ini-\ntialization,ℓbis the ﬁnal test error obtained by training on\ntaskbfrom the chosen initialization, and ℓa→bis the error\nobtained instead when starting from a solution to task aand\nthen ﬁne-tuning (with the selected procedure) on task tb.\n3.1. Symmetric and asymmetric TASK 2VECmetrics\nBy construction, the Fisher embedding on which\nTASK 2VEC is based captures fundamental information\x0cabout the structure of the task. We may therefore expect\nthat the distance between two embeddings correlate posi-\ntively with natural metrics on the space of tasks. However,\nthere are two problems in using the Euclidean distance be-\ntween embeddings: the parameters of the network have dif-\nferent scales, and the norm of the embedding is affected by\ncomplexity of the task and the number of samples used to\ncompute the embedding.\nSymmetric TASK 2VEC distance To make the distance\ncomputation robust, we propose to use the cosine distance\nbetween normalized embeddings:\ndsym(Fa,Fb) =dcos(Fa\nFa+Fb,Fb\nFa+Fb)\n,\nwheredcosis the cosine distance, FaandFbare the two\ntask embeddings ( i.e., the diagonal of the Fisher Informa-\ntion computed on the same probe network), and ', 'depends on the cross-entropy in a neighborhood\nofˆwof size Λ−1. As in the standard Fisher computation,\nwe estimate one parameter per ﬁlter, rather than per weight,\nwhich in practice means that we constrain Λii= Λjjwhen-\neverwiandwjbelongs to the same ﬁlter. In this case, opti-\nmization ofL( ˆw; Λ)can be done efﬁciently using the local\nreparametrization trick of [18].\n2.2. Properties of the TASK 2VECembedding\nThe task embedding we just deﬁned has a number of\nuseful properties. For illustrative purposes, consider a two-\nlayer sigmoidal network for which an analytic expression\ncan be derived (see Supplementary Materials). The FIM\nof the feature extractor parameters can be written using the\nKronecker product as\nF=Ex,y∼ˆp(x)pw(y|x)[(y−p)2·S⊗xxT]\nwherep=pw(y= 1|x)and the matrix S=wwT⊙zzT⊙\n(1−z)(1−z)Tis an element-wise product of classiﬁer\nweightswand ﬁrst layer feature activations z. It is informa-\ntive to compare this expression to an embedding based only\non the dataset domain statistics, such as the (non-centered)\ncovarianceC0=E[\nxxT]\nof the input data or the covari-\nanceC1=E[\nzzT]\nof the feature activations. One could\ntake such statistics as a representative domain embedding\nsince they only depend on the marginal distribution p(x)in\ncontrast to the FIM task embedding , which depends on the\njoint distribution p(x,y). These simple expressions high-\nlight some important (and more general) properties of the\nFisher embedding we now describe.\nInvariance to the label space: The task embedding does\nnot directly depend on the task labels, but only on the pre-\ndicted distribution pw(y|x)of the trained model. Infor-\nmation about the ground-truth labels yis encoded in the\nweightswwhich are a sufﬁcient statistic of the task [5]. In\nparticular, the task embedding is invariant to permutations\nof the labels y, and has ﬁxed dimension (number of ﬁlters\nof the feature extractor) regardless of the output space (e.g.,\nk-way classiﬁcation with ', 'of the total size of the dataset, in each epoch we always sample 10,000\nimages with replacement, uniformly between classes. In this way, all epochs have the same length and see approximately the\nsame number of examples for each class. We use this balanced sampling in all experiments, unless noted otherwise.\nC.2. Computation of the TASK 2VECembedding\nAs the described in the main text, the TASK 2VEC embedding is obtained by choosing a probe network, retraining the ﬁnal\nclassiﬁer on the given task, and then computing the Fisher Information Matrix for the weights of the probe network.\nUnless speciﬁed otherwise, we use an off-the-shelf ResNet-34 pretrained on ImageNet as the probe network. The Fisher\nInformation Matrix is computed in a robust way minimizing the loss function L( ˆw; Λ)with respect to the precision matrix Λ,\nas described before. To make computation of the embedding faster, instead of waiting for the convergence of the classiﬁer,\nwe train the ﬁnal classiﬁer for 2 epochs using Adam and then we continue to train it jointly with the precision matrix Λusing\nthe lossL( ˆw; Λ). We constrain Λto be positive by parametrizing it as Λ = exp(L), for some unconstrained variable L.\nWhile for the classiﬁer we use a low learning rate (1e-4), we found it useful to use an higher learning rate (1e-2) to train L.\nC.3. Training the MODEL 2VECembedding\nAs described in the main text, in the MODEL 2VECembedding we aim to learn a vector representation mj=Fj+bjof\nthej-th model in the collection, which represents both the task the model was trained on (through the TASK 2VEC embedding\nFj), and the particularities of ']","By basing the embeddings on the structure of the task via the FIM, Task2Vec creates representations that are independent of class count or label semantics, focusing on the task's intrinsic characteristics.","Task2Vec ensures its task embeddings are invariant to the number of classes and label semantics within a dataset by creating embeddings based on the predicted distribution of the trained model instead of directly depending on the task labels. This means that the information about the ground-truth labels is encoded in the weights of the model, making the task embeddings invariant to permutations of the labels. Additionally, the task embeddings have a fixed dimension (number of filters of the feature extractor) regardless of the output space (e.g., k-way classification with varying number of classes).",1.0,1.0,0.8155853152275085
How does Task2Vec handle the variance in data size and complexity across different tasks in its embeddings?,"['depends on the cross-entropy in a neighborhood\nofˆwof size Λ−1. As in the standard Fisher computation,\nwe estimate one parameter per ﬁlter, rather than per weight,\nwhich in practice means that we constrain Λii= Λjjwhen-\neverwiandwjbelongs to the same ﬁlter. In this case, opti-\nmization ofL( ˆw; Λ)can be done efﬁciently using the local\nreparametrization trick of [18].\n2.2. Properties of the TASK 2VECembedding\nThe task embedding we just deﬁned has a number of\nuseful properties. For illustrative purposes, consider a two-\nlayer sigmoidal network for which an analytic expression\ncan be derived (see Supplementary Materials). The FIM\nof the feature extractor parameters can be written using the\nKronecker product as\nF=Ex,y∼ˆp(x)pw(y|x)[(y−p)2·S⊗xxT]\nwherep=pw(y= 1|x)and the matrix S=wwT⊙zzT⊙\n(1−z)(1−z)Tis an element-wise product of classiﬁer\nweightswand ﬁrst layer feature activations z. It is informa-\ntive to compare this expression to an embedding based only\non the dataset domain statistics, such as the (non-centered)\ncovarianceC0=E[\nxxT]\nof the input data or the covari-\nanceC1=E[\nzzT]\nof the feature activations. One could\ntake such statistics as a representative domain embedding\nsince they only depend on the marginal distribution p(x)in\ncontrast to the FIM task embedding , which depends on the\njoint distribution p(x,y). These simple expressions high-\nlight some important (and more general) properties of the\nFisher embedding we now describe.\nInvariance to the label space: The task embedding does\nnot directly depend on the task labels, but only on the pre-\ndicted distribution pw(y|x)of the trained model. Infor-\nmation about the ground-truth labels yis encoded in the\nweightswwhich are a sufﬁcient statistic of the task [5]. In\nparticular, the task embedding is invariant to permutations\nof the labels y, and has ﬁxed dimension (number of ﬁlters\nof the feature extractor) regardless of the output space (e.g.,\nk-way classiﬁcation with ', 'we can deﬁne\nDtax(ta,tb) = min\ni∈Sa,j∈Sbd(i,j),\nwhereSa,Sbare the sets of categories in task ta,tband\nd(i,j)is an ultrametric or graph distance in the taxonomy\ntree. Notice that this is a proper distance, and in particular\nit is symmetric.\nTransfer distance. We deﬁne the transfer (or ﬁne-tuning)\ngain from a task tato a tasktb(which we improperly call\ndistance, but is not necessarily symmetric or positive) as\nthe difference in expected performance between a model\ntrained for task tbfrom a ﬁxed initialization (random or pre-\ntrained), and the performance of a model ﬁne-tuned for task\ntbstarting from a solution of task ta:\nDft(ta→tb) =E[ℓa→b]−E[ℓb]\nE[ℓb],\nwhere the expectations are taken over all trainings with the\nselected architecture, training procedure and network ini-\ntialization,ℓbis the ﬁnal test error obtained by training on\ntaskbfrom the chosen initialization, and ℓa→bis the error\nobtained instead when starting from a solution to task aand\nthen ﬁne-tuning (with the selected procedure) on task tb.\n3.1. Symmetric and asymmetric TASK 2VECmetrics\nBy construction, the Fisher embedding on which\nTASK 2VEC is based captures fundamental information\x0cabout the structure of the task. We may therefore expect\nthat the distance between two embeddings correlate posi-\ntively with natural metrics on the space of tasks. However,\nthere are two problems in using the Euclidean distance be-\ntween embeddings: the parameters of the network have dif-\nferent scales, and the norm of the embedding is affected by\ncomplexity of the task and the number of samples used to\ncompute the embedding.\nSymmetric TASK 2VEC distance To make the distance\ncomputation robust, we propose to use the cosine distance\nbetween normalized embeddings:\ndsym(Fa,Fb) =dcos(Fa\nFa+Fb,Fb\nFa+Fb)\n,\nwheredcosis the cosine distance, FaandFbare the two\ntask embeddings ( i.e., the diagonal of the Fisher Informa-\ntion computed on the same probe network), and ', 'varying k).\nEncoding task difﬁculty: As we can see from the ex-\npressions above, if the ﬁt model is very conﬁdent in its pre-\ndictions, E[(y−p)2]goes to zero. Hence, the norm of the\ntask embedding∥F∥⋆scales with the difﬁculty of the task\nfor a given feature extractor φ. Figure 2 (Right) shows that\neven for more complex models trained on real data, the FIM\nnorm correlates with test performance.\nEncoding task domain: Data points xthat are classi-\nﬁed with high conﬁdence, i.e., pis close to 0 or 1, will\nhave a lower contribution to the task embedding than points\x0c0 25 50 75 100 125\nSize k of neighborhood1.01.52.02.53.0Avg. top-k tax. distanceTask2Vec distance\nTax. distance\n0.4 0.6 0.8\nL1 norm of task embedding1e80%10%20%30%40%50%60%Test error on task (%)\nFigure 2: Distance between species classiﬁcation tasks. (Left) Task similarity matrix ordered by hierarchical clustering.\nNote that the dendrogram produced by the task similarity matches the taxonomic clusters (indicated by color bar). (Center)\nFor tasks extracted from iNaturalist and CUB, we compare the cosine distance between tasks to their taxonomical distance.\nAs the size of the task embedding neighborhood increases (measured by number of tasks in the neighborhood), we plot the\naverage taxonomical distance of tasks from the neighborhood center. While the task distance does not perfectly match the\ntaxonomical distance (whose curve is shown in orange), it shows a good correlation. Difference are both due to the fact that\ntaxonomically close species may need very different features to be classiﬁed, creating a mismatch between the two notions\nof distance, and because for some tasks in iNaturalist too few samples are provided to compute a good embedding. (Right)\nCorrelation between L1norm of the task ']","Through robust Fisher computation, Task2Vec accommodates variance in data size and complexity, ensuring embeddings remain informative by adjusting for irregularities in the loss landscape.","Task2Vec handles the variance in data size and complexity across different tasks in its embeddings by encoding task difficulty and task domain into the task embedding. The norm of the task embedding scales with the difficulty of the task, where if the model is confident in its predictions, the norm of the task embedding decreases. Additionally, data points that are classified with high confidence have a lower contribution to the task embedding. This allows Task2Vec to capture the differences in data size and complexity while encoding the tasks into embeddings.",1.0,1.0,0.787075936794281
"How does GLM-130B's architecture differ from traditional GPT-style models, and what are its key features?","['from all of our generous partners, we\nfinally succeeded in making our pre-training algorithms runnable across all the platforms—frankly,\na surprising achievement for this project. The timeline of GLM-130B in Figure 21 covers most of\nthe issues we have encountered and addressed as of this writing.\nOn April 26th, we received a generous computing sponsorship from Zhipu.AI — an AI startup that\naims to teach machines to think like humans. After another week of testing, we finally kicked off\nthe training of the GLM-130B model on its 96 A100 (40G * 8) servers on May 6th. Additionally,\nZhipu.AI also sent a team to help evaluate the pre-trained model and build a demonstration website.\nThe training period spanned two months, during which we began developing a toolkit to allow GLM-\n130B’s inference in low-resource setting with swapping technique and quantization. Though it is\nalready the most accessible model of its scale, together with our partner from Tsinghua NLP, we have\nbeen exploring the limit of popularized hardware platforms, which would truly make the 100B-scale\nmodel accessible to as many people as possible. To date, we managed to reach the INT4 weight\nquantization for GLM-130B. Importantly, the INT4 version of GLM-130B without post training\n16This section is largely extracted and updated from the blog introduction of GLM-130B at http://keg.\ncs.tsinghua.edu.cn/glm-130b/ (Posted date: August 4, 2022).\n53\x0cPublished as a conference paper at ICLR 2023\n2021.12•The “千亿”(100B) project towardsan opendensepre-trained GLM at 100B scale isconceived•Survey pre-trainingstrategies of existing models of similar scale,suchasGPT-3,Gopher=> Limited public infoabout how they were trainedand issues they met•SearchforpossibleGPUclusters&sponsors2022.1•TesttheperformanceofFP16/FP32at100Bscaleononetestingcluster•UnexpectedexcessivememoryusageinGLM=> Torchisbetterwithfixedlengthinputsequences•InabilitytoconvergeandtrytricksfromCogViewandViT=> UseSandwich-LN•Frequentrandomhardwarefailures=> HavetorunHCPGtestbeforeeachrun2022.2•Veryslowtrainingspeedthanpreviouslycalculated=>Optimizekernelsandfuseoperators=>Findtheinputshapeiscriticaltokernelperformance•Collectpre-trainingcorporaandtokenize=>Useicetk:thesentencepieceissettotheunigrammode•Debugthe3Dpipelineparallelinthenewly-releasedMegatronandDeepSpeed2022.3•Itcan’trecoverperfectlyfromcheckpoints=>Ourcustomizeddataloaderdonotsaveitsstateseedproperlyindistributedtraining•Thememoryperprocessoristoosmall=>Requiretoomanypipelinestages=>Batchsizeistoolarge(upto12,000)=>Harmthemodel’sconvergency•Itcan’tlaunchmorethan2,000computingnodes=>Overcomethisandsupport6,000-nodetrainingbytuningLinuxkernelTCPparameters•Collectdataformulti-taskinstructionpre-training•Receiveopportunitiestotesttrainingsonseveralotherclusters•Veryslowtrainingspeedthanexpected=>Theunderlyingelement-wiseoperatorsdon’tsupportfastcomputationonlarge-dimensionvectors.2022.4•OptimizeA100kernel’scomputingefficiency=>A100kernelsprefersquare-shapedinputs,andseq_len=2,048isoptimalforourhidden-statedimension(12,288)•Inabilitytoconvergeduetolargegradientnorms(170+)ofinputembeddings=>Tryembeddingnormandgradientshrink,whichturnouttobealmostequivalent•Naïvepost-LNorpre-LNdisconvergesafterseveralthousandsofsteps=>TrySandwich-LNwithPB-Relax•Itstilldisconvergesafteroneweek’strial=>Thedataloaderstateseedsarenotunifiedfordifferentpipelinestages,resultinginamismatchofinputdataandlabels.•Testtwopositionalencodings:RoPEandAlibi=>Alibicanbeslowerasitrequireselement-wisemanipulationonattentionmatrices---changingnum_heads*2,048* 2,048scalarsperlayer•TestGeGLUandGAU=>GAUconvergesfasterwithrelativelypoorperformanceonfine-tunedSuperGLUE•AbnormalGPUmemoryusageofnewly-addedfunctionsandclasses=>DeepSpeedhardcodesthefunctionnamesforcheckpointactivation•DecidetotrainGLMwith130billionparameters=>allowinferenceonaDGX-A10040Gnode2022.5-6•ImplementaRoPEcudaoperatorinC++=>Seeunexpectedprecisionerrorsandfinallyhaveitabandoned•Sandwich-LNstilldisconverges=>1)Reducinglearningratedoesnothelp;2)UsingHingecross-entropybecomesslowerandharmsperformance;3)ShiftingtoDeepNormstilldisconverges•UseFP32insoftmaxofattention=>Success•FindPB-RelaxunnecessaryforFP32softmax=>Italsoslowsdowntrainingasitneedstomanipulatethewholeattentionscorematrices•Experiencefewspikesinlatertraining=>1)Reducegradientshrinkfactorfrom1to0.1:useful;2)Reducethelearningrate:sometimesuseful;3)Jumpthenoisydatabatches:sometimesuseful•Findamistakeinmulti-taskdataaftertrainingfor20,000steps=>Usethecorrectdatabutitdoesnotforget2022.6-7•AdaptthepipelineparallelcheckpointstoordinaryparallelcheckpointsforefficientinferenceonasingleA100•Workonevaluationscriptsondatasets:MMLU,Big-bench,CLUE,SuperCLUE,etc.•ImplementP-TuningandP-Tuningv2forparameter-efficienttuningonGLM-130BfortuningonSuperGLUE•WorkwithBMInfonadaptingGLM-130BtoperforminferenceonasingleV100or3090=>Usepipeline-styleasynchronousswappingbetweenmainmemoryandGPUmemory•Trytofine-tuneGLM-130BwithfewerA100nodes(i.e.,12-16nodes)=>Pipeline-stylefailsduetotoomanypipelinestages=>Findthatdataparallelcannotbeintroducedforfine-tuning=>Use32-waymodelparallelforfine-tuningwithreasonableperformance\nMajorIssuesEncounteredforTrainingGLM-130B\nhttps://github.com/THUDM/GLM-130BTsinghuaKEG\nFigure 21: The timeline of major issues that training GLM-130B encountered and ', 'GLM-130B training\nConfiguration Key Value\nadam_beta1 0.9\nadam_beta2 0.95\nadam_eps 1e-08\naggregated_samples_per_sequence 4\nattention_dropout 0.1\nattention_softmax_in_fp32 True\naverage_block_length 3\nbias_dropout_fusion True\ncheckpoint_activations True\ncheckpoint_in_cpu False\ncheckpoint_num_layers 1\nclip_grad 1.0\ncontigious_checkpointing False\ncpu_optimizer False\ndata_parallel_size 24\ndeepnorm True\ndistributed_backend nccl\neval_interval 1000\neval_iters 3\nffn_hidden_size 32768\nfp16 True\nglobal_batch_size 4224\nglu_activation geglu\ngpt_prob 0.7\nhidden_dropout 0.1\nhidden_size 12288\nhysteresis 2\ninit_method_std 0.0052\ninit_method_xavier_uniform False\ninitial_loss_scale 65536\nlayernorm_epsilon 1E-05\nlearnable_rotary_embedding False\nlength_per_sample 2000\nlog_interval 1\nloss_scale 0\nloss_scale_window 2000\nlr 8e-05\nlr_decay_iters None\nlr_decay_samples 197753905\nlr_decay_style cosine\nlr_warmup_samples 1098632\nmake_vocab_size_divisible_by 768\nmask_prob 0.15\nmasked_softmax_fusion True\nmicro_batch_size 1\nmin_gmask_ratio 0.2\nmin_loss_scale 1.0\nmin_lr 8e-06\nmultitask_ratio 0.05\nnum_attention_heads 96\nnum_layers 70\nonnx_safe None\noptimizer adam\npartition_activations True\npipeline_model_parallel_size 8\nposition_embedding_type rotary\nrampup_batch_size 192, 24, 5493164\nsave_interval 250\nseed 1234\nseq_length 2048\nshort_seq_prob 0.02\nshrink_embedding_gradient_alpha 0.1\nsingle_span_prob 0.02\nsplit 949,50,1\ntensor_model_parallel_size 4\ntokenizer_type IceTokenizer\nweight_decay 0.1\nzero_contigious_gradients False\nzero_reduce_bucket_size 500000000\nzero_reduce_scatter False\nzero_stage 1\nzero-optimization.allgather_bucket_size 500000000\ntokenizer_type IceTokenizer\nweight_decay 0.1\nworld_size 768\nzero_contigious_gradients FALSE\nzero_reduce_bucket_size 500000000\nzero_reduce_scatter FALSE\nzero_stage 1\nzero-optimization.allgather_bucket_size 500000000\n48\x0cPublished as a conference paper at ICLR 2023\nTable 12: The 74 datasets involved in Multi-task Instruction Pre-training (MIP). Datasets from T0-\nPromptSource (Sanh et al., 2022; Bach et al., 2022) are named in their Hugging Face datasets iden-\ntifiers. Datasets from DeepStruct (Wang et al., 2022a) are described in Appendix C.2.\nTask Dataset Task Dataset\nCoreference Resolution super_glue/wsc.fixed Multi-choice QA cos_e/v1.11\nCoreference Resolution winogrande/winogrande_xl Multi-choice QA cosmos_qa\nNatural Language Inference super_glue/cb Multi-choice QA dream\nNatural Language Inference super_glue/rte Multi-choice QA openbookqa/main\nNatural Language Inference anli Multi-choice QA qasc\nParaphrase Identification glue/mrpc Multi-choice QA quail\nParaphrase Identification glue/qqp Multi-choice QA quarel\nParaphrase Identification paws/labeled_final Multi-choice QA quartz\nClosed-Book QA ai2_arc/ARC_Challenge Multi-choice QA race/high\nClosed-Book QA ai2_arc/ARC_Easy Multi-choice QA race/middle\nClosed-Book QA kilt_tasks/hoptpotqa Multi-choice QA sciq\nClosed-Book QA trivia_qa/unfiltered Multi-choice QA social_i_qa\nClosed-Book QA web_questions Multi-choice QA super_glue/boolq\nClosed-Book QA wiki_qa Multi-choice QA super_glue/multirc\nExtractive QA adversarial_qa/dbidaf Multi-choice QA wiki_hop/original\nExtractive QA adversarial_qa/dbert Multi-choice QA wiqa\nExtractive QA adversarial_qa/droberta Multi-choice QA piqa\nExtractive QA duorc/SelfRC Topic Classification ag_news\nExtractive QA duorc/ParaphraseRC Topic Classification dbpedia_14\nExtractive QA ropes Topic Classification trec\nExtractive QA squad_v2 Word Sense Disambiguation super_glue/wic\nExtractive QA super_glue/record Dialogue State Tracking multiwoz_2.1\nExtractive QA quoref Event Extraction ', 'is\nalso suggested in concurrent literature (Dettmers et al., 2022).\nWhat is special in GLM-130B is that 30% of its dimensions may present value outliers (Cf. Fig-\nure 12), while other GPT-based LLMs (e.g., OPT-175B and BLOOM 176B) only has very few\noutlying dimensions (Dettmers et al., 2022). Therefore, the solution to decompose matrix multipli-\n6https://github.com/NVIDIA/FasterTransformer\n7https://huggingface.co/docs/transformers/model_doc/bloom\n27\x0cPublished as a conference paper at ICLR 2023\ncation for higher-precision computation in outlying dimensions proposed in (Dettmers et al., 2022)\nis not applicable to GLM-130B.\nFigure 13: GLM-130B’s\nactivation outliers’ absolute\nvalue scale.We study whether these outliers can be ignored in LLM quantiza-\ntion, and the answer is interestingly “no”. These values can be sev-\neral orders of magnitude larger than ordinary activation values (Cf.\nFigure 13). While most values (accounts for 99.98% dimensions in\na hidden state) stay less them 6, those two outlying dimensions can\nreach 50 or even over 100. They are speculated to be some important\nclues for GLM-130B and potentially other LLMs to memorize some\nfixed world or language knowledge, and thus removing or omitting\nthem in quantization can lead to significant performance degradation.\nB.7 W EIGHT QUANTIZATION\nB.7.1 P RELIMINARIES\nAbsmax Quantization is a symmetric quantization that a range of [−absmax( x),absmax( x)]is\nmapped to [−(2b−1),2b−1]forx.\nsx=absmax( x)\n2b−1−1(7)\nxq= round( x/sx) (8)\nwhere sxis the scaling factor, xqis the quantization result and bis the bit width.\nZeropoint Quantization is an asymmetric quantization that a range of [min( x),max( x)]is\nmapped to [−(2b−1),2b−1].\nsx=max( x)−min(x)\n2b−2(9)\nzx= round(min( x)/sx) + 2b−1−1 (10)\nxq= round( x/sx)−zx (11)\nwhere zxis the zero point.\nCol/Row-wise Quantization Using a single scaling factor for the weight matrix often leads to\nmore quantization errors because one single outlier leads to a decrease in the quantization ']","Unlike GPT-style architectures, GLM-130B is based on the General Language Model (GLM) framework with bidirectional attention and autoregressive blank infilling, enabling more effective context comprehension.","GLM-130B's architecture differs from traditional GPT-style models in that it has 30% of its dimensions presenting value outliers, unlike other GPT-based LLMs which have very few outlying dimensions. These outliers are speculated to be crucial for memorizing fixed world or language knowledge. Ignoring these outliers in quantization can lead to significant performance degradation. Additionally, GLM-130B has unique activation outliers that are several orders of magnitude larger than ordinary activation values. Its key features include the importance of these outliers for model performance and the inability to apply decomposition techniques proposed for other models with fewer outlying dimensions.",1.0,1.0,0.6637833118438721
How does GLM-130B's performance compare to other 100B-scale models and PaLM 540B across English benchmarks?,"['from all of our generous partners, we\nfinally succeeded in making our pre-training algorithms runnable across all the platforms—frankly,\na surprising achievement for this project. The timeline of GLM-130B in Figure 21 covers most of\nthe issues we have encountered and addressed as of this writing.\nOn April 26th, we received a generous computing sponsorship from Zhipu.AI — an AI startup that\naims to teach machines to think like humans. After another week of testing, we finally kicked off\nthe training of the GLM-130B model on its 96 A100 (40G * 8) servers on May 6th. Additionally,\nZhipu.AI also sent a team to help evaluate the pre-trained model and build a demonstration website.\nThe training period spanned two months, during which we began developing a toolkit to allow GLM-\n130B’s inference in low-resource setting with swapping technique and quantization. Though it is\nalready the most accessible model of its scale, together with our partner from Tsinghua NLP, we have\nbeen exploring the limit of popularized hardware platforms, which would truly make the 100B-scale\nmodel accessible to as many people as possible. To date, we managed to reach the INT4 weight\nquantization for GLM-130B. Importantly, the INT4 version of GLM-130B without post training\n16This section is largely extracted and updated from the blog introduction of GLM-130B at http://keg.\ncs.tsinghua.edu.cn/glm-130b/ (Posted date: August 4, 2022).\n53\x0cPublished as a conference paper at ICLR 2023\n2021.12•The “千亿”(100B) project towardsan opendensepre-trained GLM at 100B scale isconceived•Survey pre-trainingstrategies of existing models of similar scale,suchasGPT-3,Gopher=> Limited public infoabout how they were trainedand issues they met•SearchforpossibleGPUclusters&sponsors2022.1•TesttheperformanceofFP16/FP32at100Bscaleononetestingcluster•UnexpectedexcessivememoryusageinGLM=> Torchisbetterwithfixedlengthinputsequences•InabilitytoconvergeandtrytricksfromCogViewandViT=> UseSandwich-LN•Frequentrandomhardwarefailures=> HavetorunHCPGtestbeforeeachrun2022.2•Veryslowtrainingspeedthanpreviouslycalculated=>Optimizekernelsandfuseoperators=>Findtheinputshapeiscriticaltokernelperformance•Collectpre-trainingcorporaandtokenize=>Useicetk:thesentencepieceissettotheunigrammode•Debugthe3Dpipelineparallelinthenewly-releasedMegatronandDeepSpeed2022.3•Itcan’trecoverperfectlyfromcheckpoints=>Ourcustomizeddataloaderdonotsaveitsstateseedproperlyindistributedtraining•Thememoryperprocessoristoosmall=>Requiretoomanypipelinestages=>Batchsizeistoolarge(upto12,000)=>Harmthemodel’sconvergency•Itcan’tlaunchmorethan2,000computingnodes=>Overcomethisandsupport6,000-nodetrainingbytuningLinuxkernelTCPparameters•Collectdataformulti-taskinstructionpre-training•Receiveopportunitiestotesttrainingsonseveralotherclusters•Veryslowtrainingspeedthanexpected=>Theunderlyingelement-wiseoperatorsdon’tsupportfastcomputationonlarge-dimensionvectors.2022.4•OptimizeA100kernel’scomputingefficiency=>A100kernelsprefersquare-shapedinputs,andseq_len=2,048isoptimalforourhidden-statedimension(12,288)•Inabilitytoconvergeduetolargegradientnorms(170+)ofinputembeddings=>Tryembeddingnormandgradientshrink,whichturnouttobealmostequivalent•Naïvepost-LNorpre-LNdisconvergesafterseveralthousandsofsteps=>TrySandwich-LNwithPB-Relax•Itstilldisconvergesafteroneweek’strial=>Thedataloaderstateseedsarenotunifiedfordifferentpipelinestages,resultinginamismatchofinputdataandlabels.•Testtwopositionalencodings:RoPEandAlibi=>Alibicanbeslowerasitrequireselement-wisemanipulationonattentionmatrices---changingnum_heads*2,048* 2,048scalarsperlayer•TestGeGLUandGAU=>GAUconvergesfasterwithrelativelypoorperformanceonfine-tunedSuperGLUE•AbnormalGPUmemoryusageofnewly-addedfunctionsandclasses=>DeepSpeedhardcodesthefunctionnamesforcheckpointactivation•DecidetotrainGLMwith130billionparameters=>allowinferenceonaDGX-A10040Gnode2022.5-6•ImplementaRoPEcudaoperatorinC++=>Seeunexpectedprecisionerrorsandfinallyhaveitabandoned•Sandwich-LNstilldisconverges=>1)Reducinglearningratedoesnothelp;2)UsingHingecross-entropybecomesslowerandharmsperformance;3)ShiftingtoDeepNormstilldisconverges•UseFP32insoftmaxofattention=>Success•FindPB-RelaxunnecessaryforFP32softmax=>Italsoslowsdowntrainingasitneedstomanipulatethewholeattentionscorematrices•Experiencefewspikesinlatertraining=>1)Reducegradientshrinkfactorfrom1to0.1:useful;2)Reducethelearningrate:sometimesuseful;3)Jumpthenoisydatabatches:sometimesuseful•Findamistakeinmulti-taskdataaftertrainingfor20,000steps=>Usethecorrectdatabutitdoesnotforget2022.6-7•AdaptthepipelineparallelcheckpointstoordinaryparallelcheckpointsforefficientinferenceonasingleA100•Workonevaluationscriptsondatasets:MMLU,Big-bench,CLUE,SuperCLUE,etc.•ImplementP-TuningandP-Tuningv2forparameter-efficienttuningonGLM-130BfortuningonSuperGLUE•WorkwithBMInfonadaptingGLM-130BtoperforminferenceonasingleV100or3090=>Usepipeline-styleasynchronousswappingbetweenmainmemoryandGPUmemory•Trytofine-tuneGLM-130BwithfewerA100nodes(i.e.,12-16nodes)=>Pipeline-stylefailsduetotoomanypipelinestages=>Findthatdataparallelcannotbeintroducedforfine-tuning=>Use32-waymodelparallelforfine-tuningwithreasonableperformance\nMajorIssuesEncounteredforTrainingGLM-130B\nhttps://github.com/THUDM/GLM-130BTsinghuaKEG\nFigure 21: The timeline of major issues that training GLM-130B encountered and ', 'GLM-130B training\nConfiguration Key Value\nadam_beta1 0.9\nadam_beta2 0.95\nadam_eps 1e-08\naggregated_samples_per_sequence 4\nattention_dropout 0.1\nattention_softmax_in_fp32 True\naverage_block_length 3\nbias_dropout_fusion True\ncheckpoint_activations True\ncheckpoint_in_cpu False\ncheckpoint_num_layers 1\nclip_grad 1.0\ncontigious_checkpointing False\ncpu_optimizer False\ndata_parallel_size 24\ndeepnorm True\ndistributed_backend nccl\neval_interval 1000\neval_iters 3\nffn_hidden_size 32768\nfp16 True\nglobal_batch_size 4224\nglu_activation geglu\ngpt_prob 0.7\nhidden_dropout 0.1\nhidden_size 12288\nhysteresis 2\ninit_method_std 0.0052\ninit_method_xavier_uniform False\ninitial_loss_scale 65536\nlayernorm_epsilon 1E-05\nlearnable_rotary_embedding False\nlength_per_sample 2000\nlog_interval 1\nloss_scale 0\nloss_scale_window 2000\nlr 8e-05\nlr_decay_iters None\nlr_decay_samples 197753905\nlr_decay_style cosine\nlr_warmup_samples 1098632\nmake_vocab_size_divisible_by 768\nmask_prob 0.15\nmasked_softmax_fusion True\nmicro_batch_size 1\nmin_gmask_ratio 0.2\nmin_loss_scale 1.0\nmin_lr 8e-06\nmultitask_ratio 0.05\nnum_attention_heads 96\nnum_layers 70\nonnx_safe None\noptimizer adam\npartition_activations True\npipeline_model_parallel_size 8\nposition_embedding_type rotary\nrampup_batch_size 192, 24, 5493164\nsave_interval 250\nseed 1234\nseq_length 2048\nshort_seq_prob 0.02\nshrink_embedding_gradient_alpha 0.1\nsingle_span_prob 0.02\nsplit 949,50,1\ntensor_model_parallel_size 4\ntokenizer_type IceTokenizer\nweight_decay 0.1\nzero_contigious_gradients False\nzero_reduce_bucket_size 500000000\nzero_reduce_scatter False\nzero_stage 1\nzero-optimization.allgather_bucket_size 500000000\ntokenizer_type IceTokenizer\nweight_decay 0.1\nworld_size 768\nzero_contigious_gradients FALSE\nzero_reduce_bucket_size 500000000\nzero_reduce_scatter FALSE\nzero_stage 1\nzero-optimization.allgather_bucket_size 500000000\n48\x0cPublished as a conference paper at ICLR 2023\nTable 12: The 74 datasets involved in Multi-task Instruction Pre-training (MIP). Datasets from T0-\nPromptSource (Sanh et al., 2022; Bach et al., 2022) are named in their Hugging Face datasets iden-\ntifiers. Datasets from DeepStruct (Wang et al., 2022a) are described in Appendix C.2.\nTask Dataset Task Dataset\nCoreference Resolution super_glue/wsc.fixed Multi-choice QA cos_e/v1.11\nCoreference Resolution winogrande/winogrande_xl Multi-choice QA cosmos_qa\nNatural Language Inference super_glue/cb Multi-choice QA dream\nNatural Language Inference super_glue/rte Multi-choice QA openbookqa/main\nNatural Language Inference anli Multi-choice QA qasc\nParaphrase Identification glue/mrpc Multi-choice QA quail\nParaphrase Identification glue/qqp Multi-choice QA quarel\nParaphrase Identification paws/labeled_final Multi-choice QA quartz\nClosed-Book QA ai2_arc/ARC_Challenge Multi-choice QA race/high\nClosed-Book QA ai2_arc/ARC_Easy Multi-choice QA race/middle\nClosed-Book QA kilt_tasks/hoptpotqa Multi-choice QA sciq\nClosed-Book QA trivia_qa/unfiltered Multi-choice QA social_i_qa\nClosed-Book QA web_questions Multi-choice QA super_glue/boolq\nClosed-Book QA wiki_qa Multi-choice QA super_glue/multirc\nExtractive QA adversarial_qa/dbidaf Multi-choice QA wiki_hop/original\nExtractive QA adversarial_qa/dbert Multi-choice QA wiqa\nExtractive QA adversarial_qa/droberta Multi-choice QA piqa\nExtractive QA duorc/SelfRC Topic Classification ag_news\nExtractive QA duorc/ParaphraseRC Topic Classification dbpedia_14\nExtractive QA ropes Topic Classification trec\nExtractive QA squad_v2 Word Sense Disambiguation super_glue/wic\nExtractive QA super_glue/record Dialogue State Tracking multiwoz_2.1\nExtractive QA quoref Event Extraction ', 'et al., 2022) to leverage its bidirectional at-\ntention advantage and autoregressive blank infilling objective. Table 1 summarizes the comparison\nbetween GLM-130B, GPT-3 and another two open-source efforts—OPT-175B and BLOOM-176B,\nas well as PaLM 540B (Chowdhery et al., 2022)—a 4 ×larger model—as a reference.\nAltogether, the conceptual uniqueness and engineering efforts enable GLM-130B to exhibit perfor-\nmance that surpasses the level of GPT-3 on a wide range of benchmarks (in total 112 tasks) and also\noutperforms PaLM 540B in many cases, while outperformance over GPT-3 has not been observed in\nOPT-175B and BLOOM-176B (Cf. Figure 1 left). For zero-shot performance, GLM-130B is better\nthan GPT-3 175B (+5.0%), OPT-175B (+6.5%), and BLOOM-176B (+13.0%) on LAMBADA (Pa-\nperno et al., 2016), and achieves 3 ×better performance than GPT-3 on Big-bench-lite (Srivastava\net al., 2022). For the 5-shot MMLU (Hendrycks et al., 2021) tasks, it is better than GPT-3 175B\n(+0.9%) and BLOOM-176B (+12.7%). As a bilingual LLM also in Chinese, it offers significantly\nbetter results than ERNIE TITAN 3.0 260B (Wang et al., 2021)—the largest Chinese LLM—on 7\nzero-shot CLUE (Xu et al., 2020) datasets (+24.26%) and 5 zero-shot FewCLUE (Xu et al., 2021)\nones (+12.75%). Importantly, as summarized in Figure 1 right, GLM-130B as an open model is\nassociated with significantly less bias and generation toxicity than its 100B-scale counterparts .\nFinally, we design GLM-130B to empower as many people as possible to conduct 100B-scale LLM\nstudies. First, instead of using 175B+ parameters as OPT and BLOOM, the 130B size is decided be-\ncause such a size supports inference on a single A100 (8 ×40G) server. Second, to further lower the\nGPU requirements, we quantize GLM-130B ']","GLM-130B surpasses GPT-3 and other 100B-scale models in a range of English benchmarks and performs competitively against PaLM 540B, demonstrating its effectiveness in language understanding.","GLM-130B's performance surpasses GPT-3 on a wide range of benchmarks and also outperforms PaLM 540B in many cases. It exhibits better zero-shot performance compared to GPT-3, OPT-175B, and BLOOM-176B on certain tasks. It also offers better results as a bilingual LLM in Chinese compared to ERNIE TITAN 3.0 260B on specific datasets. Additionally, GLM-130B shows significantly less bias and generation toxicity compared to other 100B-scale models.",1.0,1.0,0.7332310080528259
What record-setting performance did Megatron-LM achieve in terms of parameter count and sustained PetaFLOPs on NVIDIA V100 GPUs?,"['and conﬁgurations listed in Table 2. The 355M\nmodel is equivalent in size and conﬁguration of BERT-Large\nmodel (Devlin et al., 2018). The 2.5B model is bigger than\nthe previous largest GPT-2 model, and the 8.3B model is\nlarger than any left-to-right transformer language model\never trained, to the best of our knowledge. To train and eval-\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 2. Model conﬁgurations used for GPT-2.\nHidden Time\nParameter Layers Hidden Attn Size Total per\nCount Size Heads per GPUs Epoch\nHead (days)\n355M 24 1024 16 64 64 0.86\n2.5B 54 1920 20 96 128 2.27\n8.3B 72 3072 24 128 512 2.10\nTable 3. Zero-shot results. SOTA are from (Khandelwal et al.,\n2019) for Wikitext103 and (Radford et al., 2019) for LAMBADA.\nModel Wikitext103 LAMBADA\nPerplexity↓ Accuracy↑\n355M 19.31 45.18%\n2.5B 12.76 61.73%\n8.3B 10.81 66.51%\nPrevious SOTA 15.79 63.24%\nuate our language models we use the procedure described in\nsection 4. Table 2 also lists the time it takes to advance one\nepoch which is equivalent to 68,507 iterations. For example,\nfor the 8.3B model on 512 GPUs, each epoch takes around\ntwo days. Compared to the conﬁgurations used for our scal-\ning studies in Table 1, the 2.5B model is the same, the 8.3B\nmodel has 24 attention heads instead of 32, and the 355M is\nmuch smaller than any seen previously while still using 64\nGPUs to train, leading to the much lower time per epoch.\nFigure 6 shows validation perpelixity as a function of num-\nber of iterations. As the model size increases, the validation\nperpelixity decreases and reaches a validation perplexity of\n9.27 for the 8.3B model. We report the zero-shot evaluation\nof the trained models on the ', 'of of parallel +data\nSize heads layers parameters GPUs parallel\n(billions) GPUs\n1536 16 40 1.2 1 64\n1920 20 54 2.5 2 128\n2304 24 64 4.2 4 256\n3072 32 72 8.3 8 512\n100%95%82%77%96%83%79%74%0%20%40%60%80%100%\n1248…64128256512Weak ScalingNumber of GPUSModel ParallelModel + Data Parallel\nFigure 5. Model and model + data parallel weak scaling efﬁciency\nas a function of the number of GPUs.\ndone by scaling the batch-size, however, this approach does\nnot address training large models that do not ﬁt on a single\nGPU and it leads to training convergence degradation for\nlarge batch sizes. In contrast, here we use weak scaling to\ntrain larger models that were not possible otherwise. The\nbaseline for all the scaling numbers is the ﬁrst conﬁguration\n(1.2 billion parameters) in Table 1 running on a single GPU.\nThis is a strong baseline as it achieves 39 TeraFLOPS during\nthe overall training process, which is 30% of the theoretical\npeak FLOPS for a single GPU in a DGX-2H server.\nFigure 5 shows scaling values for both model and\nmodel+data parallelism. We observe excellent scaling num-\nbers in both settings. For example, the 8.3 billion parame-\nters case with 8-way (8 GPU) model parallelism achieves\n77% of linear scaling. Model+data parallelism requires fur-\nther communication of gradients and as a result the scaling\nnumbers drop slightly. However, even for the largest conﬁg-\nuration (8.3 billion parameters) running on 512 GPUs, we\nachieve 74% scaling relative to linear scaling of the strong\nsingle GPU baseline conﬁguration (1.2 billion parameters).\nFurther scaling analysis is provided in Appendix D\n5.2. Language Modeling Results Using GPT-2\nTo demonstrate that large language models can further ad-\nvance the state of the art, we consider training GPT-2 models\nof the sizes ', 'GLM model of 130 billion parameters after several\nrounds of discussions and exploration, because such a size makes it possible to run the inference on\na single A100 (40G * 8) server.\nOur first attempt at training the model was in January 2022, shortly after we received a small sponsor\nof GPUs for test running. However, we soon realized that we had significantly underestimated the\ntechnical difficulties of pre-training a model at such a scale (>100B). It seems that pre-training a\nhighly accurate 100B-scale model is quite different from training a 10B-scale one. Due to frequent\nrandom hardware failures, model gradients exploding, unexpected excessive memory usage in the\nalgorithm, debug for the 3D pipeline in the new Megatron and DeepSpeed frameworks, inability to\nrecover from optimizer states, blocked TCP responses between processes, and many many unex-\npected “bugs”, the project was delayed for many times. The Tsinghua PACMAN team gave us a\nhand at this difficult time and together we successfully fixed most of the “bugs”.\nBy March, we were still short on computational resources, but fortunately got a chance to try test\nruns on several other platforms, including Ascend 910, Hygon DCU, NVIDIA, and Sunway. The\nimmediate challenge was for us to adapt our training code to these different platforms, as the under-\nlying operators are quite different. Also, it introduced many new issues: the element-wise operators\nnot supporting fast computation for large-dimension vectors, various issues that hindered conver-\ngence—the large gradient norms of input embeddings, native Post-LN, Pre-LN, and Sandwich-LN,\ndataloader state seeds, and computation precision choices in Softmax and Attention — as well as\nnumerous mistakes we ourselves made. With tremendous help ']","Megatron-LM trained models up to 8.3 billion parameters, sustaining 15.1 PetaFLOPs across 512 GPUs with 76% scaling efficiency.",None,1.0,0.5,0.06260889023542404
What computational approach does PAL use to integrate programmatic reasoning within natural language tasks?,"['ChatGPT tool.1In contrast to\nthe in-context-learning methods we used in the main paper, here we instruct ChatGPT to perform program-aided reasoning\nthrough one of the user utterances.\nIn Figure 13, in COT-style reasoning, while the reasoning chain is correct, the ﬁnal answer is wrong. In contrast, PAL-style\nreasoning could not only accurately extract the color of objects from the question but also produce the correct lines of code\nto branch to different situations that yield their corresponding correct answers.\nA more intriguing example is letting an LLM count the number of letters in the word “intriguing”. In Figure 14a, while the\nstep-by-step explanation appears reasonable by splitting the letters by spaces, ChatGPT does not change the answer after\nthis explicit reasoning and insists on the wrong answer. Explicitly instructing the model to perform step-by-step reasoning\nbefore answering the question still yields the wrong answer. In contrast, PAL-style reasoning only takes a few lines of code,\nand the execution does produce the correct answer, in this case. These examples indicate that PALcan beneﬁt even an\nostensibly powerful model like ChatGPT.\n1chat.openai.com\x0cPAL: Program-aided Language Models 18\n(a) In C OT style reasoning, the correct intermediate reasoning chain leads to wrong answers.\n(b) In P AL, the execution of the code will produce the correct answer.\nFigure 13: ChatGPT with P AL and C OT to answer a user-posted question\x0cPAL: Program-aided Language Models 19\n(a) Step-by-step reasoning struggle on counting the number of letters in the word “intrigu-\ning” which has ten letters.\n(b) Explicitly instructing ChatGPT to reason step-by-step before generating answer still\nleads to the wrong answer.\n(c) P AL takes a few lines of code and ', '. tennis_balls = 5 2 cans of 3 tennis balls each is bought_balls = 2 * 3 tennis balls . The answer is answer = tennis_balls + bought_balls Q: The bakers at the Beverly Hills Bakery baked 200\nloaves of bread on Monday morning. They sold 93 loaves\nin the morning and 39 loaves in the afternoon. A grocery\nstore returned 6 unsold loaves. How many loaves of bread\ndid they have left? Program-aided Language models (this work)\nInput\nModel Output\nA: The bakers started with 200 loaves. They sold 93 in\nthe morning and 39 in the afternoon. So they sold 93 +\n39 = 132 loaves. The grocery store returned 6 loaves. So\nthey had 200 - 132 - 6 = 62 loaves left . The answer is 62. Model Output\n❌\nFigure 1: A diagram illustrating PAL: Given a mathematical reasoning question, Chain-of-thought (left) generates interme-\ndiate reasoning steps of free-form text. In contrast, Program-aided Language models ( PAL, right) generate intermediate\nsteps andPython code. This shifts the role of running the reasoning steps from the language model to the Python interpreter.\nThe ﬁnal answer is obtained by running the generated reasoning chain. Chain-of-thought reasoning is highlighted in blue;\nPAL steps are highlighted in gray and pink ; the Python interpreter run is highlighted in black and green.\nrequire LLMs, solving and reasoning can be done with the\nexternal solver. This bridges an important gap in chain-of-\nthought-like methods, where reasoning chains can be correct\nbut produce an incorrect answer.\nWe demonstrate the effectiveness of PALacross 13arith-\nmetic and symbolic reasoning tasks. In all these tasks,\nPALusing Codex (Chen et al., 2021a) outperforms much\nlarger models such ', 'PAL: Program-aided Language Models\nLuyu Gao* 1Aman Madaan* 1Shuyan Zhou* 1Uri Alon1Pengfei Liu1 2Yiming Yang1Jamie Callan1\nGraham Neubig1 2\n{luyug,amadaan,shuyanzh,ualon,pliu3,yiming,callan,gneubig }@cs.cmu.edu\nAbstract\nLarge language models (LLMs) have recently\ndemonstrated an impressive ability to perform\narithmetic and symbolic reasoning tasks, when\nprovided with a few examples at test time (“few-\nshot prompting”). Much of this success can be\nattributed to prompting methods such as “chain-\nof-thought”, which employ LLMs for both under-\nstanding the problem description by decomposing\nit into steps, as well as solving each step of the\nproblem. While LLMs seem to be adept at this\nsort of step-by-step decomposition, LLMs often\nmake logical and arithmetic mistakes in the solu-\ntion part, even when the problem is decomposed\ncorrectly. In this paper, we present Program-\nAided Language models ( PAL): a novel approach\nthat uses the LLM to read natural language prob-\nlems and generate programs as the intermediate\nreasoning steps, but ofﬂoads the solution step to a\nruntime such as a Python interpreter. With PAL,\ndecomposing the natural language problem into\nrunnable steps remains the only learning task for\nthe LLM, while solving is delegated to the inter-\npreter. We demonstrate this synergy between a\nneural LLM and a symbolic interpreter across 13\nmathematical, symbolic, and algorithmic reason-\ning tasks from BIG-Bench Hard and other bench-\nmarks. In all these natural language reasoning\ntasks, generating code using an LLM and rea-\nsoning using a Python interpreter leads to more\naccurate results than much larger models. For ex-\nample, PALusing CODEX achieves state-of-the-\nart few-shot accuracy on the GSM 8Kbenchmark\nof math word problems, surpassing PaLM-540 B\nwhich uses chain-of-thought by absolute 15% top-\n1. Our code and data are publicly available at\nhttp://reasonwithpal.com .\n*The ﬁrst three authors contributed equally.1Language Tech-\nnologies Institute, Carnegie Mellon University, ']","PAL leverages large language models (LLMs) to generate programs as intermediate reasoning steps for natural language problems, offloading solution steps to a Python interpreter, ensuring accuracy by focusing LLMs on decomposing problems into executable code.","PAL uses a computational approach that involves using a neural LLM to read natural language problems and generate programs as intermediate reasoning steps, while delegating the solution step to a runtime such as a Python interpreter.",0.6666666666666666,1.0,0.8301751613616943
How does PAL handle tasks involving large numbers differently than models relying on chain-of-thought methodologies?,"['PALavoids\nthese problems by ofﬂoading the calculation and some of\nthe reasoning to a Python interpreter, which is correct by\nconstruction, given the right program. Further, not only\nthatPALcan improve the standard chain-of-thought, it can\nimprove least-to-most prompting (Zhou et al., 2022) as well,\nas we show in Appendix I.\nLMs with external tools Several prior works have\nequipped neural models with specialized modules. For ex-\nample, Cobbe et al. (2021) employ a calculator for arith-\nmetic operations as a post hoc processing, and Demeter\n& Downey (2020) add specialized modules for generating\ncities and dates. Unlike these works, PALgenerates code\nfor a Python interpreter, which is general enough to handle\nboth arithmetic calculations and dates, without specialized\nmodules and ad-hoc ﬁxes. Chowdhery et al. (2022) and Wei\net al. (2022) have also experimented with external calcula-\ntors; however, the calculator had improved Codex by only\n2.3% (absolute) on GSM 8Kand improved PaLM-540 Bby\n1.7%, while PALimproves Codex by 6.4% on the same\nbenchmark (Section 5.1). Similarly to our work, Chowd-\nhery et al. (2022) have also experimented with generating\nPython code for solving the GSM 8Kbenchmark, but their\nexperiments resulted in lower accuracy than the standard\nPaLM-540 Bthat uses chain-of-thought. Pi et al. (2022)\npretrain the model on execution results of random expres-\nsions on a calculator, instead of using the solver at test time\nas well. While their model can hypothetically perform arith-\nmetic better than other pretrained LMs, their results on the\nSV AMP benchmark are much lower: 57.4% using a T5-11Bmodel, while PAL achieves 79.4% on the same benchmark\nwithout any specialized pretraining.\nShortly after a preprint of our work was submitted to arXiv,\nanother related work on “program of thought prompting”\n(Chen et al., ', 'the contrary, PALconﬁdently predicts objects[-1] consistently. The more consistent and\nuniform use of expressions in PALcan be attributed to the explicit and deﬁned nature of programming languages, which\nallows for clear and accurate expressions.\nH. Datasets\nIn the following tables (Table 8,Table 9, Table 10), we presents statistics and examples for the datasets we considered.\nDataset N Example\nReasoning about Colored Objects 2000 On the table, you see a bunch of objects arranged in a row: a purple\npaperclip, a pink stress ball, a brown keychain, a green scrunchiephone\ncharger, a mauve ﬁdget spinner, and a burgundy pen. What is the color\nof the object directly to the right of the stress ball?\nPenguins in a Table 149 Here is a table where the ﬁrst line is a header and each subsequent line is\na penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard,\n5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of\nLouis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80\ncm. We now add a penguin to the table: James, 12, 90, 12 How many\npenguins are less than 8 years old?\nDate Understanding 369 2015 is coming in 36 hours. What is the date one week from today in\nMM/DD/YYYY?\nTable 8: Reasoning datasets about everyday objects and concepts.\nDataset N Example\nObject Counting 1000 I have a chair, two potatoes, a cauliﬂower, a lettuce head, two tables, a\ncabbage, two onions, and three fridges. How many vegetables do I have?\nRepeat Copy 32 Repeat the word duck four times, but halfway through also say quack.\nTable 9: Reasoning ', '23.7%. Similarly, PAL\nvastly outperforms COTby absolute 21.8% on REPEAT\nCOPY. Surprisingly, DIRECT prompting performs better\nthan COTonREPEAT COPY. Yet, PALimproves over\nDIRECT by 9.3% in R EPEAT COPY.\n[0,2] [3,5] [6,8] [9,11] [12,14] [15,17] [18,20] [21,23] [24,26]0.60.81\nNumber of ObjectsAccuracy\nPaL\nCoT\nFigure 6: The solve rate on COLORED OBJECTS with re-\nspect to the number of objects included in the test question.\nIs PAL sensitive to the complexity of the question? We\nexamined how the performance of PALandCOTchange as\nthe complexity of the input question grows, measured as the\nnumber of objects in the question of COLORED OBJECTS .\nAs shown in Figure 6, PALis superior COTacross all input\nlengths. As the number of objects in the question increases,\nCOT’s accuracy is unstable and drops, while PALremains\nconsistently close to 100%. More analysis on the token-level\npredictions can be found in Appendix G.\x0cPAL: Program-aided Language Models 7\ncode-cushman-001 code-davinci-001 code-davinci-002020406080\n21.731.872.0\n19.126.060.1\n13.6%22.3%19.8%Solve ratePAL\nCOT\nRelative Improvement\nFigure 7: PALwith different models on GSM8K: though\nthe absolute accuracies with code-cushman-001\nandcode-davinci-001 are lower than\ncode-davinci-002 , the relative improvement of\nPAL over C OT is consistent across models.text-davinci-001 text-davinci-002 text-davinci-003020406080\n26.546.965.3\n8.665.869.8 COT PAL\nFigure 8: PALwith NL LMs on GSM 8K: though\nCOToutperforms PALwithtext-davinci-001 , once\nthe base LM is sufﬁciently strong, PALis beneﬁcial\nwithtext-davinci-002 andtext-davinci-003\nas well. That is, P AL is not limited to code-LMs only.\n6. Analysis\nDoes P AL work with weaker LMs? In all our experi-\nments in Section 5, PALused thecode-davinci-002\nmodel; but can PALwork with weaker models of code? We\ncompared PALwith COTwhen both prompting approaches\nuse the same weaker base LMs code-cushman-001\nandcode-davinci-001 . As shown in Figure 7, even\nthough the absolute accuracies of code-cushman-001\nandcode-davinci-001 are lower, the relative improve-\nment of PALover COTremains consistent across models.\nThis shows that PALcan ']","For tasks with large numbers, PAL demonstrates robust performance by leveraging the Python interpreter for accurate calculations, contrasting with chain-of-thought methodologies where performance drops due to LLMs' computational limitations.","PAL handles tasks involving large numbers differently than models relying on chain-of-thought methodologies by improving the standard chain-of-thought and least-to-most prompting methods. Additionally, PAL generates code for a Python interpreter that is general enough to handle both arithmetic calculations and dates without the need for specialized modules or ad-hoc fixes, resulting in better performance on benchmarks compared to models that use chain-of-thought methodologies.",1.0,1.0,0.7832475304603577
Describe the process and tools provided by CodeNet for transforming code samples into machine-learning-friendly formats.,"['related datasets.\nLarge scale. A useful dataset should contain a large number and variety of data samples to expose\nthe realistic and complex landscape of data distributions one meets in practice. CodeNet is the\nlargest dataset in its class - it has approximately 10 times more code samples than GCJ and its C++\nbenchmark is approximately 10 times larger than POJ-104.\nRich annotation. For the dataset class in question, it is important to include information beyond\nwhich problem a code sample solves to enable a wide range of applications and use cases. It is useful\nto know whether a code sample solves the problem correctly, and if not, the error category (e.g.,\ncompilation error, runtime error, and out-of-memory error). Since the source code is supposed to\nsolve a programming problem, it is advantageous to know the problem statement and have a sample\ninput for execution and a sample output for validation. All such extra information is part of CodeNet\nbut absent in GCJ and POJ-104.\nClean samples. For effective machine learning, the data samples are expected to be independent\nand identically distributed (iid); otherwise, the resulting performance metric could be signiﬁcantly\ninﬂated [ 24]. The existence of duplicate and/or near duplicate code samples makes the iid assumption\ndubious. Hence, it is crucial to identify the near duplicates. The presence of identical problems in the\ndataset poses an even bigger issue. In CodeNet, we analyzed the code samples for (near) duplication\nand used clustering to ﬁnd identical problems. This information is made available as part of the\ndataset release but it is absent in GCJ and POJ-104.\n5 Construction of CodeNet\n5.1 Collection of Code Samples\nThe CodeNet ', 'large collection of code samples with extensive metadata. It\nalso contains documented tools to transform code samples into intermediate representations and to\naccess the dataset and make tailored selections. Our goal is to provide the community with a large,\nhigh-quality curated dataset that can be used to advance AI techniques for source code.\nCodeNet is derived from the data available on two online judge websites: AIZU [ 8] and AtCoder [ 9].\nOnline judge websites pose programming problems in the form of courses and contests. The dataset\nconsists of submissions to these problems, which are judged by an automated review process for\ncorrectness. Problem descriptions, submission outcomes, and associated metadata are available via\nvarious REST APIs.\nScale and Statistics. CodeNet contains a total of 13,916,868 submissions, divided into 4053\nproblems. Among the submissions, 53.6% (7,460,588) are accepted (compilable and pass the\nprescribed tests), 29.5% are marked with wrong answer, and the remaining rejected due to their\nfailure to meet run time or memory requirements. To our knowledge, this is the largest dataset so\nfar among similar kinds. Submissions are in 55 different languages; 95% of them are coded in C++,\nPython, Java, C, Ruby, and C#. C++ is the most common language, with 8,008,527 submissions (57%\nof the total), of which 4,353,049 are accepted. With the abundance of code samples, users can extract\nlarge benchmark datasets that are customized to their downstream use. See Figure 1 for a summary.\nDiversity. The problems in CodeNet are mainly pedagogical and range from elementary exercises\nto sophisticated problems that require advanced algorithms. The submitters range from beginners\nto experienced coders. Some submissions are correct while others contain different ', 'labeled by the\nnumber of near-duplicate code pairs. Each connected graph is then a cluster of potential duplicate\nproblems and we manually inspect the problem descriptions to verify the correctness of this duplicate\ndetection.\np13 41\np535\np424\n31 64p16 20\np564\n19p23 7\np621\n6p26\n1522 p58 44\np853\n28\nFigure 3: An example of a near-duplicate problem graph.\n5.3 Benchmark Datasets\nCodeNet has a rich set of code samples, and the user can assemble a customized benchmark according\nto his/her need. Following POJ-104, we extracted benchmark datasets from CodeNet in C++, Python,\nand Java. The benchmark characteristics are shown in Table 6. For the C++ benchmarks, the number\nof problems and their solutions are chosen to make the benchmark challenging. The benchmarks are\nﬁltered in the following ways. Each code sample is “unique” in the sense that it is not a near-duplicate\nof another code sample. The same is true of each problem. Samples with a large fraction of dead code\nare excluded. Each code sample has successfully passed through the tokenizer, the SPT generator,\nand the graph generator, all described in the next section. This step is to ensure that proper processing\ncan be done to convert a code sample to a machine learning model input.\n6 Code Representation and Tools\nMachine learning with source code requires proper abstractions of the code. The abstractions are\ninstantiated as representations in speciﬁc formats. As a usability feature, we provide several pre-\nprocessing tools to transform source codes into representations that can readily be used as inputs into\nmachine learning models. They are described as follows.\nTokenizer. We offer fast C implementations of tokenizers for C, C++, Java, Python, and JavaScript.\nAdditionally, the parse-tree generator described next can ']","CodeNet offers pre-processing tools for tokenization, simplified parse tree generation, and code graph formation, easing the transformation of code into representations for machine learning models.","CodeNet provides a collection of code samples with extensive metadata and documented tools to transform code samples into intermediate representations. The process involves several steps and tools:

1. Tokenizer: CodeNet offers fast C implementations of tokenizers for C, C++, Java, Python, and JavaScript. The tokenizer converts source code into a sequence of tokens, which are essential for further processing.

2. SPT Generator: CodeNet provides a parse-tree generator to generate abstract syntax trees (ASTs) for code samples. ASTs capture the hierarchical structure of code and are commonly used in machine learning models.

3. Graph Generator: CodeNet includes a graph generator tool to convert code samples into graph representations. Graphs can capture the relationships between different elements of code and are useful for certain types of machine learning tasks.

Overall, these tools provided by CodeNet enable users to preprocess code samples into machine-learning-friendly formats, making it easier to use the dataset for training AI models on source code.",1.0,1.0,0.8801302909851074
What specific challenges do the tasks in SuperGLUE address in natural language processing?,"['corresponds well to human\njudgments of output quality. Some text generation tasks fail to meet this criteria due to issues with\nautomatic metrics like ROUGE and BLEU (Callison-Burch et al., 2006; Liu et al., 2016, i.a.).\n4\x0cPublic data: We require that tasks have existing public training data in order to minimize the risks\ninvolved in newly-created datasets. We also prefer tasks for which we have access to (or could create)\na test set with private labels.\nTask format: We prefer tasks that had relatively simple input and output formats, to avoid incentiviz-\ning the users of the benchmark to create complex task-speciﬁc model architectures. Still, while GLUE\nis restricted to tasks involving single sentence or sentence pair inputs, for SuperGLUE we expand\nthe scope to consider tasks with longer inputs. This yields a set of tasks that requires understanding\nindividual tokens in context, complete sentences, inter-sentence relations, and entire paragraphs.\nLicense: Task data must be available under licences that allow use and redistribution for research\npurposes.\nTo identify possible tasks for SuperGLUE, we disseminated a public call for task proposals to the\nNLP community, and received approximately 30 proposals. We ﬁltered these proposals according\nto our criteria. Many proposals were not suitable due to licensing issues, complex formats, and\ninsufﬁcient headroom; we provide examples of such tasks in Appendix D. For each of the remaining\ntasks, we ran a BERT-based baseline and a human baseline, and ﬁltered out tasks which were either\ntoo challenging for humans without extensive training or too easy for our machine baselines.\n3.2 Selected Tasks\nFollowing this process, we arrived at eight tasks to use in SuperGLUE. See Tables 1 and 2 ', 'advances on GLUE, but\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\nC Human Performance Estimation\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\nannotation. For both steps and all tasks, the average pay rate is $23.75/hr.7\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\nare asked to annotate up to 30 examples from the development set. After answering each example,\nworkers are also asked to check their work against the provided ground truth label. After the training\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\nperformance at, or above the median performance across all workers during training.\n7This estimate is taken from https://turkerview.com .\n16\x0cIn the annotation phase, workers are provided with the same instructions as the training phase, and\nare linked to the same FAQ page. The instructions for all tasks are provided in Appendix C. For the\nannotation phase we randomly sample 100 examples from the task’s test set, with the exception of\nWSC where we annotate the full test set. For each example, we collect annotations from ﬁve workers\nand take a majority vote to estimate human performance. For additional details, see Appendix C.3.\nC.1 Training Phase Instructions\nIn the training step, we provide workers with brief instructions about the training phase. An example\nof these instructions is given Table 5. These training instructions ', 'quality of research contributions aimed at the challenges posed by GLUE\nunderline the utility of this style of benchmark for machine learning researchers looking to evaluate\nnew application-agnostic methods on language understanding.\nLimits to current approaches are also apparent via the GLUE suite. Performance on the GLUE\ndiagnostic entailment dataset, at 0.42 R3, falls far below the average human performance of 0.80\nR3reported in the original GLUE publication, with models performing near, or even below, chance\non some linguistic phenomena (Figure 2, Appendix B). While some initially difﬁcult categories\nsaw gains from advances on GLUE (e.g., double negation), others remain hard (restrictivity) or\neven adversarial (disjunction, downward monotonicity). This suggests that even as unsupervised\npretraining produces ever-better statistical summaries of text, it remains difﬁcult to extract many\ndetails crucial to semantics without the right kind of supervision. Much recent work has made similar\nobservations about the limitations of existing pretrained models (Jia and Liang, 2017; Naik et al.,\n2018; McCoy and Linzen, 2019; McCoy et al., 2019; Liu et al., 2019a,b).\n3\x0cTable 2: Development set examples from the tasks in SuperGLUE. Bold text represents part of the\nexample format for each task. Text in italics is part of the model input. Underlined text is specially\nmarked in the input. Text in a monospaced font represents the expected model output.BoolQPassage: Barq’s – Barq’s is an American soft drink. Its brand of root beer is notable for having caffeine.\nBarq’s, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq\nfamily but bottled by the Coca-Cola Company. It was known as Barq’s Famous Olde Tyme Root Beer\nuntil ']","The tasks challenge models on various fronts, including deep textual understanding, commonsense reasoning, fine-grained word sense disambiguation, and the ability to handle ambiguity and complexity in language, reflecting a broad spectrum of NLP challenges.","The tasks in SuperGLUE address challenges such as restrictivity, disjunction, and downward monotonicity in natural language processing.",1.0,1.0,0.5724429488182068
"How does SuperGLUE's scoring system work, and what does it aim to achieve?","['advances on GLUE, but\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\nC Human Performance Estimation\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\nannotation. For both steps and all tasks, the average pay rate is $23.75/hr.7\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\nare asked to annotate up to 30 examples from the development set. After answering each example,\nworkers are also asked to check their work against the provided ground truth label. After the training\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\nperformance at, or above the median performance across all workers during training.\n7This estimate is taken from https://turkerview.com .\n16\x0cIn the annotation phase, workers are provided with the same instructions as the training phase, and\nare linked to the same FAQ page. The instructions for all tasks are provided in Appendix C. For the\nannotation phase we randomly sample 100 examples from the task’s test set, with the exception of\nWSC where we annotate the full test set. For each example, we collect annotations from ﬁve workers\nand take a majority vote to estimate human performance. For additional details, see Appendix C.3.\nC.1 Training Phase Instructions\nIn the training step, we provide workers with brief instructions about the training phase. An example\nof these instructions is given Table 5. These training instructions ', 'the nonbinding referendum favored statehood, an increase\nover the results of a 2012 referendum, ofﬁcial results from the State Electorcal Commission show. It\nwas the ﬁfth such vote on statehood. ""Today, we the people of Puerto Rico are sending a strong and\nclear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\nPuerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\nfavor of US statehood\nQuery For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\n<placeholder> presidency Correct Entities: USRTEText: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\naccording to the Christopher Reeve Foundation.\nHypothesis: Christopher Reeve had an accident. Entailment: FalseWiCContext 1: Room and board .Context 2: He nailed boards across the windows.\nSense match: FalseWSCText: Mark told Pete many lies about himself, which Pete included in his book. Heshould have been\nmore truthful. Coreference: False\n3 SuperGLUE Overview\n3.1 Design Process\nThe goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\nbeing applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\nwe identify the following desiderata of tasks in the benchmark:\nTask substance: Tasks should test a system’s ability to understand and reason about texts in English.\nTask difﬁculty: Tasks should be beyond the scope of current state-of-the-art systems, but solvable by\nmost college-educated English speakers. We exclude tasks that require domain-speciﬁc knowledge,\ne.g. medical notes or scientiﬁc papers.\nEvaluability: Tasks must have an automatic performance metric that ', 'et al., 2014). Finally, we list the best known\nresult on each task as of May 2019, except on tasks which we recast (WSC), resplit (CB), or achieve\n6For ReCoRD, we predict the entity that has the highest F1 with the other entity options.\n8\x0cthe best known result (WiC). The outside results for COPA, MultiRC, and RTE are from Sap et al.\n(2019), Trivedi et al. (2019), and Liu et al. (2019d) respectively.\nHuman Performance Pilehvar and Camacho-Collados (2019), Khashabi et al. (2018), Nangia and\nBowman (2019), and Zhang et al. (2018) respectively provide estimates for human performance\non WiC, MultiRC, RTE, and ReCoRD. For the remaining tasks, including the diagnostic set, we\nestimate human performance by hiring crowdworker annotators through Amazon’s Mechanical Turk\nplatform to reannotate a sample of each test set. We follow a two step procedure where a crowd\nworker completes a short training phase before proceeding to the annotation phase, modeled after the\nmethod used by Nangia and Bowman (2019) for GLUE. See Appendix C for details.\n5.2 Results\nTable 3 shows results for all baselines. The most frequent class and CBOW baselines do not perform\nwell overall, achieving near chance performance for several of the tasks. Using BERT increases\nthe average SuperGLUE score by 25 points, attaining signiﬁcant gains on all of the benchmark\ntasks, particularly MultiRC, ReCoRD, and RTE. On WSC, BERT actually performs worse than\nthe simple baselines, likely due to the small size of the dataset and the lack of data augmentation.\nUsing MultiNLI as an additional source of supervision for BoolQ, CB, and RTE leads to a 2-5 point\nimprovement on all tasks. Using SWAG as ']"," SuperGLUE uses a single-number metric averaging task scores to simplify comparison across models, aiming to provide a clear, comprehensive measure of progress on a wide range of language understanding tasks.","SuperGLUE's scoring system works by providing a simple and robust evaluation metric for methods applied to a wide range of language understanding tasks. It aims to evaluate tasks that test a system's ability to understand and reason about texts in English, with tasks that are challenging for current state-of-the-art systems but solvable by most college-educated English speakers. Tasks requiring domain-specific knowledge are excluded. The goal is to have tasks with automatic performance metrics that can accurately assess a system's language understanding capabilities.",1.0,1.0,0.7552890777587891
"What practical meta-task is Task2Vec particularly designed to optimize, and how does it achieve this?","['matrices. (Best viewed magniﬁed). (Left) Error matrix for the CUB+iNat\nmeta-task. The numbers in each cell is the test error obtained by training a classiﬁer on a given combination of task (rows)\nand expert (columns). The background color represent the Asymmetric TASK 2VEC distance between the target task and\nthe task used to train the expert. Numbers in red indicate the selection made by the model selection algorithm based on\nthe Asymmetric TASK 2VEC embedding. The (out-of-diagonal) optimal expert (when different from the one selected by our\nalgorithm), is highlighted in blue. (Right) Same as before, but for the Mixed meta-task.', 'we can deﬁne\nDtax(ta,tb) = min\ni∈Sa,j∈Sbd(i,j),\nwhereSa,Sbare the sets of categories in task ta,tband\nd(i,j)is an ultrametric or graph distance in the taxonomy\ntree. Notice that this is a proper distance, and in particular\nit is symmetric.\nTransfer distance. We deﬁne the transfer (or ﬁne-tuning)\ngain from a task tato a tasktb(which we improperly call\ndistance, but is not necessarily symmetric or positive) as\nthe difference in expected performance between a model\ntrained for task tbfrom a ﬁxed initialization (random or pre-\ntrained), and the performance of a model ﬁne-tuned for task\ntbstarting from a solution of task ta:\nDft(ta→tb) =E[ℓa→b]−E[ℓb]\nE[ℓb],\nwhere the expectations are taken over all trainings with the\nselected architecture, training procedure and network ini-\ntialization,ℓbis the ﬁnal test error obtained by training on\ntaskbfrom the chosen initialization, and ℓa→bis the error\nobtained instead when starting from a solution to task aand\nthen ﬁne-tuning (with the selected procedure) on task tb.\n3.1. Symmetric and asymmetric TASK 2VECmetrics\nBy construction, the Fisher embedding on which\nTASK 2VEC is based captures fundamental information\x0cabout the structure of the task. We may therefore expect\nthat the distance between two embeddings correlate posi-\ntively with natural metrics on the space of tasks. However,\nthere are two problems in using the Euclidean distance be-\ntween embeddings: the parameters of the network have dif-\nferent scales, and the norm of the embedding is affected by\ncomplexity of the task and the number of samples used to\ncompute the embedding.\nSymmetric TASK 2VEC distance To make the distance\ncomputation robust, we propose to use the cosine distance\nbetween normalized embeddings:\ndsym(Fa,Fb) =dcos(Fa\nFa+Fb,Fb\nFa+Fb)\n,\nwheredcosis the cosine distance, FaandFbare the two\ntask embeddings ( i.e., the diagonal of the Fisher Informa-\ntion computed on the same probe network), and ', 'on each task.\nA similar approach has been taken in computer vision for\nselecting a detector for a new category out of a large library\nof detectors [26, 40, 38].\n7. Discussion\nTASK 2VEC is an efﬁcient way to represent a task, or the\ncorresponding dataset, as a ﬁxed dimensional vector. It has\nseveral appealing properties, in particular its norm corre-\nlates with the test error obtained on the task, and the co-\nsine distance between embeddings correlates with natural\ndistances between tasks, when available, such as the taxo-\nnomic distance for species classiﬁcation, and the ﬁne-tuning\ndistance for transfer learning. Having a representation of\ntasks paves the way for a wide variety of meta-learning\ntasks. In this work, we focused on selection of an expert\nfeature extractor in order to solve a new task, especially\nwhen little training data is present, and showed that using\nTASK 2VEC to select an expert from a collection can sen-\nsibly improve test performance while adding only a small\noverhead to the training process.\x0cMeta-learning on the space of tasks is an important step\ntoward general artiﬁcial intelligence. In this work, we in-\ntroduce a way of dealing with thousands of tasks, enough to\nenable reconstruct a topology on the task space, and to test\nmeta-learning solutions. The current experiments highlight\nthe usefulness of our methods. Even so, our collection does\nnot capture the full complexity and variety of tasks that one\nmay encounter in real-world situations. Future work should\nfurther test effectiveness, robustness, and limitations of the\nembedding on larger and more diverse collections.\nReferences\n[1] iMaterialist Challenge (Fashion) at FGVC5 workshop,\nCVPR 2018. https://www.kaggle.com/c/\nimaterialist-challenge-fashion-2018 .\n5\n[2] S. M. Abdulrahman, P. Brazdil, J. N. van Rijn, and J. Van-\nschoren. Speeding up ']",Task2Vec is designed for selecting the most suitable pre-trained feature extractor for a new task by learning a metric on task embeddings to predict feature extractor performance.,"Task2Vec is particularly designed to optimize the selection of an expert feature extractor for a new task, especially when there is limited training data available. It achieves this by representing tasks as fixed-dimensional vectors, where the norm of the vector correlates with the test error obtained on the task and the cosine distance between embeddings correlates with natural distances between tasks. This allows Task2Vec to select an expert from a collection based on the similarity of the task vectors, thereby improving test performance with only a small additional overhead to the training process.",1.0,1.0,0.8325223326683044
In what way does Task2Vec's asymmetric distance measure contribute to task similarity assessment and model selection?,"['we can deﬁne\nDtax(ta,tb) = min\ni∈Sa,j∈Sbd(i,j),\nwhereSa,Sbare the sets of categories in task ta,tband\nd(i,j)is an ultrametric or graph distance in the taxonomy\ntree. Notice that this is a proper distance, and in particular\nit is symmetric.\nTransfer distance. We deﬁne the transfer (or ﬁne-tuning)\ngain from a task tato a tasktb(which we improperly call\ndistance, but is not necessarily symmetric or positive) as\nthe difference in expected performance between a model\ntrained for task tbfrom a ﬁxed initialization (random or pre-\ntrained), and the performance of a model ﬁne-tuned for task\ntbstarting from a solution of task ta:\nDft(ta→tb) =E[ℓa→b]−E[ℓb]\nE[ℓb],\nwhere the expectations are taken over all trainings with the\nselected architecture, training procedure and network ini-\ntialization,ℓbis the ﬁnal test error obtained by training on\ntaskbfrom the chosen initialization, and ℓa→bis the error\nobtained instead when starting from a solution to task aand\nthen ﬁne-tuning (with the selected procedure) on task tb.\n3.1. Symmetric and asymmetric TASK 2VECmetrics\nBy construction, the Fisher embedding on which\nTASK 2VEC is based captures fundamental information\x0cabout the structure of the task. We may therefore expect\nthat the distance between two embeddings correlate posi-\ntively with natural metrics on the space of tasks. However,\nthere are two problems in using the Euclidean distance be-\ntween embeddings: the parameters of the network have dif-\nferent scales, and the norm of the embedding is affected by\ncomplexity of the task and the number of samples used to\ncompute the embedding.\nSymmetric TASK 2VEC distance To make the distance\ncomputation robust, we propose to use the cosine distance\nbetween normalized embeddings:\ndsym(Fa,Fb) =dcos(Fa\nFa+Fb,Fb\nFa+Fb)\n,\nwheredcosis the cosine distance, FaandFbare the two\ntask embeddings ( i.e., the diagonal of the Fisher Informa-\ntion computed on the same probe network), and ', 'embedding (distance from origin) and test error obtained on the task.\nnear the decision boundary since p(1−p)is maximized at\np= 1/2. Compare this to the covariance matrix of the data,\nC0, to which all data points contribute equally. Instead, in\nTASK 2VEC information on the domain is based on data near\nthe decision boundary (task-weighted domain embedding).\nEncoding useful features for the task: The FIM de-\npends on the curvature of the loss function with the diagonal\nentries capturing the sensitivity of the loss to model param-\neters. Speciﬁcally, in the two-layer model one can see that,\nif a given feature is uncorrelated with y, the correspond-\ning blocks of Fare zero. In contrast, a domain embedding\nbased on feature activations of the probe network (e.g., C1)\nonly reﬂects which features vary over the dataset without\nindication of whether they are relevant to the task.\n3. Similarity Measures on the Space of Tasks\nWhat metric should be used on the space of tasks? This\ndepends critically on the meta-task we are considering. As a\nmotivation, we concentrate on the meta-task of selecting the\npre-trained feature extractor from a set in order to obtain the\nbest performance on a new training task. There are several\nnatural metrics that may be considered for this meta-task.\nIn this work, we mainly consider:\nTaxonomic distance For some tasks, there is a natural no-\ntion of semantic similarity, for instance deﬁned by sets of\ncategories organized in a taxonomic hierarchy where each\ntask is classiﬁcation inside a subtree of the hierarchy ( e.g.,\nwe may say that classifying breeds of dogs is closer to clas-siﬁcation of cats than it is to classiﬁcation of species of\nplants). In this setting, ', 'the division\nis element-wise. This is a symmetric distance which we ex-\npect to capture semantic similarity between two tasks. For\nexample, we show in Fig. 2 that it correlates well with the\ntaxonomical distance between species on iNaturalist.\nOn the other hand, precisely for this reason, this distance\nis ill-suited for tasks such as model selection, where the (in-\ntrinsically asymmetric) transfer distance is more relevant.\nAsymmetric TASK 2VEC distance In a ﬁrst approxima-\ntion, that does not consider either the model or the training\nprocedure used, positive transfer between two tasks depends\nboth on the similarity between two tasks and on the com-\nplexity of the ﬁrst. Indeed, pre-training on a general but\ncomplex task such as ImageNet often yields a better result\nthan ﬁne-tuning from a close dataset of comparable com-\nplexity. In our case, complexity can be measured as the dis-\ntance from the trivial embedding. This suggests the follow-\ning asymmetric score, again improperly called a “distance”\ndespite being asymmetric and possibly negative:\ndasym(ta→tb) =dsym(ta,tb)−αdsym(ta,t0),\nwheret0is the trivial embedding, and αis an hyperparam-\neter. This has the effect of bring more complex models\ncloser. The hyper-parameter αcan be selected based on\nthe meta-task. In our experiments, we found that the best\nvalue ofα(α= 0.15when using a ResNet-34 pretrained\non ImageNet as the probe network) is robust to the choice\nof meta-tasks.\n4.MODEL 2VEC: task/model co-embedding\nBy construction, the TASK 2VEC distance ignores details\nof the model and only relies on the task. If we know what\ntask a model was trained on, we can represent the model by\nthe embedding of that task. However, in general we may\nnot have such information ( e.g., black-box models or hand-\nconstructed feature extractors). We may also ']","The asymmetric distance accounts for task complexity and transferability, aiding in selecting feature extractors by evaluating their proximity to a task in the embedding space.","The Task2Vec's asymmetric distance measure contributes to task similarity assessment and model selection by considering both the similarity between two tasks and the complexity of the first task. It takes into account positive transfer between tasks based on how pre-training on a general, but complex task, can yield better results than fine-tuning from a close dataset of comparable complexity. This asymmetry in the distance measure helps in bringing more complex models closer, allowing for a more robust assessment of task similarity and model selection based on task complexity.",1.0,1.0,0.6656723618507385
Describe the computational approach to obtaining Task2Vec embeddings using a probe network.,"['embedding (distance from origin) and test error obtained on the task.\nnear the decision boundary since p(1−p)is maximized at\np= 1/2. Compare this to the covariance matrix of the data,\nC0, to which all data points contribute equally. Instead, in\nTASK 2VEC information on the domain is based on data near\nthe decision boundary (task-weighted domain embedding).\nEncoding useful features for the task: The FIM de-\npends on the curvature of the loss function with the diagonal\nentries capturing the sensitivity of the loss to model param-\neters. Speciﬁcally, in the two-layer model one can see that,\nif a given feature is uncorrelated with y, the correspond-\ning blocks of Fare zero. In contrast, a domain embedding\nbased on feature activations of the probe network (e.g., C1)\nonly reﬂects which features vary over the dataset without\nindication of whether they are relevant to the task.\n3. Similarity Measures on the Space of Tasks\nWhat metric should be used on the space of tasks? This\ndepends critically on the meta-task we are considering. As a\nmotivation, we concentrate on the meta-task of selecting the\npre-trained feature extractor from a set in order to obtain the\nbest performance on a new training task. There are several\nnatural metrics that may be considered for this meta-task.\nIn this work, we mainly consider:\nTaxonomic distance For some tasks, there is a natural no-\ntion of semantic similarity, for instance deﬁned by sets of\ncategories organized in a taxonomic hierarchy where each\ntask is classiﬁcation inside a subtree of the hierarchy ( e.g.,\nwe may say that classifying breeds of dogs is closer to clas-siﬁcation of cats than it is to classiﬁcation of species of\nplants). In this setting, ', 'we can deﬁne\nDtax(ta,tb) = min\ni∈Sa,j∈Sbd(i,j),\nwhereSa,Sbare the sets of categories in task ta,tband\nd(i,j)is an ultrametric or graph distance in the taxonomy\ntree. Notice that this is a proper distance, and in particular\nit is symmetric.\nTransfer distance. We deﬁne the transfer (or ﬁne-tuning)\ngain from a task tato a tasktb(which we improperly call\ndistance, but is not necessarily symmetric or positive) as\nthe difference in expected performance between a model\ntrained for task tbfrom a ﬁxed initialization (random or pre-\ntrained), and the performance of a model ﬁne-tuned for task\ntbstarting from a solution of task ta:\nDft(ta→tb) =E[ℓa→b]−E[ℓb]\nE[ℓb],\nwhere the expectations are taken over all trainings with the\nselected architecture, training procedure and network ini-\ntialization,ℓbis the ﬁnal test error obtained by training on\ntaskbfrom the chosen initialization, and ℓa→bis the error\nobtained instead when starting from a solution to task aand\nthen ﬁne-tuning (with the selected procedure) on task tb.\n3.1. Symmetric and asymmetric TASK 2VECmetrics\nBy construction, the Fisher embedding on which\nTASK 2VEC is based captures fundamental information\x0cabout the structure of the task. We may therefore expect\nthat the distance between two embeddings correlate posi-\ntively with natural metrics on the space of tasks. However,\nthere are two problems in using the Euclidean distance be-\ntween embeddings: the parameters of the network have dif-\nferent scales, and the norm of the embedding is affected by\ncomplexity of the task and the number of samples used to\ncompute the embedding.\nSymmetric TASK 2VEC distance To make the distance\ncomputation robust, we propose to use the cosine distance\nbetween normalized embeddings:\ndsym(Fa,Fb) =dcos(Fa\nFa+Fb,Fb\nFa+Fb)\n,\nwheredcosis the cosine distance, FaandFbare the two\ntask embeddings ( i.e., the diagonal of the Fisher Informa-\ntion computed on the same probe network), and ', 'of the total size of the dataset, in each epoch we always sample 10,000\nimages with replacement, uniformly between classes. In this way, all epochs have the same length and see approximately the\nsame number of examples for each class. We use this balanced sampling in all experiments, unless noted otherwise.\nC.2. Computation of the TASK 2VECembedding\nAs the described in the main text, the TASK 2VEC embedding is obtained by choosing a probe network, retraining the ﬁnal\nclassiﬁer on the given task, and then computing the Fisher Information Matrix for the weights of the probe network.\nUnless speciﬁed otherwise, we use an off-the-shelf ResNet-34 pretrained on ImageNet as the probe network. The Fisher\nInformation Matrix is computed in a robust way minimizing the loss function L( ˆw; Λ)with respect to the precision matrix Λ,\nas described before. To make computation of the embedding faster, instead of waiting for the convergence of the classiﬁer,\nwe train the ﬁnal classiﬁer for 2 epochs using Adam and then we continue to train it jointly with the precision matrix Λusing\nthe lossL( ˆw; Λ). We constrain Λto be positive by parametrizing it as Λ = exp(L), for some unconstrained variable L.\nWhile for the classiﬁer we use a low learning rate (1e-4), we found it useful to use an higher learning rate (1e-2) to train L.\nC.3. Training the MODEL 2VECembedding\nAs described in the main text, in the MODEL 2VECembedding we aim to learn a vector representation mj=Fj+bjof\nthej-th model in the collection, which represents both the task the model was trained on (through the TASK 2VEC embedding\nFj), and the particularities of ']","Task2Vec embeddings are obtained by processing images through a pre-trained probe network, retraining the classifier layer for the specific task, and computing the FIM of the network filter parameters.","The computational approach to obtaining Task2Vec embeddings using a probe network involves retraining the final classifier on the given task and then computing the Fisher Information Matrix for the weights of the probe network. An off-the-shelf pretrained ResNet-34 model is commonly used as the probe network. The Fisher Information Matrix is computed by minimizing the loss function with respect to the precision matrix. The final classifier is trained for 2 epochs using Adam, and then training continues jointly with the precision matrix. The precision matrix is constrained to be positive by parametrizing it as an exponential function of an unconstrained variable. Different learning rates are used for training the classifier and the precision matrix for efficient computation of the embedding.",1.0,1.0,0.9050207734107971
What limitations does Task2Vec face regarding its ability to capture the full complexity of tasks in real-world applications?,"['and generally are similar or worse than a\ngeneric feature extractor pre-trained on ImageNet (blue triangles). However, in some cases a carefully chosen expert, trained\non a relevant task, can greatly outperform all other experts (long whisker of the violin plot). The model selection algorithm\nbased on TASK 2VEC can, without training, suggest an expert to use for the task (red cross, lower is better). TASK 2VEC mostly\nrecover the optimal, or close to optimal, feature extractor to use without having to perform an expensive brute-force search\nover all possibilities. Columns are ordered by norm of the task embedding: Notice tasks with lower embedding norm have\nlower error and more “complex” task (task with higher embedding norm) tend to beneﬁt more from a specialized expert.\nTogether with the collection of tasks, we collect several\n“expert” feature extractors. These are ResNet-34 models\npre-trained on ImageNet and then ﬁne-tuned on a speciﬁc\ntask or collection of related tasks (see Supplementary Ma-\nterials for details). We also consider a “generic”expert pre-\ntrained on ImageNet without any ﬁnetuning. Finally, for\neach combination of expert feature extractor and task, we\ntrained a linear classiﬁer on top of the expert in order to\nsolve the selected task using the expert.\nIn total, we trained 4,100 classiﬁers, 156 feature extrac-\ntors and 1,460 embeddings. The total effort to generate the\nﬁnal results was about 1,300 GPU hours.\nMeta-tasks. In Sect. 5.2, for a given task we aim to pre-\ndict, using TASK 2VEC , which expert feature extractor will\nyield the best classiﬁcation performance. In particular, we\nformulate two model selection meta-tasks: iNat + CUB and\nMixed . The ﬁrst consists of 50 tasks and experts ', 'we can deﬁne\nDtax(ta,tb) = min\ni∈Sa,j∈Sbd(i,j),\nwhereSa,Sbare the sets of categories in task ta,tband\nd(i,j)is an ultrametric or graph distance in the taxonomy\ntree. Notice that this is a proper distance, and in particular\nit is symmetric.\nTransfer distance. We deﬁne the transfer (or ﬁne-tuning)\ngain from a task tato a tasktb(which we improperly call\ndistance, but is not necessarily symmetric or positive) as\nthe difference in expected performance between a model\ntrained for task tbfrom a ﬁxed initialization (random or pre-\ntrained), and the performance of a model ﬁne-tuned for task\ntbstarting from a solution of task ta:\nDft(ta→tb) =E[ℓa→b]−E[ℓb]\nE[ℓb],\nwhere the expectations are taken over all trainings with the\nselected architecture, training procedure and network ini-\ntialization,ℓbis the ﬁnal test error obtained by training on\ntaskbfrom the chosen initialization, and ℓa→bis the error\nobtained instead when starting from a solution to task aand\nthen ﬁne-tuning (with the selected procedure) on task tb.\n3.1. Symmetric and asymmetric TASK 2VECmetrics\nBy construction, the Fisher embedding on which\nTASK 2VEC is based captures fundamental information\x0cabout the structure of the task. We may therefore expect\nthat the distance between two embeddings correlate posi-\ntively with natural metrics on the space of tasks. However,\nthere are two problems in using the Euclidean distance be-\ntween embeddings: the parameters of the network have dif-\nferent scales, and the norm of the embedding is affected by\ncomplexity of the task and the number of samples used to\ncompute the embedding.\nSymmetric TASK 2VEC distance To make the distance\ncomputation robust, we propose to use the cosine distance\nbetween normalized embeddings:\ndsym(Fa,Fb) =dcos(Fa\nFa+Fb,Fb\nFa+Fb)\n,\nwheredcosis the cosine distance, FaandFbare the two\ntask embeddings ( i.e., the diagonal of the Fisher Informa-\ntion computed on the same probe network), and ', 'conditions: Performance of\nmodel selection on a subset of 4 tasks as a function of\nthe number of samples available to train relative to opti-\nmal model selection (dashed orange). Training a classiﬁer\non the feature extractor selected by TASK 2VEC (solid red) is\nalways better than using a generic ImageNet feature extrac-\ntor (dashed red). The same holds when allowed to ﬁne-tune\nthe feature extractor (blue curves). Also notice that in the\nlow-data regime ﬁne-tuning the ImageNet feature extractor\nis more expensive and has a worse performance than accu-\nrately selecting a good ﬁxed feature extractor.\nProbe network Top-10 All\nChance +13.95% +59.52%\nVGG-13 +4.82% +38.03%\nDenseNet-121 +0.30% +10.63%\nResNet-13 +0.00% +9.97%\nTable 1: Choice of probe network. Mean relative error\nincrease over the ground-truth optimum on the iNat+CUB\nmeta-task for different choices of the probe-network. We\nalso report the performance on the top 10 tasks with more\nsamples to show how data size affect different architectures.\nembedding recovers similar clusters on iNaturalist. How-\never, on iMaterialst domain embedding collapses all tasks\nto a single uninformative cluster (not a single point due to\nslight noise in embedding computation).\nTask Embedding encodes task difﬁculty The scatter-\nplot in Fig. 3 compares the norm of embedding vectors vs.\nperformance of the best expert (or task speciﬁc model for\ncases where we have the diagonal computed). As shown\nanalytically for the two-layers model, the norm of the task\nembedding correlates with the complexity of the task also\non real tasks and architectures.\n5.2. Model Selection\nGiven a task, our aim is to select an expert feature extrac-\ntor that maximizes the classiﬁcation performance on that\ntask. We propose two strategies: (1) embed the task andselect the feature extractor trained on the most similar ']","While effective, Task2Vec's embeddings may not fully capture the entire spectrum of task complexity and diversity found in broader real-world applications, indicating room for future enhancements.","Task2Vec faces limitations in capturing the full complexity of tasks in real-world applications due to the following reasons:
1. The Euclidean distance between embeddings may not accurately represent the true similarity or complexity of tasks, as the parameters of the network have different scales and the norm of the embedding is influenced by task complexity and sample size.
2. The Task2Vec embedding may collapse all tasks into a single uninformative cluster in certain domains, such as the iMaterialist domain, leading to an oversimplification of task complexity.
3. The Fisher embedding on which Task2Vec is based may not fully capture all the nuances and intricacies of real-world tasks, limiting its ability to accurately differentiate between tasks of varying complexities.
4. The Task2Vec metric may not be able to account for all relevant factors that contribute to task complexity, potentially leading to suboptimal selection of expert feature extractors for specific tasks.",1.0,1.0,0.8809318542480469
"How does GLM-130B manage to achieve INT4 weight quantization without post-training, and what are the benefits?","['from all of our generous partners, we\nfinally succeeded in making our pre-training algorithms runnable across all the platforms—frankly,\na surprising achievement for this project. The timeline of GLM-130B in Figure 21 covers most of\nthe issues we have encountered and addressed as of this writing.\nOn April 26th, we received a generous computing sponsorship from Zhipu.AI — an AI startup that\naims to teach machines to think like humans. After another week of testing, we finally kicked off\nthe training of the GLM-130B model on its 96 A100 (40G * 8) servers on May 6th. Additionally,\nZhipu.AI also sent a team to help evaluate the pre-trained model and build a demonstration website.\nThe training period spanned two months, during which we began developing a toolkit to allow GLM-\n130B’s inference in low-resource setting with swapping technique and quantization. Though it is\nalready the most accessible model of its scale, together with our partner from Tsinghua NLP, we have\nbeen exploring the limit of popularized hardware platforms, which would truly make the 100B-scale\nmodel accessible to as many people as possible. To date, we managed to reach the INT4 weight\nquantization for GLM-130B. Importantly, the INT4 version of GLM-130B without post training\n16This section is largely extracted and updated from the blog introduction of GLM-130B at http://keg.\ncs.tsinghua.edu.cn/glm-130b/ (Posted date: August 4, 2022).\n53\x0cPublished as a conference paper at ICLR 2023\n2021.12•The “千亿”(100B) project towardsan opendensepre-trained GLM at 100B scale isconceived•Survey pre-trainingstrategies of existing models of similar scale,suchasGPT-3,Gopher=> Limited public infoabout how they were trainedand issues they met•SearchforpossibleGPUclusters&sponsors2022.1•TesttheperformanceofFP16/FP32at100Bscaleononetestingcluster•UnexpectedexcessivememoryusageinGLM=> Torchisbetterwithfixedlengthinputsequences•InabilitytoconvergeandtrytricksfromCogViewandViT=> UseSandwich-LN•Frequentrandomhardwarefailures=> HavetorunHCPGtestbeforeeachrun2022.2•Veryslowtrainingspeedthanpreviouslycalculated=>Optimizekernelsandfuseoperators=>Findtheinputshapeiscriticaltokernelperformance•Collectpre-trainingcorporaandtokenize=>Useicetk:thesentencepieceissettotheunigrammode•Debugthe3Dpipelineparallelinthenewly-releasedMegatronandDeepSpeed2022.3•Itcan’trecoverperfectlyfromcheckpoints=>Ourcustomizeddataloaderdonotsaveitsstateseedproperlyindistributedtraining•Thememoryperprocessoristoosmall=>Requiretoomanypipelinestages=>Batchsizeistoolarge(upto12,000)=>Harmthemodel’sconvergency•Itcan’tlaunchmorethan2,000computingnodes=>Overcomethisandsupport6,000-nodetrainingbytuningLinuxkernelTCPparameters•Collectdataformulti-taskinstructionpre-training•Receiveopportunitiestotesttrainingsonseveralotherclusters•Veryslowtrainingspeedthanexpected=>Theunderlyingelement-wiseoperatorsdon’tsupportfastcomputationonlarge-dimensionvectors.2022.4•OptimizeA100kernel’scomputingefficiency=>A100kernelsprefersquare-shapedinputs,andseq_len=2,048isoptimalforourhidden-statedimension(12,288)•Inabilitytoconvergeduetolargegradientnorms(170+)ofinputembeddings=>Tryembeddingnormandgradientshrink,whichturnouttobealmostequivalent•Naïvepost-LNorpre-LNdisconvergesafterseveralthousandsofsteps=>TrySandwich-LNwithPB-Relax•Itstilldisconvergesafteroneweek’strial=>Thedataloaderstateseedsarenotunifiedfordifferentpipelinestages,resultinginamismatchofinputdataandlabels.•Testtwopositionalencodings:RoPEandAlibi=>Alibicanbeslowerasitrequireselement-wisemanipulationonattentionmatrices---changingnum_heads*2,048* 2,048scalarsperlayer•TestGeGLUandGAU=>GAUconvergesfasterwithrelativelypoorperformanceonfine-tunedSuperGLUE•AbnormalGPUmemoryusageofnewly-addedfunctionsandclasses=>DeepSpeedhardcodesthefunctionnamesforcheckpointactivation•DecidetotrainGLMwith130billionparameters=>allowinferenceonaDGX-A10040Gnode2022.5-6•ImplementaRoPEcudaoperatorinC++=>Seeunexpectedprecisionerrorsandfinallyhaveitabandoned•Sandwich-LNstilldisconverges=>1)Reducinglearningratedoesnothelp;2)UsingHingecross-entropybecomesslowerandharmsperformance;3)ShiftingtoDeepNormstilldisconverges•UseFP32insoftmaxofattention=>Success•FindPB-RelaxunnecessaryforFP32softmax=>Italsoslowsdowntrainingasitneedstomanipulatethewholeattentionscorematrices•Experiencefewspikesinlatertraining=>1)Reducegradientshrinkfactorfrom1to0.1:useful;2)Reducethelearningrate:sometimesuseful;3)Jumpthenoisydatabatches:sometimesuseful•Findamistakeinmulti-taskdataaftertrainingfor20,000steps=>Usethecorrectdatabutitdoesnotforget2022.6-7•AdaptthepipelineparallelcheckpointstoordinaryparallelcheckpointsforefficientinferenceonasingleA100•Workonevaluationscriptsondatasets:MMLU,Big-bench,CLUE,SuperCLUE,etc.•ImplementP-TuningandP-Tuningv2forparameter-efficienttuningonGLM-130BfortuningonSuperGLUE•WorkwithBMInfonadaptingGLM-130BtoperforminferenceonasingleV100or3090=>Usepipeline-styleasynchronousswappingbetweenmainmemoryandGPUmemory•Trytofine-tuneGLM-130BwithfewerA100nodes(i.e.,12-16nodes)=>Pipeline-stylefailsduetotoomanypipelinestages=>Findthatdataparallelcannotbeintroducedforfine-tuning=>Use32-waymodelparallelforfine-tuningwithreasonableperformance\nMajorIssuesEncounteredforTrainingGLM-130B\nhttps://github.com/THUDM/GLM-130BTsinghuaKEG\nFigure 21: The timeline of major issues that training GLM-130B encountered and ', 'INT4 version\nhelps additionally save half of the required GPU memory to 70GB, thus allowing GLM-130B infer-\nence on 4 ×RTX 3090 Ti (24G) or 8 ×RTX 2080 Ti (11G). Performance-wise, Table 2 left indicates\nthat without post-training at all, the INT4-version GLM-130B experiences almost no performance\ndegradation, thus maintaining the performance advantages over GPT-3 on common benchmarks.\nGLM’s INT4 Weight Quantization Scaling Law. We examine the underlying mechanism of this\nunique INT4 weight quantization scaling law exhibited in Figure 5 right. We plot the weight value\ndistributions in Figure 5 left, which turns out to directly impact the quantization quality. Specifically,\na wider-distributed linear layer needs to be quantized with larger bins, leading to more precision loss.\nThus the wide-distributed attn-dense andw2matrices explain the INT4 quantization failure for\nGPT-style BLOOM. Conversely, GLMs tend to have much narrower distributions than those of\nsimilar-sized GPTs, and the gap between INT4 and FP16 versions keeps further decreasing as the\nGLM model size scales up (Cf. Figure 15 in Appendix for details).\n5 T HERESULTS\nWe follow the common settings in LLMs such as GPT-3 and PaLM to evaluate GLM-130B for\nEnglish1. As a bilingual LLM with Chinese, GLM-130B is also evaluated on Chinese benchmarks.\nDiscussion on the Scope of Zero-Shot Learning in GLM-130B. Since GLM-130B has been\ntrained with MIP, here we clarify its scope of zero-shot evaluation. In fact, “zero-shot” seems to\nhave controversial interpretations without a consensus in the community. We follow one of the in-\nfluential related surveys (Xian et al., 2018), which says “At test time, in zero-shot learning setting,\nthe aim is to assign a test image to an unseen class label” where ', 'precision\nof all other elements. A common workaround is to group the weight matrix by rows or by columns,\nwith each group being quantized separately and having independent scaling factors.\nB.8 Q UANTIZATION SETTINGS\nOur goal is to save GPU memory as much as possible without hurting model performance. In prac-\ntice, we only quantize linear layers, which take up most of the transformer parameters, and leave\ninput/output embedding, layer normalization, and bias terms unchanged. At the quantization pre-\ncision of INT4, two INT4 weights are compressed into one INT8 weight for saving GPU memory\nusage. Absmax quantization is adopted since we found it enough to maintain model performance,\nand it is more computationally efficient than zeropoint quantization. During inference, only quan-\ntized weights are stored in GPU memory, the FP16 weights for linear layers will be dequantized at\nruntime.\nB.8.1 Q UANTIZATION RESULTS AT SCALES\nGLM models at 110M to 10B scale are from GLM’s original paper(Du et al., 2022). Although the\narchitecture of smaller scale GLMs are not the same as GLM-130B, we believe that the training\nobjective is the key factor for quantization. Table 10 shows the performance of GLM and BLOOM\nfamily models at different scales on the LAMBADA dataset with different quantization methods.\nAlmost all models maintain performance at INT8 precision. In general, GLM maintains better\nperformance than BLOOM at INT4 precision as it scales.\n28\x0cPublished as a conference paper at ICLR 2023\nTable 10: Accuracy on LAMBADA dataset for GLM and BLOOM family at 100M to 176B scales\nacross different quantization precision.\nBLOOM-560M BLOOM-1B1 BLOOM-3B BLOOM-7B BLOOM-176B\nOriginal 31.40% 40.68% 48.30% 54.91% 64.37%\nAbsmax INT8, col-wise 26.12% 40.69% 48.83% 55.33% 65.03%\nAbsmax INT4, col-wise 9.30% ']","By leveraging the GLM architecture's unique scaling law, GLM-130B achieves INT4 weight quantization, reducing memory requirements for inference with minimal performance degradation.","GLM-130B achieves INT4 weight quantization without post-training by compressing two INT4 weights into one INT8 weight for saving GPU memory usage. The benefits of this approach include saving half of the required GPU memory to 70GB, allowing GLM-130B inference on platforms with lower GPU memory, such as 4×RTX 3090 Ti or 8×RTX 2080 Ti. Additionally, the INT4 version of GLM-130B experiences almost no performance degradation, maintaining performance advantages over GPT-3 on common benchmarks.",1.0,1.0,0.8764151334762573
What contributions does GLM-130B offer to the open-source community and AI research field?,"['from all of our generous partners, we\nfinally succeeded in making our pre-training algorithms runnable across all the platforms—frankly,\na surprising achievement for this project. The timeline of GLM-130B in Figure 21 covers most of\nthe issues we have encountered and addressed as of this writing.\nOn April 26th, we received a generous computing sponsorship from Zhipu.AI — an AI startup that\naims to teach machines to think like humans. After another week of testing, we finally kicked off\nthe training of the GLM-130B model on its 96 A100 (40G * 8) servers on May 6th. Additionally,\nZhipu.AI also sent a team to help evaluate the pre-trained model and build a demonstration website.\nThe training period spanned two months, during which we began developing a toolkit to allow GLM-\n130B’s inference in low-resource setting with swapping technique and quantization. Though it is\nalready the most accessible model of its scale, together with our partner from Tsinghua NLP, we have\nbeen exploring the limit of popularized hardware platforms, which would truly make the 100B-scale\nmodel accessible to as many people as possible. To date, we managed to reach the INT4 weight\nquantization for GLM-130B. Importantly, the INT4 version of GLM-130B without post training\n16This section is largely extracted and updated from the blog introduction of GLM-130B at http://keg.\ncs.tsinghua.edu.cn/glm-130b/ (Posted date: August 4, 2022).\n53\x0cPublished as a conference paper at ICLR 2023\n2021.12•The “千亿”(100B) project towardsan opendensepre-trained GLM at 100B scale isconceived•Survey pre-trainingstrategies of existing models of similar scale,suchasGPT-3,Gopher=> Limited public infoabout how they were trainedand issues they met•SearchforpossibleGPUclusters&sponsors2022.1•TesttheperformanceofFP16/FP32at100Bscaleononetestingcluster•UnexpectedexcessivememoryusageinGLM=> Torchisbetterwithfixedlengthinputsequences•InabilitytoconvergeandtrytricksfromCogViewandViT=> UseSandwich-LN•Frequentrandomhardwarefailures=> HavetorunHCPGtestbeforeeachrun2022.2•Veryslowtrainingspeedthanpreviouslycalculated=>Optimizekernelsandfuseoperators=>Findtheinputshapeiscriticaltokernelperformance•Collectpre-trainingcorporaandtokenize=>Useicetk:thesentencepieceissettotheunigrammode•Debugthe3Dpipelineparallelinthenewly-releasedMegatronandDeepSpeed2022.3•Itcan’trecoverperfectlyfromcheckpoints=>Ourcustomizeddataloaderdonotsaveitsstateseedproperlyindistributedtraining•Thememoryperprocessoristoosmall=>Requiretoomanypipelinestages=>Batchsizeistoolarge(upto12,000)=>Harmthemodel’sconvergency•Itcan’tlaunchmorethan2,000computingnodes=>Overcomethisandsupport6,000-nodetrainingbytuningLinuxkernelTCPparameters•Collectdataformulti-taskinstructionpre-training•Receiveopportunitiestotesttrainingsonseveralotherclusters•Veryslowtrainingspeedthanexpected=>Theunderlyingelement-wiseoperatorsdon’tsupportfastcomputationonlarge-dimensionvectors.2022.4•OptimizeA100kernel’scomputingefficiency=>A100kernelsprefersquare-shapedinputs,andseq_len=2,048isoptimalforourhidden-statedimension(12,288)•Inabilitytoconvergeduetolargegradientnorms(170+)ofinputembeddings=>Tryembeddingnormandgradientshrink,whichturnouttobealmostequivalent•Naïvepost-LNorpre-LNdisconvergesafterseveralthousandsofsteps=>TrySandwich-LNwithPB-Relax•Itstilldisconvergesafteroneweek’strial=>Thedataloaderstateseedsarenotunifiedfordifferentpipelinestages,resultinginamismatchofinputdataandlabels.•Testtwopositionalencodings:RoPEandAlibi=>Alibicanbeslowerasitrequireselement-wisemanipulationonattentionmatrices---changingnum_heads*2,048* 2,048scalarsperlayer•TestGeGLUandGAU=>GAUconvergesfasterwithrelativelypoorperformanceonfine-tunedSuperGLUE•AbnormalGPUmemoryusageofnewly-addedfunctionsandclasses=>DeepSpeedhardcodesthefunctionnamesforcheckpointactivation•DecidetotrainGLMwith130billionparameters=>allowinferenceonaDGX-A10040Gnode2022.5-6•ImplementaRoPEcudaoperatorinC++=>Seeunexpectedprecisionerrorsandfinallyhaveitabandoned•Sandwich-LNstilldisconverges=>1)Reducinglearningratedoesnothelp;2)UsingHingecross-entropybecomesslowerandharmsperformance;3)ShiftingtoDeepNormstilldisconverges•UseFP32insoftmaxofattention=>Success•FindPB-RelaxunnecessaryforFP32softmax=>Italsoslowsdowntrainingasitneedstomanipulatethewholeattentionscorematrices•Experiencefewspikesinlatertraining=>1)Reducegradientshrinkfactorfrom1to0.1:useful;2)Reducethelearningrate:sometimesuseful;3)Jumpthenoisydatabatches:sometimesuseful•Findamistakeinmulti-taskdataaftertrainingfor20,000steps=>Usethecorrectdatabutitdoesnotforget2022.6-7•AdaptthepipelineparallelcheckpointstoordinaryparallelcheckpointsforefficientinferenceonasingleA100•Workonevaluationscriptsondatasets:MMLU,Big-bench,CLUE,SuperCLUE,etc.•ImplementP-TuningandP-Tuningv2forparameter-efficienttuningonGLM-130BfortuningonSuperGLUE•WorkwithBMInfonadaptingGLM-130BtoperforminferenceonasingleV100or3090=>Usepipeline-styleasynchronousswappingbetweenmainmemoryandGPUmemory•Trytofine-tuneGLM-130BwithfewerA100nodes(i.e.,12-16nodes)=>Pipeline-stylefailsduetotoomanypipelinestages=>Findthatdataparallelcannotbeintroducedforfine-tuning=>Use32-waymodelparallelforfine-tuningwithreasonableperformance\nMajorIssuesEncounteredforTrainingGLM-130B\nhttps://github.com/THUDM/GLM-130BTsinghuaKEG\nFigure 21: The timeline of major issues that training GLM-130B encountered and ', '. . . . . . . . . . 52\nE.4 Project Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nE.5 Computation Sponsor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nF A Brief History of GLM-130B 53\nG Broader Impact 55\nG.1 Impact on AI Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\nG.2 Impact on Individual Developers and Small Companies . . . . . . . . . . . . . . 55\n20\x0cPublished as a conference paper at ICLR 2023\nG.3 Social Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\nH Environmental Impact 56\nA E THICS : EVALUATION ON BIASES AND TOXICITY\nAlbeit LLMs’ strong abilities in language and beyond, which could bring substantial welfare to\nhuman beings, they can potentially produce toxic and illegal contents for evil use (Weidinger et al.,\n2021; Sheng et al., 2021; Dev et al., 2021; Bommasani et al., 2021). In GLM-130B, before granting\nmodel weight to applicants, in the model license we demand them ', 'studies of LLMs’ theory, capacity, and flaws. Researchers can also modify the\nmodel architecture and weights, to validate the proposed algorithms to improve LLMs Zhu et al.\n(2020); Cao et al. (2021); Hase et al. (2021); Mitchell et al. (2022).\nWith INT4 quantization, GLM-130B can perform inference on popularized GPUs such as 4 ×RTX\n3090 or 8 ×RTX 2080 Ti, which can be easily accessed from cloud service. As a result, researchers\nwho cannot afford powerful data-center GPU servers like DGX-A100 can also utilize GLM-130B.\nG.2 I MPACT ON INDIVIDUAL DEVELOPERS AND SMALL COMPANIES\nCurrently, individual developers and small companies who want to integrate LLMs into their busi-\nness can only choose paid inference APIs. The increased cost can hinder their attempts. Instead,\nGLM-130B can be deployed on popularized hardware that they own or can access via cloud service\nto reduce the cost. Furthermore, they can utilize distillation techniques Sanh et al. (2019); Jiao et al.\n(2020) to obtain smaller models that preserve comparable performance on their specific tasks. While\nsome developers may lack the ability to complete deployment and distillation on their own, we be-\nlieve with GLM-130B and more open LLMs in the future, the corresponding toolkits and service\nproviders will become more available.\nWe also note that currently most applications of LLMs are based on prompt engineering, partly\ndue to the limitation of inference APIs. In downstream scenarios such as online customer service,\nthe companies accumulate huge amounts of human-generated data that contain domain knowledge.\nWith the open-source weights and code, developers can finetune GLM-130B on their own data to\nmitigate the gap of domain knowledge.\nG.3 S OCIAL IMPACT\nLarge language models, together ']"," GLM-130B is fully open-sourced, including model weights, code, and training logs, promoting transparency, reproducibility, and inclusivity in large-scale language model research and applications.","GLM-130B offers the following contributions to the open-source community and AI research field:
- It provides model weights and code open-source, allowing researchers and developers to access and modify the model architecture and weights for validation and improvement of language models.
- With INT4 quantization, GLM-130B can perform inference on popularized GPUs, making it accessible to researchers who cannot afford powerful data-center GPU servers.
- It can be deployed on popularized hardware owned by individual developers and small companies or accessed via cloud service, reducing the cost of utilizing large language models.
- Developers can utilize distillation techniques to obtain smaller models with comparable performance on specific tasks, enhancing accessibility to language models.
- It enables developers to finetune GLM-130B on their own data, mitigating the gap of domain knowledge in downstream applications.
- By providing open-source weights and code, GLM-130B contributes to expanding the availability of tools and service providers in the AI research field.",1.0,1.0,0.8445500731468201
What advancements does Megatron-LM contribute to the handling of layer normalization in BERT-like models to increase performance?,"['/ 87.4 86.5 (89.0 / 85.5)\nXLNet (Yang et al., 2019) 2 90.8 / 90.8 92.3 95.1 / 89.7 90.6 / 87.9 85.4 (88.6 / 84.0)\nMegatron-336M 1 89.7 / 90.0 92.3 94.2 / 88.0 88.1 / 84.8 83.0 (86.9 / 81.5)\nMegatron-1.3B 1 90.9 / 91.0 92.6 94.9 / 89.1 90.2 / 87.1 87.3 (90.4 / 86.1)\nMegatron-3.9B 1 91.4 / 91.4 92.7 95.5 / 90.0 91.2 / 88.5 89.5 (91.8 / 88.6)\nALBERT ensemble (Lan et al., 2019) 95.5 / 90.1 91.4 / 88.9 89.4 (91.2 / 88.6)\nMegatron-3.9B ensemble 95.8 / 90.5 91.7 / 89.0 90.9 (93.1 / 90.0)\nFigure 7. Training loss for BERT model using the original architec-\nture (a) and the rearranged architecture (b). Left ﬁgure shows the\ntraining loss for 336M and 752M BERT model. While the original\narchitecture performs well on the 336M model, the modiﬁcations\nin (b) enable stable training with lower training loss.\nUsing the architecture change in Figure 7(b), we consider\nthree different cases as detailed in Table 4. The 336M model\nhas the same size as BERT-large. The 1.3B is the same as\nthe BERT-xlarge conﬁguration that was previously shown\nto get worse results than the 336M BERT-large model (Lan\net al., 2019). We further scale the BERT model using both\nlarger hidden size as well as more layers to arrive at the 3.9B\nparameter case. In all cases, the hidden size per attention\nhead is kept constant at 64. 336M and 1.3B models are\ntrained for 2 million iterations while the 3.9B model is\ntrained for 1.5 million iterations and is still training.\nOn a 3% held-out set, 336M, 1.3B, and 3.9B models achieve\nvalidation set perplexity ', 'in\nour training data.\n5.3. Bi-directional Transformer Results Using BERT\nIn this section, we apply our methodology to BERT-style\ntransformer models and study the effect of model scaling\non several downstream tasks. Prior work (Lan et al., 2019)\nfound that increasing model size beyond BERT-large with\n336M parameters results in unexpected model degradation.\nTo address this degradation, the authors of that work (Lan\net al., 2019) introduced parameter sharing and showed that\nthat their models scale much better compared to the original\nBERT model.\nWe further investigated this behaviour and empirically\ndemonstrated that rearranging the order of the layer nor-\nmalization and the residual connections as shown in Figure\n7 is critical to enable the scaling of the BERT-style mod-\nels beyond BERT-Large. The architecture (b) in Figure 7\neliminates instabilities observed using the original BERT\narchitecture in (a) and also has a lower training loss. To\nthe best of our knowledge, we are the ﬁrst to report such a\nchange enables training larger BERT models.\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 5. Development set results for MNLI, QQP, SQuAD 1.1 and SQuAD 2.0 and test set results for RACE. The trained tokens represents\nconsumed tokens during model pretraining (proportional to batch size times number of iterations) normalized by consumed tokens during\nmodel pretraining for our 336M model.\nModeltrained tokens MNLI m/mm QQP SQuAD 1.1 SQuAD 2.0 RACE m/h\nratio accuracy accuracy F1 / EM F1 / EM accuracy\n(dev set) (dev set) (dev set) (dev set) (test set)\nRoBERTa (Liu et al., 2019b) 2 90.2 / 90.2 92.2 94.6 / 88.9 89.4 / 86.5 83.2 (86.5 / 81.8)\nALBERT (Lan et al., 2019) 3 90.8 92.2 94.8 / 89.3 90.2 ', 'and conﬁgurations listed in Table 2. The 355M\nmodel is equivalent in size and conﬁguration of BERT-Large\nmodel (Devlin et al., 2018). The 2.5B model is bigger than\nthe previous largest GPT-2 model, and the 8.3B model is\nlarger than any left-to-right transformer language model\never trained, to the best of our knowledge. To train and eval-\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 2. Model conﬁgurations used for GPT-2.\nHidden Time\nParameter Layers Hidden Attn Size Total per\nCount Size Heads per GPUs Epoch\nHead (days)\n355M 24 1024 16 64 64 0.86\n2.5B 54 1920 20 96 128 2.27\n8.3B 72 3072 24 128 512 2.10\nTable 3. Zero-shot results. SOTA are from (Khandelwal et al.,\n2019) for Wikitext103 and (Radford et al., 2019) for LAMBADA.\nModel Wikitext103 LAMBADA\nPerplexity↓ Accuracy↑\n355M 19.31 45.18%\n2.5B 12.76 61.73%\n8.3B 10.81 66.51%\nPrevious SOTA 15.79 63.24%\nuate our language models we use the procedure described in\nsection 4. Table 2 also lists the time it takes to advance one\nepoch which is equivalent to 68,507 iterations. For example,\nfor the 8.3B model on 512 GPUs, each epoch takes around\ntwo days. Compared to the conﬁgurations used for our scal-\ning studies in Table 1, the 2.5B model is the same, the 8.3B\nmodel has 24 attention heads instead of 32, and the 355M is\nmuch smaller than any seen previously while still using 64\nGPUs to train, leading to the much lower time per epoch.\nFigure 6 shows validation perpelixity as a function of num-\nber of iterations. As the model size increases, the validation\nperpelixity decreases and reaches a validation perplexity of\n9.27 for the 8.3B model. We report the zero-shot evaluation\nof the trained models on the ']"," It rearranges the order of layer normalization and residual connections, allowing for stable training and enhanced accuracies as model size increases.","Megatron-LM contributes advancements in rearranging the order of layer normalization and residual connections in BERT-like models, as shown in Figure 7, to enable scaling beyond BERT-Large. This change in architecture eliminates instabilities observed with the original BERT architecture and leads to lower training loss, allowing for more stable training of larger BERT models.",1.0,1.0,0.4246464967727661
What specific properties of Task2Vec embeddings allow for effective reasoning about task space?,"['we can deﬁne\nDtax(ta,tb) = min\ni∈Sa,j∈Sbd(i,j),\nwhereSa,Sbare the sets of categories in task ta,tband\nd(i,j)is an ultrametric or graph distance in the taxonomy\ntree. Notice that this is a proper distance, and in particular\nit is symmetric.\nTransfer distance. We deﬁne the transfer (or ﬁne-tuning)\ngain from a task tato a tasktb(which we improperly call\ndistance, but is not necessarily symmetric or positive) as\nthe difference in expected performance between a model\ntrained for task tbfrom a ﬁxed initialization (random or pre-\ntrained), and the performance of a model ﬁne-tuned for task\ntbstarting from a solution of task ta:\nDft(ta→tb) =E[ℓa→b]−E[ℓb]\nE[ℓb],\nwhere the expectations are taken over all trainings with the\nselected architecture, training procedure and network ini-\ntialization,ℓbis the ﬁnal test error obtained by training on\ntaskbfrom the chosen initialization, and ℓa→bis the error\nobtained instead when starting from a solution to task aand\nthen ﬁne-tuning (with the selected procedure) on task tb.\n3.1. Symmetric and asymmetric TASK 2VECmetrics\nBy construction, the Fisher embedding on which\nTASK 2VEC is based captures fundamental information\x0cabout the structure of the task. We may therefore expect\nthat the distance between two embeddings correlate posi-\ntively with natural metrics on the space of tasks. However,\nthere are two problems in using the Euclidean distance be-\ntween embeddings: the parameters of the network have dif-\nferent scales, and the norm of the embedding is affected by\ncomplexity of the task and the number of samples used to\ncompute the embedding.\nSymmetric TASK 2VEC distance To make the distance\ncomputation robust, we propose to use the cosine distance\nbetween normalized embeddings:\ndsym(Fa,Fb) =dcos(Fa\nFa+Fb,Fb\nFa+Fb)\n,\nwheredcosis the cosine distance, FaandFbare the two\ntask embeddings ( i.e., the diagonal of the Fisher Informa-\ntion computed on the same probe network), and ', 'embedding (distance from origin) and test error obtained on the task.\nnear the decision boundary since p(1−p)is maximized at\np= 1/2. Compare this to the covariance matrix of the data,\nC0, to which all data points contribute equally. Instead, in\nTASK 2VEC information on the domain is based on data near\nthe decision boundary (task-weighted domain embedding).\nEncoding useful features for the task: The FIM de-\npends on the curvature of the loss function with the diagonal\nentries capturing the sensitivity of the loss to model param-\neters. Speciﬁcally, in the two-layer model one can see that,\nif a given feature is uncorrelated with y, the correspond-\ning blocks of Fare zero. In contrast, a domain embedding\nbased on feature activations of the probe network (e.g., C1)\nonly reﬂects which features vary over the dataset without\nindication of whether they are relevant to the task.\n3. Similarity Measures on the Space of Tasks\nWhat metric should be used on the space of tasks? This\ndepends critically on the meta-task we are considering. As a\nmotivation, we concentrate on the meta-task of selecting the\npre-trained feature extractor from a set in order to obtain the\nbest performance on a new training task. There are several\nnatural metrics that may be considered for this meta-task.\nIn this work, we mainly consider:\nTaxonomic distance For some tasks, there is a natural no-\ntion of semantic similarity, for instance deﬁned by sets of\ncategories organized in a taxonomic hierarchy where each\ntask is classiﬁcation inside a subtree of the hierarchy ( e.g.,\nwe may say that classifying breeds of dogs is closer to clas-siﬁcation of cats than it is to classiﬁcation of species of\nplants). In this setting, ', 'task. This can be particu-\nlarly valuable when there is insufﬁcient data to train or ﬁne-\ntune a generic model, and transfer of knowledge is essen-\ntial. TASK 2VEC depends solely on the task, and ignores\ninteractions with the model which may however play an\nimportant role. To address this, we learn a joint task and\nmodel embedding, called MODEL 2VEC, in such a way that\nmodels whose embeddings are close to a task exhibit good\nperfmormance on the task. We use this to select an expert\nfrom a given collection, improving performance relative to\n1arXiv:1902.03545v1 [cs.LG] 10 Feb 2019\x0cTask Embeddings\nDomain EmbeddingsActinopterygii (n)\nAmphibia (n)\nArachnida (n)\nAves (n)\nFungi (n)Insecta (n)\nMammalia (n)\nMollusca (n)\nPlantae (n)\nProtozoa (n)Reptilia (n)\nCategory (m)\nColor (m)\nGender (m)\nMaterial (m)Neckline (m)\nPants (m)\nPattern (m)\nShoes (m)Figure 1: Task embedding across a large library of tasks (best seen magniﬁed). (Left) T-SNE visualization of the embed-\nding of tasks extracted from the iNaturalist, CUB-200, iMaterialist datasets. Colors indicate ground-truth grouping of tasks\nbased on taxonomic or semantic types. Notice that the bird classiﬁcation tasks extracted from CUB-200 embed near the bird\nclassiﬁcation task from iNaturalist, even though the original datasets are different. iMaterialist is well separated from iNat-\nuralist, as it entails very different tasks (clothing attributes). Notice that some tasks of similar type (such as color attributes)\ncluster together but attributes of different task types may also mix when the underlying visual semantics are correlated. For\nexample, the tasks of jeans (clothing type), denim (material) and ripped (style) recognition are close in the task embedding.\n(Right) T-SNE visualization of the domain embeddings (using mean feature activations) for the same tasks. Domain em-\nbedding can distinguish iNaturalist tasks from iMaterialist tasks ']","Task2Vec embeddings are invariant to label space, encode task difficulty, represent task-weighted domain characteristics, and highlight features important for the task.","The specific properties of Task2Vec embeddings that allow for effective reasoning about task space include capturing fundamental information about the structure of the task, correlation with natural metrics on the space of tasks, robustness in distance computation using cosine distance between normalized embeddings, sensitivity of the loss to model parameters, and task-weighted domain embedding near the decision boundary.",0.5,1.0,0.8588199019432068
What distinctive strategy does GLM-130B employ to ensure training stability for a 130-billion-parameter model?,"['GLM model of 130 billion parameters after several\nrounds of discussions and exploration, because such a size makes it possible to run the inference on\na single A100 (40G * 8) server.\nOur first attempt at training the model was in January 2022, shortly after we received a small sponsor\nof GPUs for test running. However, we soon realized that we had significantly underestimated the\ntechnical difficulties of pre-training a model at such a scale (>100B). It seems that pre-training a\nhighly accurate 100B-scale model is quite different from training a 10B-scale one. Due to frequent\nrandom hardware failures, model gradients exploding, unexpected excessive memory usage in the\nalgorithm, debug for the 3D pipeline in the new Megatron and DeepSpeed frameworks, inability to\nrecover from optimizer states, blocked TCP responses between processes, and many many unex-\npected “bugs”, the project was delayed for many times. The Tsinghua PACMAN team gave us a\nhand at this difficult time and together we successfully fixed most of the “bugs”.\nBy March, we were still short on computational resources, but fortunately got a chance to try test\nruns on several other platforms, including Ascend 910, Hygon DCU, NVIDIA, and Sunway. The\nimmediate challenge was for us to adapt our training code to these different platforms, as the under-\nlying operators are quite different. Also, it introduced many new issues: the element-wise operators\nnot supporting fast computation for large-dimension vectors, various issues that hindered conver-\ngence—the large gradient norms of input embeddings, native Post-LN, Pre-LN, and Sandwich-LN,\ndataloader state seeds, and computation precision choices in Softmax and Attention — as well as\nnumerous mistakes we ourselves made. With tremendous help ', 'from all of our generous partners, we\nfinally succeeded in making our pre-training algorithms runnable across all the platforms—frankly,\na surprising achievement for this project. The timeline of GLM-130B in Figure 21 covers most of\nthe issues we have encountered and addressed as of this writing.\nOn April 26th, we received a generous computing sponsorship from Zhipu.AI — an AI startup that\naims to teach machines to think like humans. After another week of testing, we finally kicked off\nthe training of the GLM-130B model on its 96 A100 (40G * 8) servers on May 6th. Additionally,\nZhipu.AI also sent a team to help evaluate the pre-trained model and build a demonstration website.\nThe training period spanned two months, during which we began developing a toolkit to allow GLM-\n130B’s inference in low-resource setting with swapping technique and quantization. Though it is\nalready the most accessible model of its scale, together with our partner from Tsinghua NLP, we have\nbeen exploring the limit of popularized hardware platforms, which would truly make the 100B-scale\nmodel accessible to as many people as possible. To date, we managed to reach the INT4 weight\nquantization for GLM-130B. Importantly, the INT4 version of GLM-130B without post training\n16This section is largely extracted and updated from the blog introduction of GLM-130B at http://keg.\ncs.tsinghua.edu.cn/glm-130b/ (Posted date: August 4, 2022).\n53\x0cPublished as a conference paper at ICLR 2023\n2021.12•The “千亿”(100B) project towardsan opendensepre-trained GLM at 100B scale isconceived•Survey pre-trainingstrategies of existing models of similar scale,suchasGPT-3,Gopher=> Limited public infoabout how they were trainedand issues they met•SearchforpossibleGPUclusters&sponsors2022.1•TesttheperformanceofFP16/FP32at100Bscaleononetestingcluster•UnexpectedexcessivememoryusageinGLM=> Torchisbetterwithfixedlengthinputsequences•InabilitytoconvergeandtrytricksfromCogViewandViT=> UseSandwich-LN•Frequentrandomhardwarefailures=> HavetorunHCPGtestbeforeeachrun2022.2•Veryslowtrainingspeedthanpreviouslycalculated=>Optimizekernelsandfuseoperators=>Findtheinputshapeiscriticaltokernelperformance•Collectpre-trainingcorporaandtokenize=>Useicetk:thesentencepieceissettotheunigrammode•Debugthe3Dpipelineparallelinthenewly-releasedMegatronandDeepSpeed2022.3•Itcan’trecoverperfectlyfromcheckpoints=>Ourcustomizeddataloaderdonotsaveitsstateseedproperlyindistributedtraining•Thememoryperprocessoristoosmall=>Requiretoomanypipelinestages=>Batchsizeistoolarge(upto12,000)=>Harmthemodel’sconvergency•Itcan’tlaunchmorethan2,000computingnodes=>Overcomethisandsupport6,000-nodetrainingbytuningLinuxkernelTCPparameters•Collectdataformulti-taskinstructionpre-training•Receiveopportunitiestotesttrainingsonseveralotherclusters•Veryslowtrainingspeedthanexpected=>Theunderlyingelement-wiseoperatorsdon’tsupportfastcomputationonlarge-dimensionvectors.2022.4•OptimizeA100kernel’scomputingefficiency=>A100kernelsprefersquare-shapedinputs,andseq_len=2,048isoptimalforourhidden-statedimension(12,288)•Inabilitytoconvergeduetolargegradientnorms(170+)ofinputembeddings=>Tryembeddingnormandgradientshrink,whichturnouttobealmostequivalent•Naïvepost-LNorpre-LNdisconvergesafterseveralthousandsofsteps=>TrySandwich-LNwithPB-Relax•Itstilldisconvergesafteroneweek’strial=>Thedataloaderstateseedsarenotunifiedfordifferentpipelinestages,resultinginamismatchofinputdataandlabels.•Testtwopositionalencodings:RoPEandAlibi=>Alibicanbeslowerasitrequireselement-wisemanipulationonattentionmatrices---changingnum_heads*2,048* 2,048scalarsperlayer•TestGeGLUandGAU=>GAUconvergesfasterwithrelativelypoorperformanceonfine-tunedSuperGLUE•AbnormalGPUmemoryusageofnewly-addedfunctionsandclasses=>DeepSpeedhardcodesthefunctionnamesforcheckpointactivation•DecidetotrainGLMwith130billionparameters=>allowinferenceonaDGX-A10040Gnode2022.5-6•ImplementaRoPEcudaoperatorinC++=>Seeunexpectedprecisionerrorsandfinallyhaveitabandoned•Sandwich-LNstilldisconverges=>1)Reducinglearningratedoesnothelp;2)UsingHingecross-entropybecomesslowerandharmsperformance;3)ShiftingtoDeepNormstilldisconverges•UseFP32insoftmaxofattention=>Success•FindPB-RelaxunnecessaryforFP32softmax=>Italsoslowsdowntrainingasitneedstomanipulatethewholeattentionscorematrices•Experiencefewspikesinlatertraining=>1)Reducegradientshrinkfactorfrom1to0.1:useful;2)Reducethelearningrate:sometimesuseful;3)Jumpthenoisydatabatches:sometimesuseful•Findamistakeinmulti-taskdataaftertrainingfor20,000steps=>Usethecorrectdatabutitdoesnotforget2022.6-7•AdaptthepipelineparallelcheckpointstoordinaryparallelcheckpointsforefficientinferenceonasingleA100•Workonevaluationscriptsondatasets:MMLU,Big-bench,CLUE,SuperCLUE,etc.•ImplementP-TuningandP-Tuningv2forparameter-efficienttuningonGLM-130BfortuningonSuperGLUE•WorkwithBMInfonadaptingGLM-130BtoperforminferenceonasingleV100or3090=>Usepipeline-styleasynchronousswappingbetweenmainmemoryandGPUmemory•Trytofine-tuneGLM-130BwithfewerA100nodes(i.e.,12-16nodes)=>Pipeline-stylefailsduetotoomanypipelinestages=>Findthatdataparallelcannotbeintroducedforfine-tuning=>Use32-waymodelparallelforfine-tuningwithreasonableperformance\nMajorIssuesEncounteredforTrainingGLM-130B\nhttps://github.com/THUDM/GLM-130BTsinghuaKEG\nFigure 21: The timeline of major issues that training GLM-130B encountered and ', 'prompted datasets from (Sanh et al., 2022; Wang et al., 2022a), listed\nin Appendix C and Table 12. GLM-130B users are suggested to avoid evaluating its zero-shot and\nfew-shot capabilities on these datasets according to the criterion illustrated in Section 5.\n2.3 P LATFORM -AWARE PARALLEL STRATEGIES AND MODEL CONFIGURATIONS\nGLM-130B is trained on a cluster of 96 DGX-A100 GPU (8 ×40G) servers with a 60-day access.\nThe goal is to pass through as many tokens as possible, as a recent study (Hoffmann et al., 2022)\nsuggests that most existing LLMs are largely under-trained.\nThe 3D Parallel Strategy. The data parallelism (Valiant, 1990) and tensor model paral-\nlelism (Shoeybi et al., 2019) are the de facto practices for training billion-scale models (Wang &\nKomatsuzaki, 2021; Du et al., 2022). To further handle the huge GPU memory requirement and the\ndecrease in overall GPU utilization resulted from applying tensor parallel between nodes—as 40G\nrather than 80G A100s are used for training GLM-130B, we combine the pipeline model parallelism\nwith the other two strategies to form a 3D parallel strategy.\nThe pipeline parallelism divides the model into sequential stages for each parallel group, and to fur-\nther minimize bubbles introduced by pipeline, we leverage the PipeDream-Flush (Narayanan et al.,\n2021) implementation from DeepSpeed (Rasley et al., 2020) to train GLM-130B with a relative\n4\x0cPublished as a conference paper at ICLR 2023\nbig global batch size (4,224) to reduce time and GPU memory wasting. Through both numeri-\ncal and empirical examinations, we adopt 4-way tensor parallelism and 8-way pipeline parallelism\n(Cf. Appendix B.4 for details). Following the calculation in (Chowdhery et al., 2022), we report\nhardware FLOPs utilization (HFU) ']","GLM-130B utilizes embedding gradient shrink (EGS) and DeepNorm layer normalization to ensure training stability, significantly reducing the likelihood of loss spikes and divergence.","GLM-130B employs a 3D parallel strategy, combining data parallelism, tensor model parallelism, and pipeline model parallelism. This strategy helps handle the huge GPU memory requirement and decrease overall GPU utilization, ensuring training stability for a 130-billion-parameter model.",1.0,1.0,0.7071793675422668
What parallel strategies and configurations are utilized to train GLM-130B efficiently on a GPU cluster?,"['prompted datasets from (Sanh et al., 2022; Wang et al., 2022a), listed\nin Appendix C and Table 12. GLM-130B users are suggested to avoid evaluating its zero-shot and\nfew-shot capabilities on these datasets according to the criterion illustrated in Section 5.\n2.3 P LATFORM -AWARE PARALLEL STRATEGIES AND MODEL CONFIGURATIONS\nGLM-130B is trained on a cluster of 96 DGX-A100 GPU (8 ×40G) servers with a 60-day access.\nThe goal is to pass through as many tokens as possible, as a recent study (Hoffmann et al., 2022)\nsuggests that most existing LLMs are largely under-trained.\nThe 3D Parallel Strategy. The data parallelism (Valiant, 1990) and tensor model paral-\nlelism (Shoeybi et al., 2019) are the de facto practices for training billion-scale models (Wang &\nKomatsuzaki, 2021; Du et al., 2022). To further handle the huge GPU memory requirement and the\ndecrease in overall GPU utilization resulted from applying tensor parallel between nodes—as 40G\nrather than 80G A100s are used for training GLM-130B, we combine the pipeline model parallelism\nwith the other two strategies to form a 3D parallel strategy.\nThe pipeline parallelism divides the model into sequential stages for each parallel group, and to fur-\nther minimize bubbles introduced by pipeline, we leverage the PipeDream-Flush (Narayanan et al.,\n2021) implementation from DeepSpeed (Rasley et al., 2020) to train GLM-130B with a relative\n4\x0cPublished as a conference paper at ICLR 2023\nbig global batch size (4,224) to reduce time and GPU memory wasting. Through both numeri-\ncal and empirical examinations, we adopt 4-way tensor parallelism and 8-way pipeline parallelism\n(Cf. Appendix B.4 for details). Following the calculation in (Chowdhery et al., 2022), we report\nhardware FLOPs utilization (HFU) ', 'of of parallel +data\nSize heads layers parameters GPUs parallel\n(billions) GPUs\n1536 16 40 1.2 1 64\n1920 20 54 2.5 2 128\n2304 24 64 4.2 4 256\n3072 32 72 8.3 8 512\n100%95%82%77%96%83%79%74%0%20%40%60%80%100%\n1248…64128256512Weak ScalingNumber of GPUSModel ParallelModel + Data Parallel\nFigure 5. Model and model + data parallel weak scaling efﬁciency\nas a function of the number of GPUs.\ndone by scaling the batch-size, however, this approach does\nnot address training large models that do not ﬁt on a single\nGPU and it leads to training convergence degradation for\nlarge batch sizes. In contrast, here we use weak scaling to\ntrain larger models that were not possible otherwise. The\nbaseline for all the scaling numbers is the ﬁrst conﬁguration\n(1.2 billion parameters) in Table 1 running on a single GPU.\nThis is a strong baseline as it achieves 39 TeraFLOPS during\nthe overall training process, which is 30% of the theoretical\npeak FLOPS for a single GPU in a DGX-2H server.\nFigure 5 shows scaling values for both model and\nmodel+data parallelism. We observe excellent scaling num-\nbers in both settings. For example, the 8.3 billion parame-\nters case with 8-way (8 GPU) model parallelism achieves\n77% of linear scaling. Model+data parallelism requires fur-\nther communication of gradients and as a result the scaling\nnumbers drop slightly. However, even for the largest conﬁg-\nuration (8.3 billion parameters) running on 512 GPUs, we\nachieve 74% scaling relative to linear scaling of the strong\nsingle GPU baseline conﬁguration (1.2 billion parameters).\nFurther scaling analysis is provided in Appendix D\n5.2. Language Modeling Results Using GPT-2\nTo demonstrate that large language models can further ad-\nvance the state of the art, we consider training GPT-2 models\nof the sizes ', 'GLM model of 130 billion parameters after several\nrounds of discussions and exploration, because such a size makes it possible to run the inference on\na single A100 (40G * 8) server.\nOur first attempt at training the model was in January 2022, shortly after we received a small sponsor\nof GPUs for test running. However, we soon realized that we had significantly underestimated the\ntechnical difficulties of pre-training a model at such a scale (>100B). It seems that pre-training a\nhighly accurate 100B-scale model is quite different from training a 10B-scale one. Due to frequent\nrandom hardware failures, model gradients exploding, unexpected excessive memory usage in the\nalgorithm, debug for the 3D pipeline in the new Megatron and DeepSpeed frameworks, inability to\nrecover from optimizer states, blocked TCP responses between processes, and many many unex-\npected “bugs”, the project was delayed for many times. The Tsinghua PACMAN team gave us a\nhand at this difficult time and together we successfully fixed most of the “bugs”.\nBy March, we were still short on computational resources, but fortunately got a chance to try test\nruns on several other platforms, including Ascend 910, Hygon DCU, NVIDIA, and Sunway. The\nimmediate challenge was for us to adapt our training code to these different platforms, as the under-\nlying operators are quite different. Also, it introduced many new issues: the element-wise operators\nnot supporting fast computation for large-dimension vectors, various issues that hindered conver-\ngence—the large gradient norms of input embeddings, native Post-LN, Pre-LN, and Sandwich-LN,\ndataloader state seeds, and computation precision choices in Softmax and Attention — as well as\nnumerous mistakes we ourselves made. With tremendous help ']","GLM-130B employs a 3D parallel strategy combining data, tensor, and pipeline model parallelism, optimized for 96 DGX-A100 GPU nodes, maximizing token throughput and hardware utilization.","The parallel strategies and configurations utilized to train GLM-130B efficiently on a GPU cluster include data parallelism, tensor model parallelism, pipeline parallelism, 4-way tensor parallelism, and 8-way pipeline parallelism. These strategies are combined to form a 3D parallel strategy. Additionally, a relative big global batch size of 4,224 is adopted to reduce time and GPU memory wastage during training.",1.0,1.0,0.8604877591133118
How does Megatron-LM's model parallel approach optimize memory and computation distribution across GPUs?,"['of of parallel +data\nSize heads layers parameters GPUs parallel\n(billions) GPUs\n1536 16 40 1.2 1 64\n1920 20 54 2.5 2 128\n2304 24 64 4.2 4 256\n3072 32 72 8.3 8 512\n100%95%82%77%96%83%79%74%0%20%40%60%80%100%\n1248…64128256512Weak ScalingNumber of GPUSModel ParallelModel + Data Parallel\nFigure 5. Model and model + data parallel weak scaling efﬁciency\nas a function of the number of GPUs.\ndone by scaling the batch-size, however, this approach does\nnot address training large models that do not ﬁt on a single\nGPU and it leads to training convergence degradation for\nlarge batch sizes. In contrast, here we use weak scaling to\ntrain larger models that were not possible otherwise. The\nbaseline for all the scaling numbers is the ﬁrst conﬁguration\n(1.2 billion parameters) in Table 1 running on a single GPU.\nThis is a strong baseline as it achieves 39 TeraFLOPS during\nthe overall training process, which is 30% of the theoretical\npeak FLOPS for a single GPU in a DGX-2H server.\nFigure 5 shows scaling values for both model and\nmodel+data parallelism. We observe excellent scaling num-\nbers in both settings. For example, the 8.3 billion parame-\nters case with 8-way (8 GPU) model parallelism achieves\n77% of linear scaling. Model+data parallelism requires fur-\nther communication of gradients and as a result the scaling\nnumbers drop slightly. However, even for the largest conﬁg-\nuration (8.3 billion parameters) running on 512 GPUs, we\nachieve 74% scaling relative to linear scaling of the strong\nsingle GPU baseline conﬁguration (1.2 billion parameters).\nFurther scaling analysis is provided in Appendix D\n5.2. Language Modeling Results Using GPT-2\nTo demonstrate that large language models can further ad-\nvance the state of the art, we consider training GPT-2 models\nof the sizes ', 'data parallelism and handling random number\ngeneration.\nB.1. Hybrid Model and Data Parallelism\nModel parallelism is orthogonal to data parallelism, and so\nwe can use both simultaneously to train large models in a\nreasonable amount of time. Figure 8 shows a grouping of\nGPUs for hybrid model and data parallelism. Two or more\nGPUs within the same server form model parallel groups\n(for example GPUs 1 to 8 in Figure 8), and contain oneTable 6. Hyperparameters for ﬁnetuning BERT model on down-\nstream tasks.\nTask Model Batch Learning Training\nsize rate epochs\n336M\nMNLI 1.3B 128 1e-5 10\n3.8B\n336M 128 5e-5\nQQP 1.3B 128 3e-5 12\n3.8B 256 4e-5\n336M 64 3e-5\nSQUAD 1.1 1.3B 48 3e-5 2\n3.8B 48 1e-5\n336M 48 3e-5\nSQUAD 2.0 1.3B 64 3e-5 2\n3.8B 48 1e-5\n336M 32 2e-5\nRACE 1.3B 16 1e-5 3\n3.8B 32 2e-5\ninstance of the model distributed across these GPUs. The\nremaining GPUs, which could be within the same server but\nmore typically are located in other servers, run additional\nmodel parallel groups. GPUs with the same position in each\nof the model parallel groups (for example GPUs 1, 9, ...,\n505 in Figure 8) form data parallel groups so that all GPUs\nwithin a data parallel group hold the same model param-\neters. During back propagation we run multiple gradient\nall-reduce operations in parallel to reduce weight gradients\nwithin each distinct data parallel group. The total number\nof required GPUs is the product of the number of model\nand data parallel groups. For example, for the 8.3 billion\nparameter model we use 8 GPUs per model parallel group\nand 64-way data parallelism, for a total of 512 GPUs. All\ncommunication is implemented in PyTorch by Python calls\nto NCCL. GPUs within each model parallel ', 'group perform\nall-reduces amongst all GPUs within the group. For data\nparallelism, each of the all-reduce operations takes place\nwith one of the GPUs from each model parallel group.\nB.2. Model Parallel Random Number Generation\nTechniques that utilize random number generation, such\nas dropout, are a staple of modern deep learning training.\nTransformers have dropout layers outside the model parallel\nregions before residual connections and within model paral-\nlel regions in the self attention block. Because some dropout\nlayers are in a model parallel region, while others are not,\nwe need to treat random number generation carefully to\nensure dropout works correctly. To synchronize residual\nconnection dropout across model parallel workers we seed\nthe random number generators at the beginning of training\nwith the same seed. This results in identical dropout patterns\nacross all model parallel workers. However, dropout within\na model parallel region should result in different random\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\npatterns for each worker to achieve randomness across the\nentire operation. To achieve this we maintain a separate\nrandom number generator for dropout within model parallel\nregions. This random number generator is uniquely seeded\nfor each model parallel worker.\nFigure 8. Grouping of GPUs for hybrid model and data parallelism\nwith 8-way model parallel and 64-way data parallel.\nC. Text Samples\nBelow are some text samples generated by Megatron-LM\nusing a context prompt. Some of the texts are cut short.Context : There is a major force uniting Americas ﬁercely\npartisan politicians: big technology companies. Democrats\nand Republicans at the federal and state levels are coming\ntogether to scrutinize the power of the Silicon Valley giants\nand, potentially, to rein them in. Letitia James, the Demo-\ncratic attorney general of New York, ']", By partitioning key operations such as GEMMs within transformer layers across GPUs and minimizing communication overhead with strategic all-reduce operations.,"Megatron-LM's model parallel approach optimizes memory and computation distribution across GPUs by using hybrid model and data parallelism. This approach allows for training larger models in a reasonable amount of time by grouping GPUs into model parallel groups and data parallel groups. Model parallel groups within the same server distribute one instance of the model across GPUs, while data parallel groups hold the same model parameters. During back propagation, multiple gradient all-reduce operations are run in parallel to reduce weight gradients within each distinct data parallel group. This distribution of memory and computation across GPUs helps in training larger models efficiently.",0.0,1.0,0.5667886734008789
How does Megatron-LM address the challenges of large batch training and optimization in transformer models?,"['and conﬁgurations listed in Table 2. The 355M\nmodel is equivalent in size and conﬁguration of BERT-Large\nmodel (Devlin et al., 2018). The 2.5B model is bigger than\nthe previous largest GPT-2 model, and the 8.3B model is\nlarger than any left-to-right transformer language model\never trained, to the best of our knowledge. To train and eval-\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 2. Model conﬁgurations used for GPT-2.\nHidden Time\nParameter Layers Hidden Attn Size Total per\nCount Size Heads per GPUs Epoch\nHead (days)\n355M 24 1024 16 64 64 0.86\n2.5B 54 1920 20 96 128 2.27\n8.3B 72 3072 24 128 512 2.10\nTable 3. Zero-shot results. SOTA are from (Khandelwal et al.,\n2019) for Wikitext103 and (Radford et al., 2019) for LAMBADA.\nModel Wikitext103 LAMBADA\nPerplexity↓ Accuracy↑\n355M 19.31 45.18%\n2.5B 12.76 61.73%\n8.3B 10.81 66.51%\nPrevious SOTA 15.79 63.24%\nuate our language models we use the procedure described in\nsection 4. Table 2 also lists the time it takes to advance one\nepoch which is equivalent to 68,507 iterations. For example,\nfor the 8.3B model on 512 GPUs, each epoch takes around\ntwo days. Compared to the conﬁgurations used for our scal-\ning studies in Table 1, the 2.5B model is the same, the 8.3B\nmodel has 24 attention heads instead of 32, and the 355M is\nmuch smaller than any seen previously while still using 64\nGPUs to train, leading to the much lower time per epoch.\nFigure 6 shows validation perpelixity as a function of num-\nber of iterations. As the model size increases, the validation\nperpelixity decreases and reaches a validation perplexity of\n9.27 for the 8.3B model. We report the zero-shot evaluation\nof the trained models on the ', 'in\nour training data.\n5.3. Bi-directional Transformer Results Using BERT\nIn this section, we apply our methodology to BERT-style\ntransformer models and study the effect of model scaling\non several downstream tasks. Prior work (Lan et al., 2019)\nfound that increasing model size beyond BERT-large with\n336M parameters results in unexpected model degradation.\nTo address this degradation, the authors of that work (Lan\net al., 2019) introduced parameter sharing and showed that\nthat their models scale much better compared to the original\nBERT model.\nWe further investigated this behaviour and empirically\ndemonstrated that rearranging the order of the layer nor-\nmalization and the residual connections as shown in Figure\n7 is critical to enable the scaling of the BERT-style mod-\nels beyond BERT-Large. The architecture (b) in Figure 7\neliminates instabilities observed using the original BERT\narchitecture in (a) and also has a lower training loss. To\nthe best of our knowledge, we are the ﬁrst to report such a\nchange enables training larger BERT models.\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 5. Development set results for MNLI, QQP, SQuAD 1.1 and SQuAD 2.0 and test set results for RACE. The trained tokens represents\nconsumed tokens during model pretraining (proportional to batch size times number of iterations) normalized by consumed tokens during\nmodel pretraining for our 336M model.\nModeltrained tokens MNLI m/mm QQP SQuAD 1.1 SQuAD 2.0 RACE m/h\nratio accuracy accuracy F1 / EM F1 / EM accuracy\n(dev set) (dev set) (dev set) (dev set) (test set)\nRoBERTa (Liu et al., 2019b) 2 90.2 / 90.2 92.2 94.6 / 88.9 89.4 / 86.5 83.2 (86.5 / 81.8)\nALBERT (Lan et al., 2019) 3 90.8 92.2 94.8 / 89.3 90.2 ', 'architecture, GPT-2, and\nan encoder architecture, BERT.\nFigure 2 shows a schematic diagram of the model we used.\nWe refer the reader to prior work for a detailed descrip-\ntion of the model architecture (Vaswani et al., 2017; Devlin\net al., 2018; Radford et al., 2019). It is worthwhile to men-\ntion that both GPT-2 and BERT use GeLU (Hendrycks &\nGimpel, 2016) nonlinearities and layer normalization (Ba\net al., 2016) to the input of the multi-head attention and feed\nforward layers, whereas the original transformer (Vaswani\net al., 2017) uses ReLU nonlinearities and applies layer\nnormalization to outputs.\n2.3. Data and Model Parallelism in Deep Learning\nThere are two central paradigms for scaling out deep neu-\nral network training to numerous hardware accelerators:\ndata parallelism (Valiant, 1990) where a training minibatch\nis split across multiple workers, and model parallelism in\nwhich the memory usage and computation of a model is\ndistributed across multiple workers. By increasing the mini-\nbatch size proportionally to the number of available work-\ners (i.e. weak scaling ), one observes near linear scaling\nin training data throughput. However, large batch train-\ning introduces complications into the optimization process\nthat can result in reduced accuracy or longer time to conver-\ngence, offsetting the beneﬁt of increased training throughput\n(Keskar et al., 2017). Further research (Goyal et al., 2017;\nYou et al., 2017; 2019) has developed techniques to miti-gate these effects and drive down the training time of large\nneural networks. To scale out training even further, parallel\nwork (Chen et al., 2016) has combined data parallelism with\nactivation checkpointing: recomputing activations in the\nbackward pass without storing them in the forward pass to\nreduce memory requirements.\nHowever, these techniques have one fundamental limitation\nin the ']","Through mixed precision training with dynamic loss scaling, weight initialization strategies, and careful optimizer configurations, including Adam with weight decay.","Megatron-LM addresses the challenges of large batch training and optimization in transformer models by utilizing model parallelism in addition to data parallelism. By distributing the memory usage and computation of the model across multiple workers, Megatron-LM is able to scale out training even further. This approach allows for near-linear scaling in training data throughput by increasing the minibatch size proportionally to the number of available workers. Additionally, Megatron-LM implements techniques to mitigate the complications introduced by large batch training, such as reduced accuracy or longer time to convergence, by developing methods to drive down the training time of large neural networks.",1.0,1.0,0.4457118511199951
"What specific tasks and benchmarks were used to evaluate PAL's performance, and what were the results?","['times, but halfway through also\nsay quack” ).5. Results\n5.1. Math Results\nTable 1 shows the following results: across all tasks,\nPALusing Codex sets a new few-shot state-of-the-art top-\n1 decoding across all datasets, outperforming COTCodex,\nCOTPaLM-540 B, and COTMinerva 540B which was ﬁne-tuned\non explicit mathematical content.\nInterestingly, COTalso beneﬁts from Codex over PaLM-\n540 Bin some of the datasets such as ASDIV , but performs\nworse than PaLM-540 Bin others such as SVAMP . Yet,\nusing PALfurther improves the solve rate across all datasets.\nGSM-HARD On GSM -HARD (Table 1), the accuracy of\nDIRECT drops dramatically from 19.7% to 5.0% (a relative\ndrop of 74%), the accuracy of COTdrops from 65.6% to\n20.1% (a relative drop of almost 70%), while PALremains\nstable at 61.5%, dropping by only 14.3%. The results of\nCOTonGSM-HARD did not improve even when we replaced\nits prompts with prompts that include large numbers (Ap-\npendix B). This shows how PALprovides not only better\x0cPAL: Program-aided Language Models 6\nCOLORED OBJECT PENGUINS DATE REPEAT COPY OBJECT COUNTING\nDIRECT Codex 75.7 71.1 49.9 81.3 37.6\nCOTLaMDA-137B - - 26.8 - -\nCOTPaLM-540 B - 65.1 65.3 - -\nCOTCodex 86.3 79.2 64.8 68.8 73.0\nPALCodex 95.1 93.3 76.2 90.6 96.7\nTable 2: Solve rate on three symbolic reasoning datasets and two algorithmic datasets, In all datasets, PALachieves a much\nhigher accuracy than chain-of-thought. Results with closed models LaMDA-137B and PaLM-540B are included if available\nto public (Wei et al., 2022; Suzgun et al., 2022).\nresults on the standard benchmarks, but is also much more\nrobust . In fact, since PALofﬂoads the computation to the\nPython interpreter, any complex computation can be per-\nformed accurately given the correctly generated program.\nLarge Numbers or Incorrect Reasoning? Are the ', '23.7%. Similarly, PAL\nvastly outperforms COTby absolute 21.8% on REPEAT\nCOPY. Surprisingly, DIRECT prompting performs better\nthan COTonREPEAT COPY. Yet, PALimproves over\nDIRECT by 9.3% in R EPEAT COPY.\n[0,2] [3,5] [6,8] [9,11] [12,14] [15,17] [18,20] [21,23] [24,26]0.60.81\nNumber of ObjectsAccuracy\nPaL\nCoT\nFigure 6: The solve rate on COLORED OBJECTS with re-\nspect to the number of objects included in the test question.\nIs PAL sensitive to the complexity of the question? We\nexamined how the performance of PALandCOTchange as\nthe complexity of the input question grows, measured as the\nnumber of objects in the question of COLORED OBJECTS .\nAs shown in Figure 6, PALis superior COTacross all input\nlengths. As the number of objects in the question increases,\nCOT’s accuracy is unstable and drops, while PALremains\nconsistently close to 100%. More analysis on the token-level\npredictions can be found in Appendix G.\x0cPAL: Program-aided Language Models 7\ncode-cushman-001 code-davinci-001 code-davinci-002020406080\n21.731.872.0\n19.126.060.1\n13.6%22.3%19.8%Solve ratePAL\nCOT\nRelative Improvement\nFigure 7: PALwith different models on GSM8K: though\nthe absolute accuracies with code-cushman-001\nandcode-davinci-001 are lower than\ncode-davinci-002 , the relative improvement of\nPAL over C OT is consistent across models.text-davinci-001 text-davinci-002 text-davinci-003020406080\n26.546.965.3\n8.665.869.8 COT PAL\nFigure 8: PALwith NL LMs on GSM 8K: though\nCOToutperforms PALwithtext-davinci-001 , once\nthe base LM is sufﬁciently strong, PALis beneﬁcial\nwithtext-davinci-002 andtext-davinci-003\nas well. That is, P AL is not limited to code-LMs only.\n6. Analysis\nDoes P AL work with weaker LMs? In all our experi-\nments in Section 5, PALused thecode-davinci-002\nmodel; but can PALwork with weaker models of code? We\ncompared PALwith COTwhen both prompting approaches\nuse the same weaker base LMs code-cushman-001\nandcode-davinci-001 . As shown in Figure 7, even\nthough the absolute accuracies of code-cushman-001\nandcode-davinci-001 are lower, the relative improve-\nment of PALover COTremains consistent across models.\nThis shows that PALcan ', '56.0 75.1\nBERT BASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\nBERT LARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\nTable 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard ).\nThe number below each task denotes the number of training examples. The “Average” column is slightly different\nthan the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8BERT and OpenAI GPT are single-\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\nWe use a batch size of 32 and ﬁne-tune for 3\nepochs over the data for all GLUE tasks. For each\ntask, we selected the best ﬁne-tuning learning rate\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\nAdditionally, for BERT LARGE we found that ﬁne-\ntuning was sometimes unstable on small datasets,\nso we ran several random restarts and selected the\nbest model on the Dev set. With random restarts,\nwe use the same pre-trained checkpoint but per-\nform different ﬁne-tuning data shufﬂing and clas-\nsiﬁer layer initialization.9\nResults are presented in Table 1. Both\nBERT BASE and BERT LARGE outperform all sys-\ntems on all tasks by a substantial margin, obtaining\n4.5% and 7.0% respective average accuracy im-\nprovement over the prior state of the art. Note that\nBERT BASE and OpenAI GPT are nearly identical\nin terms of model architecture apart from the at-\ntention masking. For the largest and most widely\nreported GLUE task, MNLI, BERT obtains a 4.6%\nabsolute accuracy improvement. On the ofﬁcial\nGLUE leaderboard10, BERT LARGE obtains a score\nof ']"," PAL was evaluated across 13 tasks, including mathematical, symbolic, and algorithmic reasoning from BIG-Bench Hard and other benchmarks, consistently outperforming larger models and setting new accuracy standards across all evaluated tasks.","The specific tasks and benchmarks used to evaluate PAL's performance were symbolic reasoning datasets, algorithmic datasets, colored objects dataset, and GSM8K dataset. The results showed that PAL achieved a higher accuracy than chain-of-thought and outperformed other models such as COT and DIRECT in various tasks and benchmarks.",1.0,1.0,0.8032016754150391
How does the inclusion of specific metadata in CodeNet facilitate a wide range of code analysis tasks?,"['types of errors,\naccordingly labeled. The submissions are in many different languages.\nCode Samples. Each code sample is a single ﬁle and includes inputting the test cases and printing out\nthe computed results. The ﬁle name uses standard extensions that denote the programming language,\ne.g.,.pyfor Python. The majority of code samples contain only one function, although submissions\nto more complex problems might have several functions.\n2\x0c(a) Languages (b) Status\nFigure 1: Percentage of submissions per language (left) and per status (right).\nMetadata. The metadata enables data queries and selections among the large collection of problems,\nlanguages, and source ﬁles. The metadata is organized in a two level hierarchy. The ﬁrst is the\ndataset level, which describes all problems. The second is the problem level, which details all the\nsubmissions to a single problem. Metadata and data are separated in the dataset structure.\nAt the dataset level, a single CSV ﬁle lists all problems and their origins, along with the CPU time\nand memory limits set for them. Additionally, every problem has an HTML ﬁle with a detailed\ndescription of the problem, the requirements and constraints, and the IO examples.\nAt the problem level, every problem has a CSV ﬁle. The metadata for each submission is summarized\nin Table 2 below, which lists the ﬁelds contained in each CSV ﬁle as well as the corresponding\ndescriptions.\n2.1 How to read the CodeNet dataset\nThe data and metadata are organized in a rigorous directory structure. The top level Project_CodeNet\ndirectory contains several sub-directories: data ,metadata ,problem_descriptions , and\nderived . The code samples or submissions reside under the data directory. The data directory\nis organized as (problem_id)/(language)/(submission) , so the ', 'page 143–153, New\nYork, NY , USA, 2019. Association for Computing Machinery.\n[25] Wikipedia. Jaccard index — Wikipedia, the free encyclopedia. https://en.wikipedia.\norg/wiki/Jaccard_index , 2020.\n[26] Terence Parr. The Deﬁnitive ANTLR 4 Reference . Pragmatic Bookshelf, 2nd edition, 2013.\n[27] Sifei Luan, Di Yang, Celeste Barnaby, Koushik Sen, and Satish Chandra. Aroma: code recom-\nmendation via structural code search. Proceedings of the ACM on Programming Languages ,\n3(OOPSLA):1–28, Oct 2019.\n[28] IBM T.J. Watson Research Center. Wala. https://github.com/wala/WALA , 2021.\n[29] Forbes on codenet. https://www.forbes.com/sites/moorinsights/2021/06/04/ibm-\ncodenet-artificial-intelligence-that-can-program-computers-and-solve-a-\n100-billion-legacy-code-problem/?sh=343813636cdc .\n[30] Venturebeat on codenet. https://venturebeat.com/2021/05/10/ibms-codenet-\ndataset-aims-to-train-ai-to-tackle-programming-challenges/ .\n[31] Zdnet on codenet. https://www.zdnet.com/article/ibm-launches-autosql-watson-\norchestrate-codenet-enterprise-ai-tools-at-think/ .\n[32] Project codenet repository. https://github.com/IBM/Project_CodeNet .\n[33] Luca Buratti, Saurabh Pujar, Mihaela Bornea, Scott McCarley, Yunhui Zheng, Gaetano Rossiello,\nAlessandro Morari, Jim Laredo, Veronika Thost, Yufan Zhuang, and Giacomo Domeniconi.\nExploring software naturalness through neural language models, 2020.\n[34] Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional\nnetworks. In ICLR , 2017.\n[35] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\nnetworks? In ICLR , 2019.\n[36] Veronika Thost and Jie Chen. Directed acyclic graph neural networks. In ICLR , 2021.\n[37] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B.\nClement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,\nMichele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng,\nShengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code\nunderstanding and generation. CoRR , abs/2102.04664, 2021.\n[38] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks\nfor semi-supervised learning, 2018.\n[39] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele\nCatasta, and ', 'for source code exist, with many targeting one or a small number of\ntasks. Such tasks include clone detection, vulnerability detection [ 10,11], cloze test [ 12], code\ncompletion [ 13,14], code repair [ 15], code-to-code translation, natural language code search [ 16],\ntext-to-code generation [ 17], and code summarization [ 16]. A detailed discussion of several of these\ntasks and their respective datasets is available in CodeXGLUE [ 18], which is a collection of existing\ndatasets. CodeNet, on the other hand, is a new dataset curated from scratch, that aims to support a\nbroad set of use cases. Popular datasets of a similar kind are POJ-104 [ 19] (which is incorporated as\npart of CodeXGLUE as well) and GCJ [ 20] (derived from Google Code Jam). We compare CodeNet\nto these datasets in the following.\n3.1 POJ-104\nPOJ-104 was collected from a pedagogical online judge system. The code samples are submissions\nto 104 programming problems. With 500 submissions to each problem, there is a total of 52,000 code\nsamples in the dataset. This dataset has been used by many authors for code classiﬁcation [ 19] and\ncode similarity [21].\nPOJ-104 is faced with several limitations.\n1.The code samples are in C and C++, but the two languages are not distinguished. Although they are\nclosely related, mixing them leads to parsing errors and a reduction of useful code samples [21].\n2.Useful metadata such as the results of the judging system (acceptance, error types etc.) are missing.\nTherefore, for certain applications where compilabilty or code correctness is important, additional\npre-processing efforts are needed and useful code samples are reduced [ 21]. The dataset does\nnot contain the ']","Rich metadata including problem descriptions, submission outcomes, and programming languages enable tasks like bug fixing, performance improvement, and understanding coding patterns.","The inclusion of specific metadata in CodeNet facilitates a wide range of code analysis tasks by providing detailed information about the submissions to each problem. This metadata includes fields that describe the submissions, such as CPU time, memory limits, and judging system results (such as acceptance or error types). This information allows for more in-depth analysis of the code samples, enabling tasks such as code classification, similarity detection, compilability assessment, and code correctness verification. Without this specific metadata, such code analysis tasks would be more challenging and less accurate.",1.0,1.0,0.6278479099273682
"What types of tasks are included in SuperGLUE, and how do they enhance the benchmark's complexity?","['advances on GLUE, but\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\nC Human Performance Estimation\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\nannotation. For both steps and all tasks, the average pay rate is $23.75/hr.7\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\nare asked to annotate up to 30 examples from the development set. After answering each example,\nworkers are also asked to check their work against the provided ground truth label. After the training\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\nperformance at, or above the median performance across all workers during training.\n7This estimate is taken from https://turkerview.com .\n16\x0cIn the annotation phase, workers are provided with the same instructions as the training phase, and\nare linked to the same FAQ page. The instructions for all tasks are provided in Appendix C. For the\nannotation phase we randomly sample 100 examples from the task’s test set, with the exception of\nWSC where we annotate the full test set. For each example, we collect annotations from ﬁve workers\nand take a majority vote to estimate human performance. For additional details, see Appendix C.3.\nC.1 Training Phase Instructions\nIn the training step, we provide workers with brief instructions about the training phase. An example\nof these instructions is given Table 5. These training instructions ', 'corresponds well to human\njudgments of output quality. Some text generation tasks fail to meet this criteria due to issues with\nautomatic metrics like ROUGE and BLEU (Callison-Burch et al., 2006; Liu et al., 2016, i.a.).\n4\x0cPublic data: We require that tasks have existing public training data in order to minimize the risks\ninvolved in newly-created datasets. We also prefer tasks for which we have access to (or could create)\na test set with private labels.\nTask format: We prefer tasks that had relatively simple input and output formats, to avoid incentiviz-\ning the users of the benchmark to create complex task-speciﬁc model architectures. Still, while GLUE\nis restricted to tasks involving single sentence or sentence pair inputs, for SuperGLUE we expand\nthe scope to consider tasks with longer inputs. This yields a set of tasks that requires understanding\nindividual tokens in context, complete sentences, inter-sentence relations, and entire paragraphs.\nLicense: Task data must be available under licences that allow use and redistribution for research\npurposes.\nTo identify possible tasks for SuperGLUE, we disseminated a public call for task proposals to the\nNLP community, and received approximately 30 proposals. We ﬁltered these proposals according\nto our criteria. Many proposals were not suitable due to licensing issues, complex formats, and\ninsufﬁcient headroom; we provide examples of such tasks in Appendix D. For each of the remaining\ntasks, we ran a BERT-based baseline and a human baseline, and ﬁltered out tasks which were either\ntoo challenging for humans without extensive training or too easy for our machine baselines.\n3.2 Selected Tasks\nFollowing this process, we arrived at eight tasks to use in SuperGLUE. See Tables 1 and 2 ', 'et al., 2014). Finally, we list the best known\nresult on each task as of May 2019, except on tasks which we recast (WSC), resplit (CB), or achieve\n6For ReCoRD, we predict the entity that has the highest F1 with the other entity options.\n8\x0cthe best known result (WiC). The outside results for COPA, MultiRC, and RTE are from Sap et al.\n(2019), Trivedi et al. (2019), and Liu et al. (2019d) respectively.\nHuman Performance Pilehvar and Camacho-Collados (2019), Khashabi et al. (2018), Nangia and\nBowman (2019), and Zhang et al. (2018) respectively provide estimates for human performance\non WiC, MultiRC, RTE, and ReCoRD. For the remaining tasks, including the diagnostic set, we\nestimate human performance by hiring crowdworker annotators through Amazon’s Mechanical Turk\nplatform to reannotate a sample of each test set. We follow a two step procedure where a crowd\nworker completes a short training phase before proceeding to the annotation phase, modeled after the\nmethod used by Nangia and Bowman (2019) for GLUE. See Appendix C for details.\n5.2 Results\nTable 3 shows results for all baselines. The most frequent class and CBOW baselines do not perform\nwell overall, achieving near chance performance for several of the tasks. Using BERT increases\nthe average SuperGLUE score by 25 points, attaining signiﬁcant gains on all of the benchmark\ntasks, particularly MultiRC, ReCoRD, and RTE. On WSC, BERT actually performs worse than\nthe simple baselines, likely due to the small size of the dataset and the lack of data augmentation.\nUsing MultiNLI as an additional source of supervision for BoolQ, CB, and RTE leads to a 2-5 point\nimprovement on all tasks. Using SWAG as ']","SuperGLUE includes eight diverse tasks such as Boolean Questions (BoolQ), Choice of Plausible Alternatives (COPA), and Winograd Schema Challenge (WSC), enhancing complexity through a mix of question answering, natural language inference, coreference resolution, and word sense disambiguation tasks.","The tasks included in SuperGLUE are those that require understanding individual tokens in context, complete sentences, inter-sentence relations, and entire paragraphs. This enhances the benchmark's complexity by expanding beyond tasks involving single sentence or sentence pair inputs, thus challenging models to exhibit a deeper understanding of language and context.",0.75,1.0,0.7157787084579468
"What criteria were used to select tasks for inclusion in SuperGLUE, and how do these criteria benefit the benchmark?","['advances on GLUE, but\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\nC Human Performance Estimation\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\nannotation. For both steps and all tasks, the average pay rate is $23.75/hr.7\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\nare asked to annotate up to 30 examples from the development set. After answering each example,\nworkers are also asked to check their work against the provided ground truth label. After the training\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\nperformance at, or above the median performance across all workers during training.\n7This estimate is taken from https://turkerview.com .\n16\x0cIn the annotation phase, workers are provided with the same instructions as the training phase, and\nare linked to the same FAQ page. The instructions for all tasks are provided in Appendix C. For the\nannotation phase we randomly sample 100 examples from the task’s test set, with the exception of\nWSC where we annotate the full test set. For each example, we collect annotations from ﬁve workers\nand take a majority vote to estimate human performance. For additional details, see Appendix C.3.\nC.1 Training Phase Instructions\nIn the training step, we provide workers with brief instructions about the training phase. An example\nof these instructions is given Table 5. These training instructions ', 'et al., 2014). Finally, we list the best known\nresult on each task as of May 2019, except on tasks which we recast (WSC), resplit (CB), or achieve\n6For ReCoRD, we predict the entity that has the highest F1 with the other entity options.\n8\x0cthe best known result (WiC). The outside results for COPA, MultiRC, and RTE are from Sap et al.\n(2019), Trivedi et al. (2019), and Liu et al. (2019d) respectively.\nHuman Performance Pilehvar and Camacho-Collados (2019), Khashabi et al. (2018), Nangia and\nBowman (2019), and Zhang et al. (2018) respectively provide estimates for human performance\non WiC, MultiRC, RTE, and ReCoRD. For the remaining tasks, including the diagnostic set, we\nestimate human performance by hiring crowdworker annotators through Amazon’s Mechanical Turk\nplatform to reannotate a sample of each test set. We follow a two step procedure where a crowd\nworker completes a short training phase before proceeding to the annotation phase, modeled after the\nmethod used by Nangia and Bowman (2019) for GLUE. See Appendix C for details.\n5.2 Results\nTable 3 shows results for all baselines. The most frequent class and CBOW baselines do not perform\nwell overall, achieving near chance performance for several of the tasks. Using BERT increases\nthe average SuperGLUE score by 25 points, attaining signiﬁcant gains on all of the benchmark\ntasks, particularly MultiRC, ReCoRD, and RTE. On WSC, BERT actually performs worse than\nthe simple baselines, likely due to the small size of the dataset and the lack of data augmentation.\nUsing MultiNLI as an additional source of supervision for BoolQ, CB, and RTE leads to a 2-5 point\nimprovement on all tasks. Using SWAG as ', 'corresponds well to human\njudgments of output quality. Some text generation tasks fail to meet this criteria due to issues with\nautomatic metrics like ROUGE and BLEU (Callison-Burch et al., 2006; Liu et al., 2016, i.a.).\n4\x0cPublic data: We require that tasks have existing public training data in order to minimize the risks\ninvolved in newly-created datasets. We also prefer tasks for which we have access to (or could create)\na test set with private labels.\nTask format: We prefer tasks that had relatively simple input and output formats, to avoid incentiviz-\ning the users of the benchmark to create complex task-speciﬁc model architectures. Still, while GLUE\nis restricted to tasks involving single sentence or sentence pair inputs, for SuperGLUE we expand\nthe scope to consider tasks with longer inputs. This yields a set of tasks that requires understanding\nindividual tokens in context, complete sentences, inter-sentence relations, and entire paragraphs.\nLicense: Task data must be available under licences that allow use and redistribution for research\npurposes.\nTo identify possible tasks for SuperGLUE, we disseminated a public call for task proposals to the\nNLP community, and received approximately 30 proposals. We ﬁltered these proposals according\nto our criteria. Many proposals were not suitable due to licensing issues, complex formats, and\ninsufﬁcient headroom; we provide examples of such tasks in Appendix D. For each of the remaining\ntasks, we ran a BERT-based baseline and a human baseline, and ﬁltered out tasks which were either\ntoo challenging for humans without extensive training or too easy for our machine baselines.\n3.2 Selected Tasks\nFollowing this process, we arrived at eight tasks to use in SuperGLUE. See Tables 1 and 2 ']","Tasks were selected based on difficulty, evaluability, substance, public data availability, simple formats, and licensing conditions, ensuring a challenging, diverse, and accessible benchmark for advancing general-purpose language understanding technologies.","License, task format, public data, validation criteria, minimum headroom, restrictions on complexity, alignment with human judgments of output quality, exclusion of tasks that are too challenging for humans without extensive training, exclusion of tasks that are too easy for machine baselines. These criteria ensure that tasks are suitable for research purposes, have simple input and output formats, are not overly complex, provide enough headroom for improvement, and align with human judgment of quality, thereby making the benchmark fair and meaningful.",0.5,1.0,0.48383039236068726
"What are the main components of GLM-130B's pre-training objective, and how do they contribute to its performance?","['from all of our generous partners, we\nfinally succeeded in making our pre-training algorithms runnable across all the platforms—frankly,\na surprising achievement for this project. The timeline of GLM-130B in Figure 21 covers most of\nthe issues we have encountered and addressed as of this writing.\nOn April 26th, we received a generous computing sponsorship from Zhipu.AI — an AI startup that\naims to teach machines to think like humans. After another week of testing, we finally kicked off\nthe training of the GLM-130B model on its 96 A100 (40G * 8) servers on May 6th. Additionally,\nZhipu.AI also sent a team to help evaluate the pre-trained model and build a demonstration website.\nThe training period spanned two months, during which we began developing a toolkit to allow GLM-\n130B’s inference in low-resource setting with swapping technique and quantization. Though it is\nalready the most accessible model of its scale, together with our partner from Tsinghua NLP, we have\nbeen exploring the limit of popularized hardware platforms, which would truly make the 100B-scale\nmodel accessible to as many people as possible. To date, we managed to reach the INT4 weight\nquantization for GLM-130B. Importantly, the INT4 version of GLM-130B without post training\n16This section is largely extracted and updated from the blog introduction of GLM-130B at http://keg.\ncs.tsinghua.edu.cn/glm-130b/ (Posted date: August 4, 2022).\n53\x0cPublished as a conference paper at ICLR 2023\n2021.12•The “千亿”(100B) project towardsan opendensepre-trained GLM at 100B scale isconceived•Survey pre-trainingstrategies of existing models of similar scale,suchasGPT-3,Gopher=> Limited public infoabout how they were trainedand issues they met•SearchforpossibleGPUclusters&sponsors2022.1•TesttheperformanceofFP16/FP32at100Bscaleononetestingcluster•UnexpectedexcessivememoryusageinGLM=> Torchisbetterwithfixedlengthinputsequences•InabilitytoconvergeandtrytricksfromCogViewandViT=> UseSandwich-LN•Frequentrandomhardwarefailures=> HavetorunHCPGtestbeforeeachrun2022.2•Veryslowtrainingspeedthanpreviouslycalculated=>Optimizekernelsandfuseoperators=>Findtheinputshapeiscriticaltokernelperformance•Collectpre-trainingcorporaandtokenize=>Useicetk:thesentencepieceissettotheunigrammode•Debugthe3Dpipelineparallelinthenewly-releasedMegatronandDeepSpeed2022.3•Itcan’trecoverperfectlyfromcheckpoints=>Ourcustomizeddataloaderdonotsaveitsstateseedproperlyindistributedtraining•Thememoryperprocessoristoosmall=>Requiretoomanypipelinestages=>Batchsizeistoolarge(upto12,000)=>Harmthemodel’sconvergency•Itcan’tlaunchmorethan2,000computingnodes=>Overcomethisandsupport6,000-nodetrainingbytuningLinuxkernelTCPparameters•Collectdataformulti-taskinstructionpre-training•Receiveopportunitiestotesttrainingsonseveralotherclusters•Veryslowtrainingspeedthanexpected=>Theunderlyingelement-wiseoperatorsdon’tsupportfastcomputationonlarge-dimensionvectors.2022.4•OptimizeA100kernel’scomputingefficiency=>A100kernelsprefersquare-shapedinputs,andseq_len=2,048isoptimalforourhidden-statedimension(12,288)•Inabilitytoconvergeduetolargegradientnorms(170+)ofinputembeddings=>Tryembeddingnormandgradientshrink,whichturnouttobealmostequivalent•Naïvepost-LNorpre-LNdisconvergesafterseveralthousandsofsteps=>TrySandwich-LNwithPB-Relax•Itstilldisconvergesafteroneweek’strial=>Thedataloaderstateseedsarenotunifiedfordifferentpipelinestages,resultinginamismatchofinputdataandlabels.•Testtwopositionalencodings:RoPEandAlibi=>Alibicanbeslowerasitrequireselement-wisemanipulationonattentionmatrices---changingnum_heads*2,048* 2,048scalarsperlayer•TestGeGLUandGAU=>GAUconvergesfasterwithrelativelypoorperformanceonfine-tunedSuperGLUE•AbnormalGPUmemoryusageofnewly-addedfunctionsandclasses=>DeepSpeedhardcodesthefunctionnamesforcheckpointactivation•DecidetotrainGLMwith130billionparameters=>allowinferenceonaDGX-A10040Gnode2022.5-6•ImplementaRoPEcudaoperatorinC++=>Seeunexpectedprecisionerrorsandfinallyhaveitabandoned•Sandwich-LNstilldisconverges=>1)Reducinglearningratedoesnothelp;2)UsingHingecross-entropybecomesslowerandharmsperformance;3)ShiftingtoDeepNormstilldisconverges•UseFP32insoftmaxofattention=>Success•FindPB-RelaxunnecessaryforFP32softmax=>Italsoslowsdowntrainingasitneedstomanipulatethewholeattentionscorematrices•Experiencefewspikesinlatertraining=>1)Reducegradientshrinkfactorfrom1to0.1:useful;2)Reducethelearningrate:sometimesuseful;3)Jumpthenoisydatabatches:sometimesuseful•Findamistakeinmulti-taskdataaftertrainingfor20,000steps=>Usethecorrectdatabutitdoesnotforget2022.6-7•AdaptthepipelineparallelcheckpointstoordinaryparallelcheckpointsforefficientinferenceonasingleA100•Workonevaluationscriptsondatasets:MMLU,Big-bench,CLUE,SuperCLUE,etc.•ImplementP-TuningandP-Tuningv2forparameter-efficienttuningonGLM-130BfortuningonSuperGLUE•WorkwithBMInfonadaptingGLM-130BtoperforminferenceonasingleV100or3090=>Usepipeline-styleasynchronousswappingbetweenmainmemoryandGPUmemory•Trytofine-tuneGLM-130BwithfewerA100nodes(i.e.,12-16nodes)=>Pipeline-stylefailsduetotoomanypipelinestages=>Findthatdataparallelcannotbeintroducedforfine-tuning=>Use32-waymodelparallelforfine-tuningwithreasonableperformance\nMajorIssuesEncounteredforTrainingGLM-130B\nhttps://github.com/THUDM/GLM-130BTsinghuaKEG\nFigure 21: The timeline of major issues that training GLM-130B encountered and ', 'GLM-130B training\nConfiguration Key Value\nadam_beta1 0.9\nadam_beta2 0.95\nadam_eps 1e-08\naggregated_samples_per_sequence 4\nattention_dropout 0.1\nattention_softmax_in_fp32 True\naverage_block_length 3\nbias_dropout_fusion True\ncheckpoint_activations True\ncheckpoint_in_cpu False\ncheckpoint_num_layers 1\nclip_grad 1.0\ncontigious_checkpointing False\ncpu_optimizer False\ndata_parallel_size 24\ndeepnorm True\ndistributed_backend nccl\neval_interval 1000\neval_iters 3\nffn_hidden_size 32768\nfp16 True\nglobal_batch_size 4224\nglu_activation geglu\ngpt_prob 0.7\nhidden_dropout 0.1\nhidden_size 12288\nhysteresis 2\ninit_method_std 0.0052\ninit_method_xavier_uniform False\ninitial_loss_scale 65536\nlayernorm_epsilon 1E-05\nlearnable_rotary_embedding False\nlength_per_sample 2000\nlog_interval 1\nloss_scale 0\nloss_scale_window 2000\nlr 8e-05\nlr_decay_iters None\nlr_decay_samples 197753905\nlr_decay_style cosine\nlr_warmup_samples 1098632\nmake_vocab_size_divisible_by 768\nmask_prob 0.15\nmasked_softmax_fusion True\nmicro_batch_size 1\nmin_gmask_ratio 0.2\nmin_loss_scale 1.0\nmin_lr 8e-06\nmultitask_ratio 0.05\nnum_attention_heads 96\nnum_layers 70\nonnx_safe None\noptimizer adam\npartition_activations True\npipeline_model_parallel_size 8\nposition_embedding_type rotary\nrampup_batch_size 192, 24, 5493164\nsave_interval 250\nseed 1234\nseq_length 2048\nshort_seq_prob 0.02\nshrink_embedding_gradient_alpha 0.1\nsingle_span_prob 0.02\nsplit 949,50,1\ntensor_model_parallel_size 4\ntokenizer_type IceTokenizer\nweight_decay 0.1\nzero_contigious_gradients False\nzero_reduce_bucket_size 500000000\nzero_reduce_scatter False\nzero_stage 1\nzero-optimization.allgather_bucket_size 500000000\ntokenizer_type IceTokenizer\nweight_decay 0.1\nworld_size 768\nzero_contigious_gradients FALSE\nzero_reduce_bucket_size 500000000\nzero_reduce_scatter FALSE\nzero_stage 1\nzero-optimization.allgather_bucket_size 500000000\n48\x0cPublished as a conference paper at ICLR 2023\nTable 12: The 74 datasets involved in Multi-task Instruction Pre-training (MIP). Datasets from T0-\nPromptSource (Sanh et al., 2022; Bach et al., 2022) are named in their Hugging Face datasets iden-\ntifiers. Datasets from DeepStruct (Wang et al., 2022a) are described in Appendix C.2.\nTask Dataset Task Dataset\nCoreference Resolution super_glue/wsc.fixed Multi-choice QA cos_e/v1.11\nCoreference Resolution winogrande/winogrande_xl Multi-choice QA cosmos_qa\nNatural Language Inference super_glue/cb Multi-choice QA dream\nNatural Language Inference super_glue/rte Multi-choice QA openbookqa/main\nNatural Language Inference anli Multi-choice QA qasc\nParaphrase Identification glue/mrpc Multi-choice QA quail\nParaphrase Identification glue/qqp Multi-choice QA quarel\nParaphrase Identification paws/labeled_final Multi-choice QA quartz\nClosed-Book QA ai2_arc/ARC_Challenge Multi-choice QA race/high\nClosed-Book QA ai2_arc/ARC_Easy Multi-choice QA race/middle\nClosed-Book QA kilt_tasks/hoptpotqa Multi-choice QA sciq\nClosed-Book QA trivia_qa/unfiltered Multi-choice QA social_i_qa\nClosed-Book QA web_questions Multi-choice QA super_glue/boolq\nClosed-Book QA wiki_qa Multi-choice QA super_glue/multirc\nExtractive QA adversarial_qa/dbidaf Multi-choice QA wiki_hop/original\nExtractive QA adversarial_qa/dbert Multi-choice QA wiqa\nExtractive QA adversarial_qa/droberta Multi-choice QA piqa\nExtractive QA duorc/SelfRC Topic Classification ag_news\nExtractive QA duorc/ParaphraseRC Topic Classification dbpedia_14\nExtractive QA ropes Topic Classification trec\nExtractive QA squad_v2 Word Sense Disambiguation super_glue/wic\nExtractive QA super_glue/record Dialogue State Tracking multiwoz_2.1\nExtractive QA quoref Event Extraction ', '17.43% 37.88% 38.04% 34.83%\nAbsmax INT4, row-wise 21.37% 35.80% 40.95% 46.75% NaN\nZeropoint INT4, col-wise 11.51% 26.51% 41.65% 46.63% 48.26%\nZeropoint INT4, row-wise 24.95% 33.05% 43.63% 49.41% NaN\nGLM-110M GLM-335M GLM-2B GLM-10B GLM-130B\nOriginal 29.36% 48.51% 68.19% 72.35% 80.21%\nAbsmax INT8, row-wise 29.25% 48.69% 68.12% 72.37% 80.21%\nAbsmax INT4, row-wise 3.26% 38.25% 62.62% 71.03% 79.47%\nZeropoint INT4, row-wise 5.45% 42.64% 64.74% 70.50% 80.63%\nLAMBADA MMLU WiC ReCoRD Hellaswag WSC BoolQ ANLI R120406080\n67.3\n26.351.765.4\n27.363.5 64.1\n35.072.7\n33.756.166.4\n27.763.571.2\n35.674.8\n34.552.5 50.7\n27.367.378.3\n40.0Model\nGLM (uni)\nGLM (bi)\nGLM + MIP (bi)\nFigure 14: Contribution attribution analysis on GLM objective and MIP training. We take GLM-\n10B (English only) as an example in the ablation. Generally, GLM objective’s bidirectional attention\naccounts for 70% of the improvements, while MIP’s major contribution lies in text similarity tasks.\nB.8.2 W EIGHT DISTRIBUTION ANALYSIS\nTo achieve INT4 weight quantization, we analyze the weight value distribution of major linear layers\nin GLM-130B and a counterpart BLOOM-176B in a histogram (Cf. Figure 15). The horizontal axis\ndenotes the weight value, and the vertical axis denotes the number of weights of such value in\nlog scale. As we can see, it is majorly the w2linear layers in BLOOM-176B that present skewed\ndistributions, which would hinder the symmetrical quantization. On the contrary, GLM-130B’s w2\nis well-shaped without many outliers and skewed distribution, and thus paces the way for its INT4\nquantization with little performance loss.\nB.9 A BLATION ON CONTRIBUTION ATTRIBUTION\nWe analyze the contribution attribution of techniques leveraged in GLM-130B. A series of ablation\nstudies have been presented in the paper, and for the convenience of reading, they were originally\nscattered around the whole passage. Here we summarize them here into the following list for read-\ners’ reference:\n•Ablation on ordinary PostLN ']","GLM-130B's pre-training includes self-supervised blank infilling for 95% of tokens and multi-task instruction pre-training (MIP) for 5% of tokens, enhancing downstream zero-shot performance.","The main components of GLM-130B's pre-training objective are bidirectional attention from the GLM objective, with text similarity tasks from the MIP training. Bidirectional attention contributes to 70% of the improvements, while text similarity tasks contribute significantly to the overall performance.",0.5,1.0,0.6443949341773987
How does GLM-130B address ethical concerns and biases compared to its counterparts?,"['to agree that they will not use it\nfor any deeds that may be harmful to society and human beings.\nAdditionally, from a technical perspective, we argue that we must also understand LLMs’ toxic\nand biased behaviors and ultimately eliminate them. This aligns with our commitment to “LLM\nInclusivity”, as it is necessary to include more people in the open-sourced LLM research to facilitate\nthe process. Moreover, if an LLM is shown to be good at identifying toxic and biased content,\ntechniques such as self-diagnoses (Schick et al., 2021) can help to reduce the harmful generation\nin a self-consistent post-processing procedure. Therefore, as an initial step, we evaluate GLM-\n130B over a variety of related benchmarks to shed light on the challenging topic. Despite their\nlimitations (Blodgett et al., 2021; Jacobs & Wallach, 2021) which should be addressed in future\nwork, they still serve as a good start to arouse the community’s awareness of the problem.\nA.1 B IASMEASUREMENT : CROW S-P AIRSTable 5: CrowS-Pairs (Nangia et al., 2020) Bias\nMeasurement. The lower scores the better.\nCategory GPT-3 OPT-175B GLM-130B\nGender 62.6 65.7 55.7\nReligion 73.3 68.6 73.3\nRace/Color 64.7 68.6 58.5\nSexual orientation 76.2 78.6 60.7\nAge 64.4 67.8 63.2\nNationality 61.6 62.9 64.1\nDisability 76.7 76.7 71.6\nPhysical appearance 74.6 76.2 74.6\nSocioeconomic status 73.8 76.2 70.9\nOverall 67.2 69.5 65.8CrowS-Pairs (Nangia et al., 2020), or namely\nCrowdsourced Stereotype Pairs benchmark, is\nwidely used for measuring biases for masked\nlanguage models. It collects 1508 examples with\nnine different conventional biases and adopts a\nprobing-based approach to compare the pseudo-\nlog-likelihood of a pair of stereotypical and anti-\nstereotypical sentences. Since GLM-130B is\npre-trained with autoregressive blanking infill-\ning, CrowS-Pairs evaluation is directly appli-\ncable. We compare the ', 'decision-\nmaking and information-seeking activities. To prevent\nthis, it is essential to keep the training data current and to\ncontinuously monitor and update the model as new data\nbecomes available, so that the language model provides\nusers with the most accurate and relevant information.\nd) Toxicity: The ethical considerations related to\ntoxicity in the development and deployment of language\nmodels are of utmost importance. Toxicity refers to the\nmodel’s ability to generate or understand harmful or\noffensive content.\nOffensive Language: One form of toxicity that may\narise is the presence of offensive language in the training\x0cdata. This can result in the model generating or under-\nstanding offensive or harmful content when interacting\nwith users [44]. For instance, if a language model is\ntrained on data that includes racist or sexist language,\nit may generate or understand racist or sexist content\nwhen interacting with users. To mitigate this, it is crucial\nto ensure that the training data does not contain any\noffensive or hurtful language, and to actively identify and\nremove any offensive or harmful information that may be\npresent in the data.\nPornography: Another form of toxicity that may arise is\nthe presence of pornographic content in the training data.\nThis can lead to the model generating or understanding\npornographic content when interacting with users [45]. To\nmitigate this, it is crucial to guarantee that the training\ndata is free of pornographic content and to actively\nidentify and remove any pornographic content that may\nbe present in the data. Additionally, it is essential to\nimplement the necessary security measures to prevent\nimproper use of the model.\nIII. Diagnosing AI ethics Of ChatGPT\nThe objective of this research is to evaluate ChatGPT\nwith respect to four critical ethical considerations: Bias,\nReliability, Robustness, ', '. . . . . . . . . . 52\nE.4 Project Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nE.5 Computation Sponsor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nF A Brief History of GLM-130B 53\nG Broader Impact 55\nG.1 Impact on AI Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\nG.2 Impact on Individual Developers and Small Companies . . . . . . . . . . . . . . 55\n20\x0cPublished as a conference paper at ICLR 2023\nG.3 Social Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\nH Environmental Impact 56\nA E THICS : EVALUATION ON BIASES AND TOXICITY\nAlbeit LLMs’ strong abilities in language and beyond, which could bring substantial welfare to\nhuman beings, they can potentially produce toxic and illegal contents for evil use (Weidinger et al.,\n2021; Sheng et al., 2021; Dev et al., 2021; Bommasani et al., 2021). In GLM-130B, before granting\nmodel weight to applicants, in the model license we demand them ']","Through comprehensive ethical evaluations, GLM-130B exhibits significantly less bias and toxicity generation than similar-scale models, highlighting its consideration for ethical AI development.","GLM-130B addresses ethical concerns and biases by evaluating biases through the CrowS-Pairs benchmark, monitoring and updating the model with current data to provide accurate information, and mitigating toxicity by ensuring training data is free of offensive language and pornography. Additionally, GLM-130B aims to identify and eliminate toxic and biased behaviors to ultimately promote inclusivity and awareness within the community.",1.0,1.0,0.7094229459762573
How does Megatron-LM's implementation ensure training stability for extremely large transformer models?,"['and conﬁgurations listed in Table 2. The 355M\nmodel is equivalent in size and conﬁguration of BERT-Large\nmodel (Devlin et al., 2018). The 2.5B model is bigger than\nthe previous largest GPT-2 model, and the 8.3B model is\nlarger than any left-to-right transformer language model\never trained, to the best of our knowledge. To train and eval-\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 2. Model conﬁgurations used for GPT-2.\nHidden Time\nParameter Layers Hidden Attn Size Total per\nCount Size Heads per GPUs Epoch\nHead (days)\n355M 24 1024 16 64 64 0.86\n2.5B 54 1920 20 96 128 2.27\n8.3B 72 3072 24 128 512 2.10\nTable 3. Zero-shot results. SOTA are from (Khandelwal et al.,\n2019) for Wikitext103 and (Radford et al., 2019) for LAMBADA.\nModel Wikitext103 LAMBADA\nPerplexity↓ Accuracy↑\n355M 19.31 45.18%\n2.5B 12.76 61.73%\n8.3B 10.81 66.51%\nPrevious SOTA 15.79 63.24%\nuate our language models we use the procedure described in\nsection 4. Table 2 also lists the time it takes to advance one\nepoch which is equivalent to 68,507 iterations. For example,\nfor the 8.3B model on 512 GPUs, each epoch takes around\ntwo days. Compared to the conﬁgurations used for our scal-\ning studies in Table 1, the 2.5B model is the same, the 8.3B\nmodel has 24 attention heads instead of 32, and the 355M is\nmuch smaller than any seen previously while still using 64\nGPUs to train, leading to the much lower time per epoch.\nFigure 6 shows validation perpelixity as a function of num-\nber of iterations. As the model size increases, the validation\nperpelixity decreases and reaches a validation perplexity of\n9.27 for the 8.3B model. We report the zero-shot evaluation\nof the trained models on the ', 'is also largely impacted\nby the number of tokens it passes through (Hoffmann et al., 2022). Thus, given the computing\nusage constraint, there has to be a trade-off between efficiency and stability with regard to floating-\npoint (FP) formats: low-precision FP formats (e.g., 16-bit precision—FP16) improve computing\nefficiency but are prone to overflow and underflow errors, resulting in training collapses.\n(a) Gradient norm with EGS α = 0.1\n(b) EGS in 40B-scale testing\nFigure 4: EGS reduces gradi-\nent scale and variance to stabi-\nlize LLMs’ pre-training.Mixed-Precision. We follow the common practice of a mixed-\nprecision (Micikevicius et al., 2018) strategy (Apex O2), i.e., FP16\nfor forwards and backwards and FP32 for optimizer states and mas-\nter weights, to reduce the GPU memory usage and improve train-\ning efficiency. Similar to OPT-175B and BLOOM-176B (C.f. Fig-\nure 10 in Appendix), the training of GLM-130B faces frequent loss\nspikes resulted from this choice, which tends to become increas-\ningly frequent as the training goes on. The precision related spikes\nare often without clear reasons: some recover on their own; others\ncome with a portent of suddenly soaring gradient norm and even-\ntually a spike or even NaN in loss. OPT-175B attempted to fix by\nmanually skipping data and adjusting hyper-parameters; BLOOM-\n176B did so via the embedding norm technique (Dettmers et al.,\n2021). We spent months to empirically investigate the spikes and\nrealize that a few issues emerge when transformers scale up:\nFirst, the transformer main branch’s value scale can be extremely\nlarge in deeper layers if using Pre-LN. This is addressed in GLM-\n130B by using DeepNorm based Post-LN (Cf. Section 2.1), which\nmakes the value scale always bounded.\nSecond, the attention scores grow ', 'major challenge for training\nLLMs (Zhang et al., 2022; Scao et al., 2022; Chowdhery et al., 2022) (Cf. Figure 10 in Appendix\nfor collapses in training several 100B-scale models). A proper choice of LNs can help stabilize\nthe training of LLMs. We experiment with existing practices, e.g., Pre-LN (Xiong et al., 2020),\n3\x0cPublished as a conference paper at ICLR 2023\nPost-LN (Ba et al., 2016), Sandwich-LN (Ding et al., 2021), which are unfortunately incapable of\nstabilizing our GLM-130B test runs (Cf. Figure 3 (a) and Appendix B.2 for details).\nOur search is later focused on Post-LN due to its favorable downstream results in preliminary ex-\nperiments though it does not stabilize GLM-130B. Fortunately, one of the attempts on Post-LN\ninitialized with the newly-proposed DeepNorm (Wang et al., 2022b) generates promising training\nstability. Specifically, given the number of GLM-130B’s layers N, we adopt DeepNorm (x) =\nLayerNorm (α·x+Network (x)), where α= (2N)1\n2, and apply the Xavier normal initialization\nwith the scaling factor of (2N)−1\n2toffn,v_proj andout_proj . Additionally, all bias terms\nare initialized to zero. Figure 3 shows it significantly benefits the training stability of GLM-130B.\nPositional Encoding and FFNs. We empirically test different options for positional encoding (PE)\nand FFN improvements in terms of both training stability and downstream performance (Cf. Ap-\npendix B.3 for details). For PEs in GLM-130B, we adopt Rotary Positional Encoding (RoPE, Su\net al. (2021)) rather than ALiBi (Press et al., 2021). To improve FFNs in Transformer, we pick GLU\nwith the GeLU (Hendrycks & Gimpel, 2016) activation as the replacement.\n2.2 GLM-130B’ SPRE-TRAINING SETUP\nInspired by recent works (Aribandi et al., 2022; Wei et al., 2022a; Sanh et al., 2022), the ']","Through techniques like embedding gradient shrink (EGS) and DeepNorm layer normalization, Megatron-LM stabilizes training of models with billions of parameters.","Megatron-LM's implementation ensures training stability for extremely large transformer models through techniques such as mixed-precision (FP16 for forwards and backwards, FP32 for optimizer states and master weights), using DeepNorm based Post-LN to address value scale issues in deeper layers, experimenting with different types of Layer Normalization (LN) techniques, focusing on Post-LN initialized with DeepNorm for stability, adopting Rotary Positional Encoding (RoPE) and using GeLU activation for FFNs improvements.",1.0,1.0,0.695324182510376
How does PAL's performance on the GSM8K benchmark compare to other advanced models?,"['times, but halfway through also\nsay quack” ).5. Results\n5.1. Math Results\nTable 1 shows the following results: across all tasks,\nPALusing Codex sets a new few-shot state-of-the-art top-\n1 decoding across all datasets, outperforming COTCodex,\nCOTPaLM-540 B, and COTMinerva 540B which was ﬁne-tuned\non explicit mathematical content.\nInterestingly, COTalso beneﬁts from Codex over PaLM-\n540 Bin some of the datasets such as ASDIV , but performs\nworse than PaLM-540 Bin others such as SVAMP . Yet,\nusing PALfurther improves the solve rate across all datasets.\nGSM-HARD On GSM -HARD (Table 1), the accuracy of\nDIRECT drops dramatically from 19.7% to 5.0% (a relative\ndrop of 74%), the accuracy of COTdrops from 65.6% to\n20.1% (a relative drop of almost 70%), while PALremains\nstable at 61.5%, dropping by only 14.3%. The results of\nCOTonGSM-HARD did not improve even when we replaced\nits prompts with prompts that include large numbers (Ap-\npendix B). This shows how PALprovides not only better\x0cPAL: Program-aided Language Models 6\nCOLORED OBJECT PENGUINS DATE REPEAT COPY OBJECT COUNTING\nDIRECT Codex 75.7 71.1 49.9 81.3 37.6\nCOTLaMDA-137B - - 26.8 - -\nCOTPaLM-540 B - 65.1 65.3 - -\nCOTCodex 86.3 79.2 64.8 68.8 73.0\nPALCodex 95.1 93.3 76.2 90.6 96.7\nTable 2: Solve rate on three symbolic reasoning datasets and two algorithmic datasets, In all datasets, PALachieves a much\nhigher accuracy than chain-of-thought. Results with closed models LaMDA-137B and PaLM-540B are included if available\nto public (Wei et al., 2022; Suzgun et al., 2022).\nresults on the standard benchmarks, but is also much more\nrobust . In fact, since PALofﬂoads the computation to the\nPython interpreter, any complex computation can be per-\nformed accurately given the correctly generated program.\nLarge Numbers or Incorrect Reasoning? Are the ', '23.7%. Similarly, PAL\nvastly outperforms COTby absolute 21.8% on REPEAT\nCOPY. Surprisingly, DIRECT prompting performs better\nthan COTonREPEAT COPY. Yet, PALimproves over\nDIRECT by 9.3% in R EPEAT COPY.\n[0,2] [3,5] [6,8] [9,11] [12,14] [15,17] [18,20] [21,23] [24,26]0.60.81\nNumber of ObjectsAccuracy\nPaL\nCoT\nFigure 6: The solve rate on COLORED OBJECTS with re-\nspect to the number of objects included in the test question.\nIs PAL sensitive to the complexity of the question? We\nexamined how the performance of PALandCOTchange as\nthe complexity of the input question grows, measured as the\nnumber of objects in the question of COLORED OBJECTS .\nAs shown in Figure 6, PALis superior COTacross all input\nlengths. As the number of objects in the question increases,\nCOT’s accuracy is unstable and drops, while PALremains\nconsistently close to 100%. More analysis on the token-level\npredictions can be found in Appendix G.\x0cPAL: Program-aided Language Models 7\ncode-cushman-001 code-davinci-001 code-davinci-002020406080\n21.731.872.0\n19.126.060.1\n13.6%22.3%19.8%Solve ratePAL\nCOT\nRelative Improvement\nFigure 7: PALwith different models on GSM8K: though\nthe absolute accuracies with code-cushman-001\nandcode-davinci-001 are lower than\ncode-davinci-002 , the relative improvement of\nPAL over C OT is consistent across models.text-davinci-001 text-davinci-002 text-davinci-003020406080\n26.546.965.3\n8.665.869.8 COT PAL\nFigure 8: PALwith NL LMs on GSM 8K: though\nCOToutperforms PALwithtext-davinci-001 , once\nthe base LM is sufﬁciently strong, PALis beneﬁcial\nwithtext-davinci-002 andtext-davinci-003\nas well. That is, P AL is not limited to code-LMs only.\n6. Analysis\nDoes P AL work with weaker LMs? In all our experi-\nments in Section 5, PALused thecode-davinci-002\nmodel; but can PALwork with weaker models of code? We\ncompared PALwith COTwhen both prompting approaches\nuse the same weaker base LMs code-cushman-001\nandcode-davinci-001 . As shown in Figure 7, even\nthough the absolute accuracies of code-cushman-001\nandcode-davinci-001 are lower, the relative improve-\nment of PALover COTremains consistent across models.\nThis shows that PALcan ', 'and conﬁgurations listed in Table 2. The 355M\nmodel is equivalent in size and conﬁguration of BERT-Large\nmodel (Devlin et al., 2018). The 2.5B model is bigger than\nthe previous largest GPT-2 model, and the 8.3B model is\nlarger than any left-to-right transformer language model\never trained, to the best of our knowledge. To train and eval-\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 2. Model conﬁgurations used for GPT-2.\nHidden Time\nParameter Layers Hidden Attn Size Total per\nCount Size Heads per GPUs Epoch\nHead (days)\n355M 24 1024 16 64 64 0.86\n2.5B 54 1920 20 96 128 2.27\n8.3B 72 3072 24 128 512 2.10\nTable 3. Zero-shot results. SOTA are from (Khandelwal et al.,\n2019) for Wikitext103 and (Radford et al., 2019) for LAMBADA.\nModel Wikitext103 LAMBADA\nPerplexity↓ Accuracy↑\n355M 19.31 45.18%\n2.5B 12.76 61.73%\n8.3B 10.81 66.51%\nPrevious SOTA 15.79 63.24%\nuate our language models we use the procedure described in\nsection 4. Table 2 also lists the time it takes to advance one\nepoch which is equivalent to 68,507 iterations. For example,\nfor the 8.3B model on 512 GPUs, each epoch takes around\ntwo days. Compared to the conﬁgurations used for our scal-\ning studies in Table 1, the 2.5B model is the same, the 8.3B\nmodel has 24 attention heads instead of 32, and the 355M is\nmuch smaller than any seen previously while still using 64\nGPUs to train, leading to the much lower time per epoch.\nFigure 6 shows validation perpelixity as a function of num-\nber of iterations. As the model size increases, the validation\nperpelixity decreases and reaches a validation perplexity of\n9.27 for the 8.3B model. We report the zero-shot evaluation\nof the trained models on the ']","PAL achieves state-of-the-art few-shot accuracy on the GSM8K benchmark, surpassing PaLM-540B by a significant 15% top-1 accuracy improvement, demonstrating its effectiveness in handling complex arithmetic and symbolic reasoning tasks.",None.,1.0,1.0,0.0966753363609314
Can PAL's approach be generalized to models trained primarily on natural language rather than code?,"['work with weaker models, while\nits beneﬁt scales elegantly to stronger models as well.\nDoes P AL work with LMs of natural language? We\nalso experimented with PALusing thetext-davinci\nseries. Figure 8 shows the following interesting re-\nsults: when the base LM’s “code modeling ability” is\nweak (using text-davinci-001 ),COTperforms better\nthan PAL. However, once the LM’s code modeling abil-\nity is sufﬁciently high (using text-davinci-002 and\ntext-davinci-003 ),PALoutperforms COT, and PAL\ntext-davinci-003 performs almost as PALcode-davinci-002 .\nThis shows that PALis not limited to LMs of code, but it\ncan work with LMs that were mainly trained for natural\nlanguage, if they have a sufﬁciently high coding ability.\nIs P AL better because of the Python prompt or because\nof the interpreter? We experimented with generating\nPython code, while requiring the neural LM to “execute” it\nas well, without using an interpreter, following Nye et al.\n(2021); Madaan et al. (2022). We created prompts that are\nsimilar to PAL’s, except that they do include the ﬁnal answer.\nThis resulted in a 23.2 solve rate on GSM 8K, much lower\nthan PAL(72.0), and only 4.5 points higher than DIRECT .\nThese results reinforce our hypothesis that the main beneﬁt\nofPALcomes from the synergy with the interpreter, andnot only from having a better prompt. Additional details\nare provided in Appendix B. For additional discussion on\nthe advantages of code-prompts over textual-prompts, see\nAppendix G.\nDo variable names matter? In all our experiments, we\nused meaningful variable names in the PALprompts, to ease\nthe model’s grounding of variables to the entities they rep-\nresent. For the Python interpreter, however, variable names\nare meaningless. To measure the importance of meaningful\nvariable names, we experimented with two prompts variants:\n1. P AL−comment – ', 'the PALprompt without intermediate\nNL comments.\n2. P AL−var\n−comment – the PALprompt without intermediate\nNL comments and with variable names substituted\nwith random characters.\nThe results are shown in Figure 9. In COLORED OBJECTED\nandDATE, removing intermediate NL comments but keep-\ning meaningful variable names ( PAL−comment ) – slightly re-\nduces the results compared to the full PALprompt, but it still\nachieves higher accuracy than the baselines COT. Remov-\ning variable names as well ( PAL−var\n−comment ) further decreases\naccuracy, and performs worse than COT. Since variable\nnames have an important part in code quality (Gellenbeck\n& Cook, 1991; Takang et al., 1996), meaningful variable\nnames are only expected to ease reasoning for Codex, which\nwas trained on mostly meaningful names, as was also found\nby Madaan et al. (2022).\n7. Related Work\nPrompting Few-shot prompting (Brown et al., 2020) has\nbeen shown to be an effective approach for a variety of\ntasks (Liu et al., 2021) ranging from text- (Gehrmann et al.,\x0cPAL: Program-aided Language Models 8\nColored Objects Date Penguins60708090100\n84.4\n64.879.295.2\n76.293.391.1\n69.191.3\n79.9\n63.491.9COT PAL PAL−comment PAL−var\n−comment\nFigure 9: Ablation study of PALprompt formats. We consider the original PALprompt, it with natural language comments\nremoved ( PAL−comment ), and further variable names replaced with random character ( PAL−var\n−comment ). As a reference, we also\nshow the C OT performance (blue).\n2021; Reif et al., 2021; Wei et al., 2021; Sanh et al., 2021)\nto code-generation (Chen et al., 2021b). Methods such as\nchain-of-thought prompting ( COT) have further unlocked a\nvariety of reasoning tasks, boosting the performance of mod-\nels on a variety of benchmarks. Nevertheless, all previous\napproaches suffer from inaccuracy in arithmetic calculation\nand incorrect reasoning (Lewkowycz et al., 2022; Hendrycks\net al., 2021; Madaan & Yazdanbakhsh, 2022). ', 'with Codex, but can also\nbe applied to natural language (NL) models, as long as the model is sufﬁciently strong. Speciﬁcally, our results showed that\nin the text-davinci-001 model, the use of the CoT approach resulted in better performance.\nModel CoT PaL\ntext-davinci-001 26.5 8.6\ntext-davinci-002 46.9 65.8\ntext-davinci-003 65.3 69.8\nD. Analyzing the Effect of Increasing Number of Samples on P AL\nIn Section 5.1, we show that PALoutperforms strong baselines both for a single sample and by drawing 40 samples and\nusing majority voting. Figure 12 illustrates the trends for cases when the number of samples drawn are between 1 and 40,\nand the interpolation estimates demonstrate that P AL remains competitive throughout the number of samples.\x0cPAL: Program-aided Language Models 15\ndefsolution():\n""""""Shawn has five toys. For Christmas, he got two toys each from his\nmom and dad. How many toys does he have now?"""""";\ntoys_initial = 5\nmom_toys = 2\ndad_toys = 2\ntotal_received = mom_toys + dad_toys\ntotal_toys = toys_initial + total_received\nresult = total_toys\nreturnresult\n(a) Original Example\ndefsolution():\nreturn5 + 2 + 2\n(b) Succinct Code\ndefsolution():\n""""""Shawn has 10312864 toys. For Christmas, he got 13267894 toys each\nfrom his mom and dad. How many toys does he have now?""""""\ntoys_initial = 10312864\nmom_toys = 13267894\ndad_toys = 13267894\ntotal_received = mom_toys + dad_toys\ntotal_toys = toys_initial + total_received\nresult = total_toys\nreturnresult\n(c) Hard Examples in Prompt (P AL)\nExample(\nquestion=""Shawn has 10312864 toys. For Christmas, he got 13267894 toys\neach from his mom and dad. How many toys does he have now?"",\nthought=""Shawn started with 10312864 toys. If he got 13267894 toys each\nfrom his mom and dad, then that is 26535788 more toys. 10312864 +\n26535788 = 36848652."",\nanswer=""36848652"",\n),\n(d) Hard Examples in Prompt (CoT)\ndefsolution():\n""""""Shawn has five toys. For Christmas, ']","PAL is effective with models trained on code, but experiments indicate it can also enhance models primarily trained on natural language, provided they possess sufficient coding ability, demonstrating PAL's versatility.","Yes, PAL's approach can be generalized to models trained primarily on natural language rather than code, as long as the model has a sufficiently high coding ability.",1.0,1.0,0.7820087671279907
What contributions does CodeNet make towards the creation of AI models capable of understanding and generating code?,"['page 143–153, New\nYork, NY , USA, 2019. Association for Computing Machinery.\n[25] Wikipedia. Jaccard index — Wikipedia, the free encyclopedia. https://en.wikipedia.\norg/wiki/Jaccard_index , 2020.\n[26] Terence Parr. The Deﬁnitive ANTLR 4 Reference . Pragmatic Bookshelf, 2nd edition, 2013.\n[27] Sifei Luan, Di Yang, Celeste Barnaby, Koushik Sen, and Satish Chandra. Aroma: code recom-\nmendation via structural code search. Proceedings of the ACM on Programming Languages ,\n3(OOPSLA):1–28, Oct 2019.\n[28] IBM T.J. Watson Research Center. Wala. https://github.com/wala/WALA , 2021.\n[29] Forbes on codenet. https://www.forbes.com/sites/moorinsights/2021/06/04/ibm-\ncodenet-artificial-intelligence-that-can-program-computers-and-solve-a-\n100-billion-legacy-code-problem/?sh=343813636cdc .\n[30] Venturebeat on codenet. https://venturebeat.com/2021/05/10/ibms-codenet-\ndataset-aims-to-train-ai-to-tackle-programming-challenges/ .\n[31] Zdnet on codenet. https://www.zdnet.com/article/ibm-launches-autosql-watson-\norchestrate-codenet-enterprise-ai-tools-at-think/ .\n[32] Project codenet repository. https://github.com/IBM/Project_CodeNet .\n[33] Luca Buratti, Saurabh Pujar, Mihaela Bornea, Scott McCarley, Yunhui Zheng, Gaetano Rossiello,\nAlessandro Morari, Jim Laredo, Veronika Thost, Yufan Zhuang, and Giacomo Domeniconi.\nExploring software naturalness through neural language models, 2020.\n[34] Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional\nnetworks. In ICLR , 2017.\n[35] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\nnetworks? In ICLR , 2019.\n[36] Veronika Thost and Jie Chen. Directed acyclic graph neural networks. In ICLR , 2021.\n[37] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B.\nClement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,\nMichele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng,\nShengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code\nunderstanding and generation. CoRR , abs/2102.04664, 2021.\n[38] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks\nfor semi-supervised learning, 2018.\n[39] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele\nCatasta, and ', 'be readily used as inputs into machine learning models. Results of code classi-\nﬁcation and code similarity experiments using the CodeNet dataset are provided as\na reference. We hope that the scale, diversity and rich, high-quality annotations of\nCodeNet will offer unprecedented research opportunities at the intersection of AI\nand Software Engineering.\n1 Introduction\nThere is a growing trend towards leveraging AI for building tools that support software engineering\nand development [ 1,2]. AI can manipulate and generate computer code, but can it do so with\nhigh quality? Many researchers are fascinated by this possibility, encouraged by AI successes in\nother domains and tantalized by the vision of computers programming computers. Some recent\ndeep-learning models [ 3,4] for code have received a lot of publicity: trained on vast amounts of\ndata and using novel architectures with billions of parameters, they sometimes generate surprisingly\nplausible code.\nPreprint. Under review.arXiv:2105.12655v2 [cs.SE] 29 Aug 2021\x0cGiven the success of non-AI tools for code, why should we consider AI to augment or possibly\nreplace them? Firstly, AI can help reﬁne and re-tune the heuristics used by traditional coding tools.\nSecondly, based on the training data from past experience, AI can help prioritize when there is more\nthan one sound answer [ 5]. Thirdly, an AI-based tool may handle incomplete or invalid code more\nrobustly, thus expanding its scope. Finally, AI can incorporate signals usually ignored by traditional\ntools for code, such as the natural language in identiﬁers or comments.\nIn the enterprise environment, developers often face code written by large teams over many years\nand geographies. Developers must manipulate such code to modernize it, ﬁx bugs, improve its\nperformance, evolve it when ', 'requirements change, make it more secure, and/or comply with regu-\nlations. These tasks are challenging, and it is crucial to provide tool support for developers to be\nmore productive at performing them. It is well known that the latest advancements in deep learning\nalgorithms rely on best-of-breed datasets, such as ImageNet, to create increasingly complex and\npowerful models. In this paper, we present ""CodeNet"", a ﬁrst-of-its-kind dataset in scale, diversity,\nand quality, to accelerate the algorithmic advances in AI for Code.\nTo promote widespread adoption of CodeNet, we will be launching contests involving use cases based\non the dataset. The ﬁrst contest [ 6] will focus on diversity, inclusion and spurring interest among\naspiring data scientists. We are partnering with the Global Women in Data Science organization (with\npresence in over 50 countries) founded by Stanford University [ 7] and targeting teams with at least\nﬁfty percent women. We are planning follow-up contests that target experienced AI practitioners.\nThe rest of the paper is organized as follows. Section 2 introduces the CodeNet dataset. Related\ndatasets are discussed in Section 3, and the differentiation of CodeNet with respect to these related\ndatasets is elaborated in Section 4. Section 5 describes how CodeNet was curated and Section 6\nenumerates the usability features of CodeNet with several pre-processing tools to transform source\ncodes into representations that can be readily used as inputs into machine learning models. Section 7\ndiscusses the upcoming CodeNet contest and Section 8 describes important baseline experiments\nwith the CodeNet dataset. Section 9 presents further uses of the CodeNet dataset and Section 10\nconcludes the paper.\n2 The CodeNet Dataset\nThe CodeNet dataset consists of a ']"," By providing a comprehensive dataset with diverse programming tasks and solutions, CodeNet accelerates the development of AI models for code generation and understanding.","CodeNet contributes by providing a first-of-its-kind dataset in scale, diversity, and quality, which accelerates algorithmic advances in AI for Code. It offers unprecedented research opportunities at the intersection of AI and Software Engineering, enabling the training of increasingly complex and powerful models. Additionally, CodeNet includes rich, high-quality annotations, usability features, and pre-processing tools that allow source codes to be readily used as inputs into machine learning models for code understanding and generation.",1.0,1.0,0.9221318960189819
How does SuperGLUE facilitate the development and evaluation of models across its diverse set of tasks?,"['advances on GLUE, but\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\nC Human Performance Estimation\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\nannotation. For both steps and all tasks, the average pay rate is $23.75/hr.7\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\nare asked to annotate up to 30 examples from the development set. After answering each example,\nworkers are also asked to check their work against the provided ground truth label. After the training\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\nperformance at, or above the median performance across all workers during training.\n7This estimate is taken from https://turkerview.com .\n16\x0cIn the annotation phase, workers are provided with the same instructions as the training phase, and\nare linked to the same FAQ page. The instructions for all tasks are provided in Appendix C. For the\nannotation phase we randomly sample 100 examples from the task’s test set, with the exception of\nWSC where we annotate the full test set. For each example, we collect annotations from ﬁve workers\nand take a majority vote to estimate human performance. For additional details, see Appendix C.3.\nC.1 Training Phase Instructions\nIn the training step, we provide workers with brief instructions about the training phase. An example\nof these instructions is given Table 5. These training instructions ', 'et al., 2014). Finally, we list the best known\nresult on each task as of May 2019, except on tasks which we recast (WSC), resplit (CB), or achieve\n6For ReCoRD, we predict the entity that has the highest F1 with the other entity options.\n8\x0cthe best known result (WiC). The outside results for COPA, MultiRC, and RTE are from Sap et al.\n(2019), Trivedi et al. (2019), and Liu et al. (2019d) respectively.\nHuman Performance Pilehvar and Camacho-Collados (2019), Khashabi et al. (2018), Nangia and\nBowman (2019), and Zhang et al. (2018) respectively provide estimates for human performance\non WiC, MultiRC, RTE, and ReCoRD. For the remaining tasks, including the diagnostic set, we\nestimate human performance by hiring crowdworker annotators through Amazon’s Mechanical Turk\nplatform to reannotate a sample of each test set. We follow a two step procedure where a crowd\nworker completes a short training phase before proceeding to the annotation phase, modeled after the\nmethod used by Nangia and Bowman (2019) for GLUE. See Appendix C for details.\n5.2 Results\nTable 3 shows results for all baselines. The most frequent class and CBOW baselines do not perform\nwell overall, achieving near chance performance for several of the tasks. Using BERT increases\nthe average SuperGLUE score by 25 points, attaining signiﬁcant gains on all of the benchmark\ntasks, particularly MultiRC, ReCoRD, and RTE. On WSC, BERT actually performs worse than\nthe simple baselines, likely due to the small size of the dataset and the lack of data augmentation.\nUsing MultiNLI as an additional source of supervision for BoolQ, CB, and RTE leads to a 2-5 point\nimprovement on all tasks. Using SWAG as ', 'the nonbinding referendum favored statehood, an increase\nover the results of a 2012 referendum, ofﬁcial results from the State Electorcal Commission show. It\nwas the ﬁfth such vote on statehood. ""Today, we the people of Puerto Rico are sending a strong and\nclear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\nPuerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\nfavor of US statehood\nQuery For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\n<placeholder> presidency Correct Entities: USRTEText: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\naccording to the Christopher Reeve Foundation.\nHypothesis: Christopher Reeve had an accident. Entailment: FalseWiCContext 1: Room and board .Context 2: He nailed boards across the windows.\nSense match: FalseWSCText: Mark told Pete many lies about himself, which Pete included in his book. Heshould have been\nmore truthful. Coreference: False\n3 SuperGLUE Overview\n3.1 Design Process\nThe goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\nbeing applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\nwe identify the following desiderata of tasks in the benchmark:\nTask substance: Tasks should test a system’s ability to understand and reason about texts in English.\nTask difﬁculty: Tasks should be beyond the scope of current state-of-the-art systems, but solvable by\nmost college-educated English speakers. We exclude tasks that require domain-speciﬁc knowledge,\ne.g. medical notes or scientiﬁc papers.\nEvaluability: Tasks must have an automatic performance metric that ']","SuperGLUE provides a public leaderboard, a single-number performance metric for easy comparison, a software toolkit for model analysis, and detailed task instructions, supporting the development, evaluation, and comparison of models on its comprehensive suite of tasks.","SuperGLUE facilitates the development and evaluation of models across its diverse set of tasks by providing a simple, robust evaluation metric that tests a system's ability to understand and reason about texts in English. Tasks included in SuperGLUE are designed to be challenging beyond the scope of current state-of-the-art systems but solvable by most college-educated English speakers. Additionally, tasks are selected based on their evaluability, meaning they must have an automatic performance metric for accurate evaluation of model performance.",0.5,1.0,0.5732627511024475
 What tools and support does SuperGLUE offer to researchers working on language understanding models?,"[""Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman.\nSuperglue: A stickier benchmark for general-purpose language understanding systems, 2019.\nR. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi. Hellaswag: Can a machine really ﬁnish\nyour sentence?, 2019.\nR. Zellers, A. Holtzman, E. Clark, L. Qin, A. Farhadi, and Y . Choi. Evaluating machines by their\nreal-world language use, 2020.\n10\x0cPublished as a conference paper at ICLR 2021\nA A DDITIONAL ANALYSIS\nThis appendix includes ﬁgures with sorted results (Figure 9), few-shot examples vs. accuracy\n(Figure 10), and few-shot calibration (Figure 11). It also includes sections on ﬁne-tuning, error\nanalysis, and format sensitivity.\n0 20 40 60 80 100\nAccuracy (%)College ChemistryMoral ScenariosCollege PhysicsHigh School PhysicsHigh School MathematicsFormal LogicElementary MathematicsAbstract AlgebraHigh School StatisticsMachine LearningEconometricsHigh School ChemistryProfessional AccountingProfessional LawCollege MathematicsProfessional MedicineConceptual PhysicsGlobal FactsHigh School Comp SciMedical GeneticsHigh School MacroeconomicsHigh School MicroeconomicsMoral DisputesProfessional PsychologyCollege BiologyVirologyCollege Comp SciBusiness EthicsNutritionCollege MedicineAnatomyClinical KnowledgeLogical FallaciesHigh School BiologyPublic RelationsAstronomyElectrical EngineeringHuman AgingPhilosophySecurity StudiesPrehistoryHigh School US HistorySociologyHigh School European HistoryHuman SexualityJurisprudenceWorld ReligionsInternational LawHigh School World HistoryManagementComputer SecurityHigh School GeographyHigh School Gov't and PoliticsMarketingMiscellaneousHigh School PsychologyUS Foreign PolicyGPT-3 Results\nRandom Chance\n0102030405060708090100\nAccuracy (%)Moral ScenariosFormal LogicAbstract AlgebraEconometricsHigh School MathematicsCollege PhysicsMachine LearningHigh School StatisticsCollege ChemistryElementary MathematicsCollege MathematicsHigh School ChemistryGlobal FactsProfessional LawMedical GeneticsProfessional AccountingCollege BiologyHigh School PhysicsAnatomyCollege Comp SciConceptual PhysicsCollege MedicineVirologyProfessional MedicineAstronomyHigh School MacroeconomicsElectrical EngineeringProfessional PsychologySecurity StudiesHuman SexualityNutritionHigh School Comp SciPrehistoryClinical KnowledgeHigh School BiologyHuman AgingHigh School MicroeconomicsPhilosophyPublic RelationsWorld ReligionsMoral DisputesLogical FallaciesHigh School European HistoryHigh School US HistoryHigh School World HistoryMiscellaneousUS Foreign PolicyComputer SecuritySociologyInternational LawHigh School GeographyJurisprudenceBusiness EthicsHigh School PsychologyManagementHigh School Gov't and PoliticsMarketingUnifiedQA Results\nRandom Chance\nFigure 9: On the left are GPT-3 few shot accuracies for all of the "", 'advances on GLUE, but\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\nC Human Performance Estimation\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\nannotation. For both steps and all tasks, the average pay rate is $23.75/hr.7\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\nare asked to annotate up to 30 examples from the development set. After answering each example,\nworkers are also asked to check their work against the provided ground truth label. After the training\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\nperformance at, or above the median performance across all workers during training.\n7This estimate is taken from https://turkerview.com .\n16\x0cIn the annotation phase, workers are provided with the same instructions as the training phase, and\nare linked to the same FAQ page. The instructions for all tasks are provided in Appendix C. For the\nannotation phase we randomly sample 100 examples from the task’s test set, with the exception of\nWSC where we annotate the full test set. For each example, we collect annotations from ﬁve workers\nand take a majority vote to estimate human performance. For additional details, see Appendix C.3.\nC.1 Training Phase Instructions\nIn the training step, we provide workers with brief instructions about the training phase. An example\nof these instructions is given Table 5. These training instructions ', 'quality of research contributions aimed at the challenges posed by GLUE\nunderline the utility of this style of benchmark for machine learning researchers looking to evaluate\nnew application-agnostic methods on language understanding.\nLimits to current approaches are also apparent via the GLUE suite. Performance on the GLUE\ndiagnostic entailment dataset, at 0.42 R3, falls far below the average human performance of 0.80\nR3reported in the original GLUE publication, with models performing near, or even below, chance\non some linguistic phenomena (Figure 2, Appendix B). While some initially difﬁcult categories\nsaw gains from advances on GLUE (e.g., double negation), others remain hard (restrictivity) or\neven adversarial (disjunction, downward monotonicity). This suggests that even as unsupervised\npretraining produces ever-better statistical summaries of text, it remains difﬁcult to extract many\ndetails crucial to semantics without the right kind of supervision. Much recent work has made similar\nobservations about the limitations of existing pretrained models (Jia and Liang, 2017; Naik et al.,\n2018; McCoy and Linzen, 2019; McCoy et al., 2019; Liu et al., 2019a,b).\n3\x0cTable 2: Development set examples from the tasks in SuperGLUE. Bold text represents part of the\nexample format for each task. Text in italics is part of the model input. Underlined text is specially\nmarked in the input. Text in a monospaced font represents the expected model output.BoolQPassage: Barq’s – Barq’s is an American soft drink. Its brand of root beer is notable for having caffeine.\nBarq’s, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq\nfamily but bottled by the Coca-Cola Company. It was known as Barq’s Famous Olde Tyme Root Beer\nuntil ']","SuperGLUE offers jiant, a software toolkit for pretraining, multi-task learning, and transfer learning in NLP, built around standard tools and supporting popular pretrained models, facilitating research and experimentation on the benchmark's tasks.","SuperGLUE offers training instructions, data collection procedures, benchmark tasks, and annotations to support researchers working on language understanding models.",1.0,1.0,0.6937844753265381
In what ways does GLM-130B's bilingual capability extend its application compared to monolingual models?,"['”. The first option servers as a\ndistractor to test models’ language capability and calculate LMS; the second and third statements\nare anti-stereotypical and stereotypical respectively and used for calculating SS. A widely-adopted\ntechnique here is to calibrate the likelihood of an option according to its length (Lieber et al., 2021;\nZhang et al., 2022), as the distractor term is particularly short.\nFollowing (Zhang et al., 2022), we normalize scores over tokens rather than characters (Lieber et al.,\n2021) to yield model predictions for calculating the metrics. The results are shown in Table 6. As we\nobserve, GLM-130B exceedingly outperforms GPT-3 Davinci and OPT-175B on all metrics. Such\nresults accurately align with our discoveries in language modeling experiments and CrowS-Pairs\nbias evaluation, that GLM-130B has a high quality in both language modeling and social fairness.\nTable 6: StereoSet (Nadeem et al., 2021) Bias Measurement with LMS ( ↑), SS (↓), and ICAT ( ↑).\nCategoryProfession Gender Religion Race Overall\nLMS SS ICAT LMS SS ICAT LMS SS ICAT LMS SS ICAT LMS SS ICAT\nGPT-3 78.4 63.4 57.5 75.6 66.5 50.6 80.8 59.0 66.3 77.0 57.4 65.7 77.6 60.8 60.8\nOPT-175B 74.1 62.6 55.4 74.0 63.6 53.8 84.0 59.0 68.9 74.9 56.8 64.8 74.8 59.9 60.0\nGLM-130B 86.5 59.6 69.9 83.9 63.5 61.2 91.0 53.5 84.6 85.7 54.1 78.7 86.0 57.3 73.5\nA.3 H ATESPEECH DETECTION : ETHOS\nSocial media corpus may contain hate speeches, and to investigate to what extent LLMs know and\ncan help to identify them is crucial. We adopt the ETHOS dataset originally proposed in (Mollas\net al., 2020) to detect sexism and racism speech on zero-shot or few-shot datasets created ', 'Jie\nTang\n•Project Leader: Jie Tang\nE.5 C OMPUTATION SPONSOR\n•GPU Sponsor: Zhipu.AI\n52\x0cPublished as a conference paper at ICLR 2023\nF A B RIEF HISTORY OF GLM-130B\nThe GLM-130B project16was conceived in Dec. 2021 in a brainstorming meeting at Tsinghua KEG.\nWe firmly believe that it is of value to pre-train a highly accurate language model, in particular for\nboth Chinese and English. Though GPT-3 (Brown et al., 2020) is the pioneer for this effort, it is not\navailable to most people in the world. In addition, it supports English only. We therefore decide to\ninitialize the project GLM-130B. Please note that the WuDao 1.75T model we built last year is a\nsparse model with 480 mixture-of-experts (MoE), rather than a dense one as GPT-3. Our goal then\nis to train a bilingual pre-trained dense model with high accuracy on downstream tasks, and to make\nit open to everyone in the world-anyone, anywhere can download it and use it on a single server with\nappropriate GPUs.\nThe ambitious project soon faced several important challenges:\n•Lack of computational resources : No organization is willing to sponsor such a big project and\nfreely make it public.\n•Lack of a robust pre-training algorithm : Despite GPT-3’s success on English corpus, it is\nunclear how to train a high-accurate bilingual model for both English and Chinese.\n•Lack of fast inference solutions : Since the goal is to have the model public to everyone, we need\nto design fast inference solutions with low resource requirements to run the model.\nFor the pre-training algorithm, we finally chose GLM (Du et al., 2022) due to its high performance\nin practice. We eventually decided to train a ', 'Published as a conference paper at ICLR 2023\nGLM-130B: A NOPEN BILINGUAL PRE-TRAINED\nMODEL\nAohan Zeng⋄†∗, Xiao Liu⋄†∗, Zhengxiao Du⋄†, Zihan Wang⋄, Hanyu Lai⋄, Ming Ding⋄,\nZhuoyi Yang⋄, Yifan Xu⋄, Wendi Zheng⋄, Xiao Xia⋄, Weng Lam Tam⋄§, Zixuan Ma⋄,\nYufei Xue§, Jidong Zhai⋄, Wenguang Chen⋄, Peng Zhang§, Yuxiao Dong⋄‡, Jie Tang⋄‡\nTsinghua University⋄Zhipu.AI§\nABSTRACT\nWe introduce GLM-130B, a bilingual (English and Chinese) pre-trained language\nmodel with 130 billion parameters. It is an attempt to open-source a 100B-scale\nmodel at least as good as GPT-3 (davinci) and unveil how models of such a scale\ncan be successfully pre-trained. Over the course of this effort, we face numer-\nous unexpected technical and engineering challenges, particularly on loss spikes\nand divergence. In this paper, we introduce the training process of GLM-130B\nincluding its design choices, training strategies for both efficiency and stabil-\nity, and engineering efforts. The resultant GLM-130B model offers significant\noutperformance over GPT-3 175B (davinci) on a wide range of popular English\nbenchmarks while the performance advantage is not observed in OPT-175B and\nBLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN\n3.0 260B—the largest Chinese language model—across related benchmarks. Fi-\nnally, we leverage a unique scaling property of GLM-130B to reach INT4 quanti-\nzation without post training, with almost no performance loss, making it the first\namong 100B-scale models and more importantly, allowing its effective inference\non 4×RTX 3090 (24G) or 8 ×RTX 2080 Ti (11G) GPUs, the most affordable\nGPUs required for using 100B-scale models. The GLM-130B model weights are\npublicly accessible and its code, training logs, related toolkit, and lessons learned\nare open-sourced at https://github.com/THUDM/GLM-130B/ .\n1 I NTRODUCTION\nLarge language models (LLMs), particularly those with over 100 billion (100B) parameters (Brown\net ']","As a bilingual model in English and Chinese, GLM-130B significantly outperforms monolingual models in related benchmarks, showcasing its superior performance in cross-lingual understanding and applications.","GLM-130B's bilingual capability allows it to be applied in both English and Chinese contexts, extending its application compared to monolingual models that are limited to just one language.",1.0,1.0,0.9124672412872314
What intrinsic model characteristic allows Megatron-LM to achieve efficient training with multi-billion parameter transformer models?,"['and conﬁgurations listed in Table 2. The 355M\nmodel is equivalent in size and conﬁguration of BERT-Large\nmodel (Devlin et al., 2018). The 2.5B model is bigger than\nthe previous largest GPT-2 model, and the 8.3B model is\nlarger than any left-to-right transformer language model\never trained, to the best of our knowledge. To train and eval-\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 2. Model conﬁgurations used for GPT-2.\nHidden Time\nParameter Layers Hidden Attn Size Total per\nCount Size Heads per GPUs Epoch\nHead (days)\n355M 24 1024 16 64 64 0.86\n2.5B 54 1920 20 96 128 2.27\n8.3B 72 3072 24 128 512 2.10\nTable 3. Zero-shot results. SOTA are from (Khandelwal et al.,\n2019) for Wikitext103 and (Radford et al., 2019) for LAMBADA.\nModel Wikitext103 LAMBADA\nPerplexity↓ Accuracy↑\n355M 19.31 45.18%\n2.5B 12.76 61.73%\n8.3B 10.81 66.51%\nPrevious SOTA 15.79 63.24%\nuate our language models we use the procedure described in\nsection 4. Table 2 also lists the time it takes to advance one\nepoch which is equivalent to 68,507 iterations. For example,\nfor the 8.3B model on 512 GPUs, each epoch takes around\ntwo days. Compared to the conﬁgurations used for our scal-\ning studies in Table 1, the 2.5B model is the same, the 8.3B\nmodel has 24 attention heads instead of 32, and the 355M is\nmuch smaller than any seen previously while still using 64\nGPUs to train, leading to the much lower time per epoch.\nFigure 6 shows validation perpelixity as a function of num-\nber of iterations. As the model size increases, the validation\nperpelixity decreases and reaches a validation perplexity of\n9.27 for the 8.3B model. We report the zero-shot evaluation\nof the trained models on the ', 'in\nour training data.\n5.3. Bi-directional Transformer Results Using BERT\nIn this section, we apply our methodology to BERT-style\ntransformer models and study the effect of model scaling\non several downstream tasks. Prior work (Lan et al., 2019)\nfound that increasing model size beyond BERT-large with\n336M parameters results in unexpected model degradation.\nTo address this degradation, the authors of that work (Lan\net al., 2019) introduced parameter sharing and showed that\nthat their models scale much better compared to the original\nBERT model.\nWe further investigated this behaviour and empirically\ndemonstrated that rearranging the order of the layer nor-\nmalization and the residual connections as shown in Figure\n7 is critical to enable the scaling of the BERT-style mod-\nels beyond BERT-Large. The architecture (b) in Figure 7\neliminates instabilities observed using the original BERT\narchitecture in (a) and also has a lower training loss. To\nthe best of our knowledge, we are the ﬁrst to report such a\nchange enables training larger BERT models.\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 5. Development set results for MNLI, QQP, SQuAD 1.1 and SQuAD 2.0 and test set results for RACE. The trained tokens represents\nconsumed tokens during model pretraining (proportional to batch size times number of iterations) normalized by consumed tokens during\nmodel pretraining for our 336M model.\nModeltrained tokens MNLI m/mm QQP SQuAD 1.1 SQuAD 2.0 RACE m/h\nratio accuracy accuracy F1 / EM F1 / EM accuracy\n(dev set) (dev set) (dev set) (dev set) (test set)\nRoBERTa (Liu et al., 2019b) 2 90.2 / 90.2 92.2 94.6 / 88.9 89.4 / 86.5 83.2 (86.5 / 81.8)\nALBERT (Lan et al., 2019) 3 90.8 92.2 94.8 / 89.3 90.2 ', '2014).\nChelba et al. (2013) introduced the One Billion\nWord benchmark, a large scale training dataset to\nmeasure the progress of language models.\nIn the context of neural language models, Joze-\nfowicz et al. (2016) obtained state-of-the-art re-\nsults on the Billion Word benchmark by scaling\nLSTMs to 1 billion parameters. Later, scaling\ntransformers lead to improvement on many NLP\ntasks. Notable models include BERT (Devlin et al.,\n2018), GPT-2 (Radford et al., 2019), Megatron-\nLM (Shoeybi et al., 2019), and T5 (Raffel et al.,\n2020). A signiﬁcant breakthrough was obtained\nwith GPT-3 (Brown et al., 2020), a model with\n175 billion parameters. This lead to a series of\nLarge Language Models , such as Jurassic-1 (Lieber\net al., 2021), Megatron-Turing NLG (Smith et al.,2022), Gopher (Rae et al., 2021), Chinchilla (Hoff-\nmann et al., 2022), PaLM (Chowdhery et al., 2022),\nOPT (Zhang et al., 2022), and GLM (Zeng et al.,\n2022). Hestness et al. (2017) and Rosenfeld et al.\n(2019) studied the impact of scaling on the perfor-\nmance of deep learning models, showing the exis-\ntence of power laws between the model and dataset\nsizes and the performance of the system. Kaplan\net al. (2020) derived power laws speciﬁcally for\ntransformer based language models, which were\nlater reﬁned by Hoffmann et al. (2022), by adapting\nthe learning rate schedule when scaling datasets.\nFinally, Wei et al. (2022) studied the effect of scal-\ning on the abilities of large language models.\n8 Conclusion\nIn this paper, we presented a series of language\nmodels that are released openly, and competitive\nwith state-of-the-art foundation models. Most\nnotably, LLaMA-13B outperforms GPT-3 while\nbeing more than 10 ×smaller, and LLaMA-65B is\ncompetitive with Chinchilla-70B and PaLM-540B.\nUnlike previous studies, we show that it is possible\nto achieve ']","Megatron-LM utilizes intra-layer model parallelism exploiting the inherent structure of transformer-based language models, enabling efficient training without custom C++ or compiler requirements.",Model parallelism allows Megatron-LM to achieve efficient training with multi-billion parameter transformer models.,1.0,1.0,0.7585973739624023
Describe Megatron-LM's approach to handling the output embedding weight matrix for model parallelism.,"['and conﬁgurations listed in Table 2. The 355M\nmodel is equivalent in size and conﬁguration of BERT-Large\nmodel (Devlin et al., 2018). The 2.5B model is bigger than\nthe previous largest GPT-2 model, and the 8.3B model is\nlarger than any left-to-right transformer language model\never trained, to the best of our knowledge. To train and eval-\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 2. Model conﬁgurations used for GPT-2.\nHidden Time\nParameter Layers Hidden Attn Size Total per\nCount Size Heads per GPUs Epoch\nHead (days)\n355M 24 1024 16 64 64 0.86\n2.5B 54 1920 20 96 128 2.27\n8.3B 72 3072 24 128 512 2.10\nTable 3. Zero-shot results. SOTA are from (Khandelwal et al.,\n2019) for Wikitext103 and (Radford et al., 2019) for LAMBADA.\nModel Wikitext103 LAMBADA\nPerplexity↓ Accuracy↑\n355M 19.31 45.18%\n2.5B 12.76 61.73%\n8.3B 10.81 66.51%\nPrevious SOTA 15.79 63.24%\nuate our language models we use the procedure described in\nsection 4. Table 2 also lists the time it takes to advance one\nepoch which is equivalent to 68,507 iterations. For example,\nfor the 8.3B model on 512 GPUs, each epoch takes around\ntwo days. Compared to the conﬁgurations used for our scal-\ning studies in Table 1, the 2.5B model is the same, the 8.3B\nmodel has 24 attention heads instead of 32, and the 355M is\nmuch smaller than any seen previously while still using 64\nGPUs to train, leading to the much lower time per epoch.\nFigure 6 shows validation perpelixity as a function of num-\nber of iterations. As the model size increases, the validation\nperpelixity decreases and reaches a validation perplexity of\n9.27 for the 8.3B model. We report the zero-shot evaluation\nof the trained models on the ', 'data parallelism and handling random number\ngeneration.\nB.1. Hybrid Model and Data Parallelism\nModel parallelism is orthogonal to data parallelism, and so\nwe can use both simultaneously to train large models in a\nreasonable amount of time. Figure 8 shows a grouping of\nGPUs for hybrid model and data parallelism. Two or more\nGPUs within the same server form model parallel groups\n(for example GPUs 1 to 8 in Figure 8), and contain oneTable 6. Hyperparameters for ﬁnetuning BERT model on down-\nstream tasks.\nTask Model Batch Learning Training\nsize rate epochs\n336M\nMNLI 1.3B 128 1e-5 10\n3.8B\n336M 128 5e-5\nQQP 1.3B 128 3e-5 12\n3.8B 256 4e-5\n336M 64 3e-5\nSQUAD 1.1 1.3B 48 3e-5 2\n3.8B 48 1e-5\n336M 48 3e-5\nSQUAD 2.0 1.3B 64 3e-5 2\n3.8B 48 1e-5\n336M 32 2e-5\nRACE 1.3B 16 1e-5 3\n3.8B 32 2e-5\ninstance of the model distributed across these GPUs. The\nremaining GPUs, which could be within the same server but\nmore typically are located in other servers, run additional\nmodel parallel groups. GPUs with the same position in each\nof the model parallel groups (for example GPUs 1, 9, ...,\n505 in Figure 8) form data parallel groups so that all GPUs\nwithin a data parallel group hold the same model param-\neters. During back propagation we run multiple gradient\nall-reduce operations in parallel to reduce weight gradients\nwithin each distinct data parallel group. The total number\nof required GPUs is the product of the number of model\nand data parallel groups. For example, for the 8.3 billion\nparameter model we use 8 GPUs per model parallel group\nand 64-way data parallelism, for a total of 512 GPUs. All\ncommunication is implemented in PyTorch by Python calls\nto NCCL. GPUs within each model parallel ', 'self-attention. The subsequent GEMM from the output\nlinear layer (after self attention) is parallelized along its\nrows and takes the output of the parallel attention layer di-\nrectly, without requiring communication between the GPUs.\nThis approach for both the MLP and self attention layer\nfuses groups of two GEMMs, eliminates a synchronization\npoint in between, and results in better scaling. This enables\nus to perform all GEMMs in a simple transformer layer\nusing only two all-reduces in the forward path and two in\nthe backward path (see Figure 4).\nThe transformer language model has an output embedding\nwith the dimension of hidden-size ( H) times vocabulary-\nsize (v). Since the vocabulary size is on the order of tens\nof thousands of tokens for modern language models (for\nexample, GPT-2 used a vocabulary size of 50,257), it is ben-\neﬁcial to parallelize the output embedding GEMM. How-\never, in transformer language models, the output embed-\nding layer shares weights with the input embedding, requir-\ning modiﬁcations to both. We parallelize the input embed-\nding weight matrix EH×valong the vocabulary dimension\nE= [E1,E2](column-wise). Since each partition now only\x0cMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nFigure 4. Communication operations in a transformer layer. There\nare 4 total communication operations in the forward and backward\npass of a single model parallel transformer layer.\ncontains a portion of the embedding table, an all-reduce ( g\noperator) is required after the input embedding. For the\noutput embedding, one approach is to perform the parallel\nGEMM [Y1,Y2] = [XE 1,XE 2]to obtain the logits, add an\nall-gatherY=all-gather ([Y1,Y2]), and send the results to\nthe cross-entropy loss function. However, for this case, the\nall-gather will communicate b×s×velements (bis the\nbatch-size and sis the ']","The output embedding weight matrix is parallelized along the vocabulary dimension, requiring minimal communication and maximizing compute efficiency during training.","Megatron-LM parallelizes the output embedding weight matrix by partitioning it along the vocabulary dimension. The approach involves performing a parallel GEMM operation to obtain the logits, adding an all-gather operation to gather the results, and then sending the results to the cross-entropy loss function. Additionally, Megatron-LM modifies both the input and output embedding layers to share weights and efficiently handle communication between GPUs.",1.0,1.0,0.8050616979598999
How does the addition of a Python interpreter in PAL's framework influence the accuracy of solutions?,"['times, but halfway through also\nsay quack” ).5. Results\n5.1. Math Results\nTable 1 shows the following results: across all tasks,\nPALusing Codex sets a new few-shot state-of-the-art top-\n1 decoding across all datasets, outperforming COTCodex,\nCOTPaLM-540 B, and COTMinerva 540B which was ﬁne-tuned\non explicit mathematical content.\nInterestingly, COTalso beneﬁts from Codex over PaLM-\n540 Bin some of the datasets such as ASDIV , but performs\nworse than PaLM-540 Bin others such as SVAMP . Yet,\nusing PALfurther improves the solve rate across all datasets.\nGSM-HARD On GSM -HARD (Table 1), the accuracy of\nDIRECT drops dramatically from 19.7% to 5.0% (a relative\ndrop of 74%), the accuracy of COTdrops from 65.6% to\n20.1% (a relative drop of almost 70%), while PALremains\nstable at 61.5%, dropping by only 14.3%. The results of\nCOTonGSM-HARD did not improve even when we replaced\nits prompts with prompts that include large numbers (Ap-\npendix B). This shows how PALprovides not only better\x0cPAL: Program-aided Language Models 6\nCOLORED OBJECT PENGUINS DATE REPEAT COPY OBJECT COUNTING\nDIRECT Codex 75.7 71.1 49.9 81.3 37.6\nCOTLaMDA-137B - - 26.8 - -\nCOTPaLM-540 B - 65.1 65.3 - -\nCOTCodex 86.3 79.2 64.8 68.8 73.0\nPALCodex 95.1 93.3 76.2 90.6 96.7\nTable 2: Solve rate on three symbolic reasoning datasets and two algorithmic datasets, In all datasets, PALachieves a much\nhigher accuracy than chain-of-thought. Results with closed models LaMDA-137B and PaLM-540B are included if available\nto public (Wei et al., 2022; Suzgun et al., 2022).\nresults on the standard benchmarks, but is also much more\nrobust . In fact, since PALofﬂoads the computation to the\nPython interpreter, any complex computation can be per-\nformed accurately given the correctly generated program.\nLarge Numbers or Incorrect Reasoning? Are the ', 'work with weaker models, while\nits beneﬁt scales elegantly to stronger models as well.\nDoes P AL work with LMs of natural language? We\nalso experimented with PALusing thetext-davinci\nseries. Figure 8 shows the following interesting re-\nsults: when the base LM’s “code modeling ability” is\nweak (using text-davinci-001 ),COTperforms better\nthan PAL. However, once the LM’s code modeling abil-\nity is sufﬁciently high (using text-davinci-002 and\ntext-davinci-003 ),PALoutperforms COT, and PAL\ntext-davinci-003 performs almost as PALcode-davinci-002 .\nThis shows that PALis not limited to LMs of code, but it\ncan work with LMs that were mainly trained for natural\nlanguage, if they have a sufﬁciently high coding ability.\nIs P AL better because of the Python prompt or because\nof the interpreter? We experimented with generating\nPython code, while requiring the neural LM to “execute” it\nas well, without using an interpreter, following Nye et al.\n(2021); Madaan et al. (2022). We created prompts that are\nsimilar to PAL’s, except that they do include the ﬁnal answer.\nThis resulted in a 23.2 solve rate on GSM 8K, much lower\nthan PAL(72.0), and only 4.5 points higher than DIRECT .\nThese results reinforce our hypothesis that the main beneﬁt\nofPALcomes from the synergy with the interpreter, andnot only from having a better prompt. Additional details\nare provided in Appendix B. For additional discussion on\nthe advantages of code-prompts over textual-prompts, see\nAppendix G.\nDo variable names matter? In all our experiments, we\nused meaningful variable names in the PALprompts, to ease\nthe model’s grounding of variables to the entities they rep-\nresent. For the Python interpreter, however, variable names\nare meaningless. To measure the importance of meaningful\nvariable names, we experimented with two prompts variants:\n1. P AL−comment – ', 'return a single line expression (see Figure 11b) to\ncalculate the result? Results in Table 6 ( 4throw) shows that is not the case. With single-line expressions, the performance of\nPAL falls to the level of direct prompting.\nGenerating the answer directly PALﬁrst generates a reasoning chain in the form of a Python program, and passes the\ngenerated program to a runtime to obtain an answer. Is PALbetter only because of the program-style intermediate reasoning\nchains, or are the improvements derived from ofﬂoading execution to the Python runtime? To investigate this, we experiment\nwith a variant that forces the LLM to generate the answer after generating the reasoning chain (Figure 11e). This setting\ncompels the LLM to condition on the generated code-based reasoning to generate an answer, simulating the runtime. The\nresults in Table 6 ( 5throw) show that the solve rate drops to near DIRECT levels. This reinforces our hypothesis that while\ncurrent LLMs can be excellent at specifying a high-level plan to solve a task—they are still incapable of executing them.\nAblation Solve Rate\nDIRECT (no intermediate reasoning) 19.7\nCOT 65.6\nPAL 72.0\nSuccinct Code 47.8\nLLM Simulating Runtime 23.2\nTable 6: Solve Rates for Ablations\nC. Effect of Using Language Models of Code\nIn our experiments, we focused on evaluating the performance of a language model for code. We aimed to investigate\nwhether the additional performance boost observed in our results was due to the use of models like Codex, or whether our\nformulation was useful even for text-based models. To this end, we conducted additional experiments using text-based\nlanguage models. Our ﬁndings indicate that the PAL approach is not restricted to working solely ']","Incorporating a Python interpreter allows PAL to execute generated programs accurately, bridging the gap in LLMs' arithmetic and logical capabilities, thus significantly improving solution accuracy compared to relying solely on LLMs.","The addition of a Python interpreter in PAL's framework greatly influences the accuracy of solutions, as it offloads computation to the interpreter and allows for complex computations to be performed accurately. The accuracy remains stable or only slightly drops when compared to other models, whereas models without the interpreter see a significant drop in accuracy.",1.0,1.0,0.718704879283905
How does CodeNet's dataset size and diversity support advanced AI for code research compared to previous datasets?,"['page 143–153, New\nYork, NY , USA, 2019. Association for Computing Machinery.\n[25] Wikipedia. Jaccard index — Wikipedia, the free encyclopedia. https://en.wikipedia.\norg/wiki/Jaccard_index , 2020.\n[26] Terence Parr. The Deﬁnitive ANTLR 4 Reference . Pragmatic Bookshelf, 2nd edition, 2013.\n[27] Sifei Luan, Di Yang, Celeste Barnaby, Koushik Sen, and Satish Chandra. Aroma: code recom-\nmendation via structural code search. Proceedings of the ACM on Programming Languages ,\n3(OOPSLA):1–28, Oct 2019.\n[28] IBM T.J. Watson Research Center. Wala. https://github.com/wala/WALA , 2021.\n[29] Forbes on codenet. https://www.forbes.com/sites/moorinsights/2021/06/04/ibm-\ncodenet-artificial-intelligence-that-can-program-computers-and-solve-a-\n100-billion-legacy-code-problem/?sh=343813636cdc .\n[30] Venturebeat on codenet. https://venturebeat.com/2021/05/10/ibms-codenet-\ndataset-aims-to-train-ai-to-tackle-programming-challenges/ .\n[31] Zdnet on codenet. https://www.zdnet.com/article/ibm-launches-autosql-watson-\norchestrate-codenet-enterprise-ai-tools-at-think/ .\n[32] Project codenet repository. https://github.com/IBM/Project_CodeNet .\n[33] Luca Buratti, Saurabh Pujar, Mihaela Bornea, Scott McCarley, Yunhui Zheng, Gaetano Rossiello,\nAlessandro Morari, Jim Laredo, Veronika Thost, Yufan Zhuang, and Giacomo Domeniconi.\nExploring software naturalness through neural language models, 2020.\n[34] Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional\nnetworks. In ICLR , 2017.\n[35] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\nnetworks? In ICLR , 2019.\n[36] Veronika Thost and Jie Chen. Directed acyclic graph neural networks. In ICLR , 2021.\n[37] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B.\nClement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,\nMichele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng,\nShengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code\nunderstanding and generation. CoRR , abs/2102.04664, 2021.\n[38] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks\nfor semi-supervised learning, 2018.\n[39] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele\nCatasta, and ', 'requirements change, make it more secure, and/or comply with regu-\nlations. These tasks are challenging, and it is crucial to provide tool support for developers to be\nmore productive at performing them. It is well known that the latest advancements in deep learning\nalgorithms rely on best-of-breed datasets, such as ImageNet, to create increasingly complex and\npowerful models. In this paper, we present ""CodeNet"", a ﬁrst-of-its-kind dataset in scale, diversity,\nand quality, to accelerate the algorithmic advances in AI for Code.\nTo promote widespread adoption of CodeNet, we will be launching contests involving use cases based\non the dataset. The ﬁrst contest [ 6] will focus on diversity, inclusion and spurring interest among\naspiring data scientists. We are partnering with the Global Women in Data Science organization (with\npresence in over 50 countries) founded by Stanford University [ 7] and targeting teams with at least\nﬁfty percent women. We are planning follow-up contests that target experienced AI practitioners.\nThe rest of the paper is organized as follows. Section 2 introduces the CodeNet dataset. Related\ndatasets are discussed in Section 3, and the differentiation of CodeNet with respect to these related\ndatasets is elaborated in Section 4. Section 5 describes how CodeNet was curated and Section 6\nenumerates the usability features of CodeNet with several pre-processing tools to transform source\ncodes into representations that can be readily used as inputs into machine learning models. Section 7\ndiscusses the upcoming CodeNet contest and Section 8 describes important baseline experiments\nwith the CodeNet dataset. Section 9 presents further uses of the CodeNet dataset and Section 10\nconcludes the paper.\n2 The CodeNet Dataset\nThe CodeNet dataset consists of a ', 'large collection of code samples with extensive metadata. It\nalso contains documented tools to transform code samples into intermediate representations and to\naccess the dataset and make tailored selections. Our goal is to provide the community with a large,\nhigh-quality curated dataset that can be used to advance AI techniques for source code.\nCodeNet is derived from the data available on two online judge websites: AIZU [ 8] and AtCoder [ 9].\nOnline judge websites pose programming problems in the form of courses and contests. The dataset\nconsists of submissions to these problems, which are judged by an automated review process for\ncorrectness. Problem descriptions, submission outcomes, and associated metadata are available via\nvarious REST APIs.\nScale and Statistics. CodeNet contains a total of 13,916,868 submissions, divided into 4053\nproblems. Among the submissions, 53.6% (7,460,588) are accepted (compilable and pass the\nprescribed tests), 29.5% are marked with wrong answer, and the remaining rejected due to their\nfailure to meet run time or memory requirements. To our knowledge, this is the largest dataset so\nfar among similar kinds. Submissions are in 55 different languages; 95% of them are coded in C++,\nPython, Java, C, Ruby, and C#. C++ is the most common language, with 8,008,527 submissions (57%\nof the total), of which 4,353,049 are accepted. With the abundance of code samples, users can extract\nlarge benchmark datasets that are customized to their downstream use. See Figure 1 for a summary.\nDiversity. The problems in CodeNet are mainly pedagogical and range from elementary exercises\nto sophisticated problems that require advanced algorithms. The submitters range from beginners\nto experienced coders. Some submissions are correct while others contain different ']","CodeNet surpasses previous datasets with over 14 million code samples and around 500 million lines of code in 55 languages, providing unparalleled scale and diversity for AI for code research.","CodeNet's dataset size and diversity is significantly larger and more diverse compared to previous datasets, such as ImageNet. This allows for more complex and powerful models to be created, accelerating algorithmic advances in AI for code. The dataset contains over 13 million submissions across 55 different programming languages, with a majority of submissions in C++, Python, Java, C, Ruby, and C#. This vast dataset enables researchers to extract large benchmark datasets customized to their specific research needs, providing a rich source of code samples for training and testing machine learning models. The problems in CodeNet range from elementary exercises to sophisticated problems, catering to a wide range of coding proficiency levels, from beginners to experienced coders. This diversity in problem types and submitters makes CodeNet a valuable resource for exploring various aspects of code understanding and generation with AI technologies.",1.0,1.0,0.8912047147750854
What methodologies were employed in SuperGLUE to establish robust human baseline performances across its tasks?,"['advances on GLUE, but\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\nC Human Performance Estimation\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\nannotation. For both steps and all tasks, the average pay rate is $23.75/hr.7\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\nare asked to annotate up to 30 examples from the development set. After answering each example,\nworkers are also asked to check their work against the provided ground truth label. After the training\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\nperformance at, or above the median performance across all workers during training.\n7This estimate is taken from https://turkerview.com .\n16\x0cIn the annotation phase, workers are provided with the same instructions as the training phase, and\nare linked to the same FAQ page. The instructions for all tasks are provided in Appendix C. For the\nannotation phase we randomly sample 100 examples from the task’s test set, with the exception of\nWSC where we annotate the full test set. For each example, we collect annotations from ﬁve workers\nand take a majority vote to estimate human performance. For additional details, see Appendix C.3.\nC.1 Training Phase Instructions\nIn the training step, we provide workers with brief instructions about the training phase. An example\nof these instructions is given Table 5. These training instructions ', 'et al., 2014). Finally, we list the best known\nresult on each task as of May 2019, except on tasks which we recast (WSC), resplit (CB), or achieve\n6For ReCoRD, we predict the entity that has the highest F1 with the other entity options.\n8\x0cthe best known result (WiC). The outside results for COPA, MultiRC, and RTE are from Sap et al.\n(2019), Trivedi et al. (2019), and Liu et al. (2019d) respectively.\nHuman Performance Pilehvar and Camacho-Collados (2019), Khashabi et al. (2018), Nangia and\nBowman (2019), and Zhang et al. (2018) respectively provide estimates for human performance\non WiC, MultiRC, RTE, and ReCoRD. For the remaining tasks, including the diagnostic set, we\nestimate human performance by hiring crowdworker annotators through Amazon’s Mechanical Turk\nplatform to reannotate a sample of each test set. We follow a two step procedure where a crowd\nworker completes a short training phase before proceeding to the annotation phase, modeled after the\nmethod used by Nangia and Bowman (2019) for GLUE. See Appendix C for details.\n5.2 Results\nTable 3 shows results for all baselines. The most frequent class and CBOW baselines do not perform\nwell overall, achieving near chance performance for several of the tasks. Using BERT increases\nthe average SuperGLUE score by 25 points, attaining signiﬁcant gains on all of the benchmark\ntasks, particularly MultiRC, ReCoRD, and RTE. On WSC, BERT actually performs worse than\nthe simple baselines, likely due to the small size of the dataset and the lack of data augmentation.\nUsing MultiNLI as an additional source of supervision for BoolQ, CB, and RTE leads to a 2-5 point\nimprovement on all tasks. Using SWAG as ', 'the nonbinding referendum favored statehood, an increase\nover the results of a 2012 referendum, ofﬁcial results from the State Electorcal Commission show. It\nwas the ﬁfth such vote on statehood. ""Today, we the people of Puerto Rico are sending a strong and\nclear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\nPuerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\nfavor of US statehood\nQuery For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\n<placeholder> presidency Correct Entities: USRTEText: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\naccording to the Christopher Reeve Foundation.\nHypothesis: Christopher Reeve had an accident. Entailment: FalseWiCContext 1: Room and board .Context 2: He nailed boards across the windows.\nSense match: FalseWSCText: Mark told Pete many lies about himself, which Pete included in his book. Heshould have been\nmore truthful. Coreference: False\n3 SuperGLUE Overview\n3.1 Design Process\nThe goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\nbeing applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\nwe identify the following desiderata of tasks in the benchmark:\nTask substance: Tasks should test a system’s ability to understand and reason about texts in English.\nTask difﬁculty: Tasks should be beyond the scope of current state-of-the-art systems, but solvable by\nmost college-educated English speakers. We exclude tasks that require domain-speciﬁc knowledge,\ne.g. medical notes or scientiﬁc papers.\nEvaluability: Tasks must have an automatic performance metric that ']","SuperGLUE's human baseline performances were established through careful collection of non-expert annotations for a sample of each test set, following structured annotation procedures that included a short training phase for annotators, ensuring reliable estimates of human performance.","For collecting data to establish human performance on the SuperGLUE tasks, a two-step procedure was followed. Workers were first provided with training before proceeding to annotation. In the training phase, workers were given instructions on the task, linked to an FAQ page, and asked to annotate examples from the development set. After the training phase, workers who annotated a minimum of five examples and achieved performance at or above the median were provided qualification to work on the annotation phase. In the annotation phase, workers were provided with the same instructions as the training phase and were linked to the same FAQ page. Annotations from five workers were collected for each example, and a majority vote was taken to estimate human performance.",1.0,1.0,0.8545346260070801
How do the selected tasks in SuperGLUE reflect the benchmark's goals for advancing language understanding technologies?,"['high-level goals, the original version of\nthe benchmark is no longer a suitable metric for quantifying such progress.\nIn response, we introduce SuperGLUE, a new benchmark designed to pose a more rigorous test of\nlanguage understanding. SuperGLUE has the same high-level motivation as GLUE: to provide a\nsimple, hard-to-game measure of progress toward general-purpose language understanding technolo-\ngies for English. We anticipate that signiﬁcant progress on SuperGLUE should require substantive\ninnovations in a number of core areas of machine learning, including sample-efﬁcient, transfer,\nmultitask, and unsupervised or self-supervised learning.\nSuperGLUE follows the basic design of GLUE: It consists of a public leaderboard built around\neight language understanding tasks, drawing on existing data, accompanied by a single-number\nperformance metric, and an analysis toolkit. However, it improves upon GLUE in several ways:\nMore challenging tasks: SuperGLUE retains the two hardest tasks in GLUE. The remaining tasks\nwere identiﬁed from those submitted to an open call for task proposals and were selected based on\ndifﬁculty for current NLP approaches.\nMore diverse task formats: The task formats in GLUE are limited to sentence- and sentence-pair\nclassiﬁcation. We expand the set of task formats in SuperGLUE to include coreference resolution\nand question answering (QA).\nComprehensive human baselines: We include human performance estimates for all benchmark\ntasks, which verify that substantial headroom exists between a strong BERT-based baseline and\nhuman performance.\nImproved code support: SuperGLUE is distributed with a new, modular toolkit for work on\npretraining, multi-task learning, and transfer learning in NLP, built around standard tools including\nPyTorch (Paszke et al., 2017) and AllenNLP (Gardner et al., 2017).\nReﬁned usage rules: The conditions for inclusion on the SuperGLUE leaderboard have been\nrevamped to ensure fair ', ""Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman.\nSuperglue: A stickier benchmark for general-purpose language understanding systems, 2019.\nR. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi. Hellaswag: Can a machine really ﬁnish\nyour sentence?, 2019.\nR. Zellers, A. Holtzman, E. Clark, L. Qin, A. Farhadi, and Y . Choi. Evaluating machines by their\nreal-world language use, 2020.\n10\x0cPublished as a conference paper at ICLR 2021\nA A DDITIONAL ANALYSIS\nThis appendix includes ﬁgures with sorted results (Figure 9), few-shot examples vs. accuracy\n(Figure 10), and few-shot calibration (Figure 11). It also includes sections on ﬁne-tuning, error\nanalysis, and format sensitivity.\n0 20 40 60 80 100\nAccuracy (%)College ChemistryMoral ScenariosCollege PhysicsHigh School PhysicsHigh School MathematicsFormal LogicElementary MathematicsAbstract AlgebraHigh School StatisticsMachine LearningEconometricsHigh School ChemistryProfessional AccountingProfessional LawCollege MathematicsProfessional MedicineConceptual PhysicsGlobal FactsHigh School Comp SciMedical GeneticsHigh School MacroeconomicsHigh School MicroeconomicsMoral DisputesProfessional PsychologyCollege BiologyVirologyCollege Comp SciBusiness EthicsNutritionCollege MedicineAnatomyClinical KnowledgeLogical FallaciesHigh School BiologyPublic RelationsAstronomyElectrical EngineeringHuman AgingPhilosophySecurity StudiesPrehistoryHigh School US HistorySociologyHigh School European HistoryHuman SexualityJurisprudenceWorld ReligionsInternational LawHigh School World HistoryManagementComputer SecurityHigh School GeographyHigh School Gov't and PoliticsMarketingMiscellaneousHigh School PsychologyUS Foreign PolicyGPT-3 Results\nRandom Chance\n0102030405060708090100\nAccuracy (%)Moral ScenariosFormal LogicAbstract AlgebraEconometricsHigh School MathematicsCollege PhysicsMachine LearningHigh School StatisticsCollege ChemistryElementary MathematicsCollege MathematicsHigh School ChemistryGlobal FactsProfessional LawMedical GeneticsProfessional AccountingCollege BiologyHigh School PhysicsAnatomyCollege Comp SciConceptual PhysicsCollege MedicineVirologyProfessional MedicineAstronomyHigh School MacroeconomicsElectrical EngineeringProfessional PsychologySecurity StudiesHuman SexualityNutritionHigh School Comp SciPrehistoryClinical KnowledgeHigh School BiologyHuman AgingHigh School MicroeconomicsPhilosophyPublic RelationsWorld ReligionsMoral DisputesLogical FallaciesHigh School European HistoryHigh School US HistoryHigh School World HistoryMiscellaneousUS Foreign PolicyComputer SecuritySociologyInternational LawHigh School GeographyJurisprudenceBusiness EthicsHigh School PsychologyManagementHigh School Gov't and PoliticsMarketingUnifiedQA Results\nRandom Chance\nFigure 9: On the left are GPT-3 few shot accuracies for all of the "", 'SuperGLUE: A Stickier Benchmark for\nGeneral-Purpose Language Understanding Systems\nAlex Wang∗\nNew York UniversityYada Pruksachatkun∗\nNew York UniversityNikita Nangia∗\nNew York University\nAmanpreet Singh∗\nFacebook AI ResearchJulian Michael\nUniversity of WashingtonFelix Hill\nDeepMindOmer Levy\nFacebook AI Research\nSamuel R. Bowman\nNew York University\nAbstract\nIn the last year, new models and methods for pretraining and transfer learning have\ndriven striking performance improvements across a range of language understand-\ning tasks. The GLUE benchmark, introduced a little over one year ago, offers\na single-number metric that summarizes progress on a diverse set of such tasks,\nbut performance on the benchmark has recently surpassed the level of non-expert\nhumans, suggesting limited headroom for further research. In this paper we present\nSuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁ-\ncult language understanding tasks, a software toolkit, and a public leaderboard.\nSuperGLUE is available at super.gluebenchmark.com .\n1 Introduction\nRecently there has been notable progress across many natural language processing (NLP) tasks, led\nby methods such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT\n(Devlin et al., 2019). The unifying theme of these methods is that they couple self-supervised learning\nfrom massive unlabelled text corpora with effective adapting of the resulting model to target tasks.\nThe tasks that have proven amenable to this general approach include question answering, textual\nentailment, and parsing, among many others (Devlin et al., 2019; Kitaev et al., 2019, i.a.).\nIn this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation\nframework for research towards general-purpose language understanding technologies. GLUE is\na collection of nine language understanding tasks built on existing public datasets, together with\nprivate test data, an evaluation server, a single-number ']","The selected tasks, chosen for their difficulty and diversity, reflect SuperGLUE's goals by requiring substantive innovations in machine learning to improve sample-efficient transfer, multitask, and unsupervised learning, driving forward the capabilities of language understanding systems.","The selected tasks in SuperGLUE reflect the benchmark's goals for advancing language understanding technologies by including more challenging tasks, introducing more diverse task formats (such as coreference resolution and question answering), providing comprehensive human baselines for benchmark tasks, improving code support with a modular toolkit, and refining usage rules to ensure fairness.",1.0,1.0,0.8140982985496521
"In the context of PAL, what role do meaningful variable names play in the generated program's effectiveness?","['the PALprompt without intermediate\nNL comments.\n2. P AL−var\n−comment – the PALprompt without intermediate\nNL comments and with variable names substituted\nwith random characters.\nThe results are shown in Figure 9. In COLORED OBJECTED\nandDATE, removing intermediate NL comments but keep-\ning meaningful variable names ( PAL−comment ) – slightly re-\nduces the results compared to the full PALprompt, but it still\nachieves higher accuracy than the baselines COT. Remov-\ning variable names as well ( PAL−var\n−comment ) further decreases\naccuracy, and performs worse than COT. Since variable\nnames have an important part in code quality (Gellenbeck\n& Cook, 1991; Takang et al., 1996), meaningful variable\nnames are only expected to ease reasoning for Codex, which\nwas trained on mostly meaningful names, as was also found\nby Madaan et al. (2022).\n7. Related Work\nPrompting Few-shot prompting (Brown et al., 2020) has\nbeen shown to be an effective approach for a variety of\ntasks (Liu et al., 2021) ranging from text- (Gehrmann et al.,\x0cPAL: Program-aided Language Models 8\nColored Objects Date Penguins60708090100\n84.4\n64.879.295.2\n76.293.391.1\n69.191.3\n79.9\n63.491.9COT PAL PAL−comment PAL−var\n−comment\nFigure 9: Ablation study of PALprompt formats. We consider the original PALprompt, it with natural language comments\nremoved ( PAL−comment ), and further variable names replaced with random character ( PAL−var\n−comment ). As a reference, we also\nshow the C OT performance (blue).\n2021; Reif et al., 2021; Wei et al., 2021; Sanh et al., 2021)\nto code-generation (Chen et al., 2021b). Methods such as\nchain-of-thought prompting ( COT) have further unlocked a\nvariety of reasoning tasks, boosting the performance of mod-\nels on a variety of benchmarks. Nevertheless, all previous\napproaches suffer from inaccuracy in arithmetic calculation\nand incorrect reasoning (Lewkowycz et al., 2022; Hendrycks\net al., 2021; Madaan & Yazdanbakhsh, 2022). ', '23.7%. Similarly, PAL\nvastly outperforms COTby absolute 21.8% on REPEAT\nCOPY. Surprisingly, DIRECT prompting performs better\nthan COTonREPEAT COPY. Yet, PALimproves over\nDIRECT by 9.3% in R EPEAT COPY.\n[0,2] [3,5] [6,8] [9,11] [12,14] [15,17] [18,20] [21,23] [24,26]0.60.81\nNumber of ObjectsAccuracy\nPaL\nCoT\nFigure 6: The solve rate on COLORED OBJECTS with re-\nspect to the number of objects included in the test question.\nIs PAL sensitive to the complexity of the question? We\nexamined how the performance of PALandCOTchange as\nthe complexity of the input question grows, measured as the\nnumber of objects in the question of COLORED OBJECTS .\nAs shown in Figure 6, PALis superior COTacross all input\nlengths. As the number of objects in the question increases,\nCOT’s accuracy is unstable and drops, while PALremains\nconsistently close to 100%. More analysis on the token-level\npredictions can be found in Appendix G.\x0cPAL: Program-aided Language Models 7\ncode-cushman-001 code-davinci-001 code-davinci-002020406080\n21.731.872.0\n19.126.060.1\n13.6%22.3%19.8%Solve ratePAL\nCOT\nRelative Improvement\nFigure 7: PALwith different models on GSM8K: though\nthe absolute accuracies with code-cushman-001\nandcode-davinci-001 are lower than\ncode-davinci-002 , the relative improvement of\nPAL over C OT is consistent across models.text-davinci-001 text-davinci-002 text-davinci-003020406080\n26.546.965.3\n8.665.869.8 COT PAL\nFigure 8: PALwith NL LMs on GSM 8K: though\nCOToutperforms PALwithtext-davinci-001 , once\nthe base LM is sufﬁciently strong, PALis beneﬁcial\nwithtext-davinci-002 andtext-davinci-003\nas well. That is, P AL is not limited to code-LMs only.\n6. Analysis\nDoes P AL work with weaker LMs? In all our experi-\nments in Section 5, PALused thecode-davinci-002\nmodel; but can PALwork with weaker models of code? We\ncompared PALwith COTwhen both prompting approaches\nuse the same weaker base LMs code-cushman-001\nandcode-davinci-001 . As shown in Figure 7, even\nthough the absolute accuracies of code-cushman-001\nandcode-davinci-001 are lower, the relative improve-\nment of PALover COTremains consistent across models.\nThis shows that PALcan ', 'of 3 tennis balls each is 6 , in PALwe also aug-\nment each such NL step with its corresponding pro-\ngrammatic statement such as tennis balls = 5 and\nbought balls = 2 *3. This way, the model learns\nto generate a program that will provide the answer for the\ntest question, instead of relying on LLM to perform the\ncalculation correctly.\nWe prompt the language model to generate NL intermediate\nsteps using comment syntax (e.g. “ # ... ” in Python)\nsuch they will be ignored by the interpreter. We pass the\ngenerated program ttestto its corresponding solver, we run\nit, and obtain the ﬁnal run result ytest. In this work we use\na standard Python interpreter, but this can be any solver,\ninterpreter or a compiler.\nCrafting prompts for P AL In our experiments, we lever-\naged the prompts of existing work whenever available, and\notherwise randomly selected the same number (3-6) of ex-\namples as previous work for creating a ﬁxed prompt for\nevery benchmark. In all cases, we augmented the free-form\ntext prompts into PAL-styled prompts, leveraging program-\nming constructs such as for loops and dictionaries when\nneeded. Generally, writing P AL prompts is easy and quick.\nWe also ensure that variable names in the prompt mean-\ningfully reﬂect their roles. For example, a variable that\ndescribes the number of apples in the basket should have a\nname such as numapplesinbasket . This keeps the\ngenerated code linked to the entities in the question. In\nSection 6 we show that such meaningful variable names are\ncritical. Notably, it is also possible to incrementally run\nthe PL segments and feed the execution results back to the\nLLM to generate the following blocks. ']","Meaningful variable names in PAL's generated code are crucial for linking programmatic steps to natural language problem entities, enhancing readability and maintainability of the code, and improving the model's reasoning and execution accuracy.","In the context of PAL, meaningful variable names play a critical role in the generated program's effectiveness. By ensuring that variable names in the prompt reflect their roles accurately, such as describing the entities in the question, the generated code remains linked to the context. This enhances the model's ability to generate a program that provides the correct answer for the test question, rather than relying solely on the language model to perform calculations accurately.",1.0,1.0,0.8564838171005249
How does PAL address the execution of complex computations in natural language processing tasks?,"['work with weaker models, while\nits beneﬁt scales elegantly to stronger models as well.\nDoes P AL work with LMs of natural language? We\nalso experimented with PALusing thetext-davinci\nseries. Figure 8 shows the following interesting re-\nsults: when the base LM’s “code modeling ability” is\nweak (using text-davinci-001 ),COTperforms better\nthan PAL. However, once the LM’s code modeling abil-\nity is sufﬁciently high (using text-davinci-002 and\ntext-davinci-003 ),PALoutperforms COT, and PAL\ntext-davinci-003 performs almost as PALcode-davinci-002 .\nThis shows that PALis not limited to LMs of code, but it\ncan work with LMs that were mainly trained for natural\nlanguage, if they have a sufﬁciently high coding ability.\nIs P AL better because of the Python prompt or because\nof the interpreter? We experimented with generating\nPython code, while requiring the neural LM to “execute” it\nas well, without using an interpreter, following Nye et al.\n(2021); Madaan et al. (2022). We created prompts that are\nsimilar to PAL’s, except that they do include the ﬁnal answer.\nThis resulted in a 23.2 solve rate on GSM 8K, much lower\nthan PAL(72.0), and only 4.5 points higher than DIRECT .\nThese results reinforce our hypothesis that the main beneﬁt\nofPALcomes from the synergy with the interpreter, andnot only from having a better prompt. Additional details\nare provided in Appendix B. For additional discussion on\nthe advantages of code-prompts over textual-prompts, see\nAppendix G.\nDo variable names matter? In all our experiments, we\nused meaningful variable names in the PALprompts, to ease\nthe model’s grounding of variables to the entities they rep-\nresent. For the Python interpreter, however, variable names\nare meaningless. To measure the importance of meaningful\nvariable names, we experimented with two prompts variants:\n1. P AL−comment – ', '23.7%. Similarly, PAL\nvastly outperforms COTby absolute 21.8% on REPEAT\nCOPY. Surprisingly, DIRECT prompting performs better\nthan COTonREPEAT COPY. Yet, PALimproves over\nDIRECT by 9.3% in R EPEAT COPY.\n[0,2] [3,5] [6,8] [9,11] [12,14] [15,17] [18,20] [21,23] [24,26]0.60.81\nNumber of ObjectsAccuracy\nPaL\nCoT\nFigure 6: The solve rate on COLORED OBJECTS with re-\nspect to the number of objects included in the test question.\nIs PAL sensitive to the complexity of the question? We\nexamined how the performance of PALandCOTchange as\nthe complexity of the input question grows, measured as the\nnumber of objects in the question of COLORED OBJECTS .\nAs shown in Figure 6, PALis superior COTacross all input\nlengths. As the number of objects in the question increases,\nCOT’s accuracy is unstable and drops, while PALremains\nconsistently close to 100%. More analysis on the token-level\npredictions can be found in Appendix G.\x0cPAL: Program-aided Language Models 7\ncode-cushman-001 code-davinci-001 code-davinci-002020406080\n21.731.872.0\n19.126.060.1\n13.6%22.3%19.8%Solve ratePAL\nCOT\nRelative Improvement\nFigure 7: PALwith different models on GSM8K: though\nthe absolute accuracies with code-cushman-001\nandcode-davinci-001 are lower than\ncode-davinci-002 , the relative improvement of\nPAL over C OT is consistent across models.text-davinci-001 text-davinci-002 text-davinci-003020406080\n26.546.965.3\n8.665.869.8 COT PAL\nFigure 8: PALwith NL LMs on GSM 8K: though\nCOToutperforms PALwithtext-davinci-001 , once\nthe base LM is sufﬁciently strong, PALis beneﬁcial\nwithtext-davinci-002 andtext-davinci-003\nas well. That is, P AL is not limited to code-LMs only.\n6. Analysis\nDoes P AL work with weaker LMs? In all our experi-\nments in Section 5, PALused thecode-davinci-002\nmodel; but can PALwork with weaker models of code? We\ncompared PALwith COTwhen both prompting approaches\nuse the same weaker base LMs code-cushman-001\nandcode-davinci-001 . As shown in Figure 7, even\nthough the absolute accuracies of code-cushman-001\nandcode-davinci-001 are lower, the relative improve-\nment of PALover COTremains consistent across models.\nThis shows that PALcan ', 'PALavoids\nthese problems by ofﬂoading the calculation and some of\nthe reasoning to a Python interpreter, which is correct by\nconstruction, given the right program. Further, not only\nthatPALcan improve the standard chain-of-thought, it can\nimprove least-to-most prompting (Zhou et al., 2022) as well,\nas we show in Appendix I.\nLMs with external tools Several prior works have\nequipped neural models with specialized modules. For ex-\nample, Cobbe et al. (2021) employ a calculator for arith-\nmetic operations as a post hoc processing, and Demeter\n& Downey (2020) add specialized modules for generating\ncities and dates. Unlike these works, PALgenerates code\nfor a Python interpreter, which is general enough to handle\nboth arithmetic calculations and dates, without specialized\nmodules and ad-hoc ﬁxes. Chowdhery et al. (2022) and Wei\net al. (2022) have also experimented with external calcula-\ntors; however, the calculator had improved Codex by only\n2.3% (absolute) on GSM 8Kand improved PaLM-540 Bby\n1.7%, while PALimproves Codex by 6.4% on the same\nbenchmark (Section 5.1). Similarly to our work, Chowd-\nhery et al. (2022) have also experimented with generating\nPython code for solving the GSM 8Kbenchmark, but their\nexperiments resulted in lower accuracy than the standard\nPaLM-540 Bthat uses chain-of-thought. Pi et al. (2022)\npretrain the model on execution results of random expres-\nsions on a calculator, instead of using the solver at test time\nas well. While their model can hypothetically perform arith-\nmetic better than other pretrained LMs, their results on the\nSV AMP benchmark are much lower: 57.4% using a T5-11Bmodel, while PAL achieves 79.4% on the same benchmark\nwithout any specialized pretraining.\nShortly after a preprint of our work was submitted to arXiv,\nanother related work on “program of thought prompting”\n(Chen et al., ']"," PAL addresses complex computations by generating programmatic reasoning steps for LLMs and executing them via an interpreter, ensuring accurate and efficient problem-solving acros various domains.","PAL addresses the execution of complex computations in natural language processing tasks by offloading the calculation and reasoning to a Python interpreter. This allows PAL to improve the standard chain-of-thought as well as other prompting approaches, such as least-to-most prompting. Additionally, PAL generates code for a Python interpreter, which is able to handle a wide range of computations without the need for specialized modules or ad-hoc fixes, thus improving the accuracy and efficiency of executing computations in natural language processing tasks.",1.0,1.0,0.6573118567466736
How does SuperGLUE's design ensure it presents a more challenging benchmark than its predecessor GLUE for evaluating language understanding models?,"['quality of research contributions aimed at the challenges posed by GLUE\nunderline the utility of this style of benchmark for machine learning researchers looking to evaluate\nnew application-agnostic methods on language understanding.\nLimits to current approaches are also apparent via the GLUE suite. Performance on the GLUE\ndiagnostic entailment dataset, at 0.42 R3, falls far below the average human performance of 0.80\nR3reported in the original GLUE publication, with models performing near, or even below, chance\non some linguistic phenomena (Figure 2, Appendix B). While some initially difﬁcult categories\nsaw gains from advances on GLUE (e.g., double negation), others remain hard (restrictivity) or\neven adversarial (disjunction, downward monotonicity). This suggests that even as unsupervised\npretraining produces ever-better statistical summaries of text, it remains difﬁcult to extract many\ndetails crucial to semantics without the right kind of supervision. Much recent work has made similar\nobservations about the limitations of existing pretrained models (Jia and Liang, 2017; Naik et al.,\n2018; McCoy and Linzen, 2019; McCoy et al., 2019; Liu et al., 2019a,b).\n3\x0cTable 2: Development set examples from the tasks in SuperGLUE. Bold text represents part of the\nexample format for each task. Text in italics is part of the model input. Underlined text is specially\nmarked in the input. Text in a monospaced font represents the expected model output.BoolQPassage: Barq’s – Barq’s is an American soft drink. Its brand of root beer is notable for having caffeine.\nBarq’s, created by Edward Barq and bottled since the turn of the 20th century, is owned by the Barq\nfamily but bottled by the Coca-Cola Company. It was known as Barq’s Famous Olde Tyme Root Beer\nuntil ', ""Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman.\nSuperglue: A stickier benchmark for general-purpose language understanding systems, 2019.\nR. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi. Hellaswag: Can a machine really ﬁnish\nyour sentence?, 2019.\nR. Zellers, A. Holtzman, E. Clark, L. Qin, A. Farhadi, and Y . Choi. Evaluating machines by their\nreal-world language use, 2020.\n10\x0cPublished as a conference paper at ICLR 2021\nA A DDITIONAL ANALYSIS\nThis appendix includes ﬁgures with sorted results (Figure 9), few-shot examples vs. accuracy\n(Figure 10), and few-shot calibration (Figure 11). It also includes sections on ﬁne-tuning, error\nanalysis, and format sensitivity.\n0 20 40 60 80 100\nAccuracy (%)College ChemistryMoral ScenariosCollege PhysicsHigh School PhysicsHigh School MathematicsFormal LogicElementary MathematicsAbstract AlgebraHigh School StatisticsMachine LearningEconometricsHigh School ChemistryProfessional AccountingProfessional LawCollege MathematicsProfessional MedicineConceptual PhysicsGlobal FactsHigh School Comp SciMedical GeneticsHigh School MacroeconomicsHigh School MicroeconomicsMoral DisputesProfessional PsychologyCollege BiologyVirologyCollege Comp SciBusiness EthicsNutritionCollege MedicineAnatomyClinical KnowledgeLogical FallaciesHigh School BiologyPublic RelationsAstronomyElectrical EngineeringHuman AgingPhilosophySecurity StudiesPrehistoryHigh School US HistorySociologyHigh School European HistoryHuman SexualityJurisprudenceWorld ReligionsInternational LawHigh School World HistoryManagementComputer SecurityHigh School GeographyHigh School Gov't and PoliticsMarketingMiscellaneousHigh School PsychologyUS Foreign PolicyGPT-3 Results\nRandom Chance\n0102030405060708090100\nAccuracy (%)Moral ScenariosFormal LogicAbstract AlgebraEconometricsHigh School MathematicsCollege PhysicsMachine LearningHigh School StatisticsCollege ChemistryElementary MathematicsCollege MathematicsHigh School ChemistryGlobal FactsProfessional LawMedical GeneticsProfessional AccountingCollege BiologyHigh School PhysicsAnatomyCollege Comp SciConceptual PhysicsCollege MedicineVirologyProfessional MedicineAstronomyHigh School MacroeconomicsElectrical EngineeringProfessional PsychologySecurity StudiesHuman SexualityNutritionHigh School Comp SciPrehistoryClinical KnowledgeHigh School BiologyHuman AgingHigh School MicroeconomicsPhilosophyPublic RelationsWorld ReligionsMoral DisputesLogical FallaciesHigh School European HistoryHigh School US HistoryHigh School World HistoryMiscellaneousUS Foreign PolicyComputer SecuritySociologyInternational LawHigh School GeographyJurisprudenceBusiness EthicsHigh School PsychologyManagementHigh School Gov't and PoliticsMarketingUnifiedQA Results\nRandom Chance\nFigure 9: On the left are GPT-3 few shot accuracies for all of the "", 'target metric, and an accompanying expert-\nconstructed diagnostic set. GLUE was designed to provide a general-purpose evaluation of language\nunderstanding that covers a range of training data volumes, task genres, and task formulations. We\nbelieve it was these aspects that made GLUE particularly appropriate for exhibiting the transfer-\nlearning potential of approaches like OpenAI GPT and BERT.\nThe progress of the last twelve months has eroded headroom on the GLUE benchmark dramatically.\nWhile some tasks (Figure 1) and some linguistic phenomena (Figure 2 in Appendix B) measured\nin GLUE remain difﬁcult, the current state of the art GLUE Score as of early July 2019 (88.4 from\nYang et al., 2019) surpasses human performance (87.1 from Nangia and Bowman, 2019) by 1.3\npoints, and in fact exceeds this human performance estimate on four tasks. Consequently, while there\n∗Equal contribution. Correspondence: glue-benchmark-admin@googlegroups.com\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1905.00537v3 [cs.CL] 13 Feb 2020\x0cBiLSTM+ELMo+Attn\nOpenAI GPT\nBERT + Single-task Adapters\nBERT (Large)\nBERT on STILTs\nBERT + BAM\nSemBERT\nSnorkel MeTaL\nALICE (Large)\nMT-DNN (ensemble)\nXLNet-Large (ensemble)0.50.60.70.80.91.01.11.2\nGLUE Score\nHuman Performance\nCoLA\nSST-2MRPC\nSTS-B\nQQP\nMNLIQNLI\nRTE\nWNLIFigure 1: GLUE benchmark performance for submitted systems, rescaled to set human performance\nto 1.0, shown as a single number score, and broken down into the nine constituent task performances.\nFor tasks with multiple metrics, we use an average of the metrics. More information on the tasks\nincluded in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2019, CoLA), Socher\net al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al.\n(2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI).\nremains substantial scope for improvement towards GLUE’s ']","SuperGLUE introduces more challenging tasks retained from GLUE, diversifies task formats beyond sentence- and sentence-pair classification to include coreference resolution and question answering, and features comprehensive human baseline performances to ensure significant headroom for model improvement.","SuperGLUE's design ensures it presents a more challenging benchmark than its predecessor GLUE for evaluating language understanding models by including sorted results, few-shot examples vs. accuracy, few-shot calibration, sections on fine-tuning, error analysis, and format sensitivity. Additionally, SuperGLUE includes examples from tasks in the SuperGLUE suite that are formatted to represent model input, model output, and specially marked text, making the evaluation more rigorous and comprehensive.",1.0,1.0,0.7315114140510559
" In what ways does SuperGLUE expand beyond GLUE's task formats, and why is this expansion significant?","['advances on GLUE, but\nothers remain hard (restrictivity) or even adversarial (disjunction, downward monotone).\nC Human Performance Estimation\nFor collecting data to establish human performance on the SuperGLUE tasks, we follow a two\nstep procedure where we ﬁrst provide some training to the crowd workers before they proceed to\nannotation. For both steps and all tasks, the average pay rate is $23.75/hr.7\nIn the training phase, workers are provided with instructions on the task, linked to an FAQ page, and\nare asked to annotate up to 30 examples from the development set. After answering each example,\nworkers are also asked to check their work against the provided ground truth label. After the training\nphase is complete, we provide the qualiﬁcation to work on the annotation phase to all workers\nwho annotated a minimum of ﬁve examples, i.e. completed ﬁve HITs during training and achieved\nperformance at, or above the median performance across all workers during training.\n7This estimate is taken from https://turkerview.com .\n16\x0cIn the annotation phase, workers are provided with the same instructions as the training phase, and\nare linked to the same FAQ page. The instructions for all tasks are provided in Appendix C. For the\nannotation phase we randomly sample 100 examples from the task’s test set, with the exception of\nWSC where we annotate the full test set. For each example, we collect annotations from ﬁve workers\nand take a majority vote to estimate human performance. For additional details, see Appendix C.3.\nC.1 Training Phase Instructions\nIn the training step, we provide workers with brief instructions about the training phase. An example\nof these instructions is given Table 5. These training instructions ', 'corresponds well to human\njudgments of output quality. Some text generation tasks fail to meet this criteria due to issues with\nautomatic metrics like ROUGE and BLEU (Callison-Burch et al., 2006; Liu et al., 2016, i.a.).\n4\x0cPublic data: We require that tasks have existing public training data in order to minimize the risks\ninvolved in newly-created datasets. We also prefer tasks for which we have access to (or could create)\na test set with private labels.\nTask format: We prefer tasks that had relatively simple input and output formats, to avoid incentiviz-\ning the users of the benchmark to create complex task-speciﬁc model architectures. Still, while GLUE\nis restricted to tasks involving single sentence or sentence pair inputs, for SuperGLUE we expand\nthe scope to consider tasks with longer inputs. This yields a set of tasks that requires understanding\nindividual tokens in context, complete sentences, inter-sentence relations, and entire paragraphs.\nLicense: Task data must be available under licences that allow use and redistribution for research\npurposes.\nTo identify possible tasks for SuperGLUE, we disseminated a public call for task proposals to the\nNLP community, and received approximately 30 proposals. We ﬁltered these proposals according\nto our criteria. Many proposals were not suitable due to licensing issues, complex formats, and\ninsufﬁcient headroom; we provide examples of such tasks in Appendix D. For each of the remaining\ntasks, we ran a BERT-based baseline and a human baseline, and ﬁltered out tasks which were either\ntoo challenging for humans without extensive training or too easy for our machine baselines.\n3.2 Selected Tasks\nFollowing this process, we arrived at eight tasks to use in SuperGLUE. See Tables 1 and 2 ', 'the nonbinding referendum favored statehood, an increase\nover the results of a 2012 referendum, ofﬁcial results from the State Electorcal Commission show. It\nwas the ﬁfth such vote on statehood. ""Today, we the people of Puerto Rico are sending a strong and\nclear message to the US Congress ... and to the world ... claiming our equal rights as American citizens,\nPuerto Rico Gov. Ricardo Rossello said in a news release. @highlight Puerto Rico voted Sunday in\nfavor of US statehood\nQuery For one, they can truthfully say, “Don’t blame me, I didn’t vote for them, ” when discussing the\n<placeholder> presidency Correct Entities: USRTEText: Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44,\naccording to the Christopher Reeve Foundation.\nHypothesis: Christopher Reeve had an accident. Entailment: FalseWiCContext 1: Room and board .Context 2: He nailed boards across the windows.\nSense match: FalseWSCText: Mark told Pete many lies about himself, which Pete included in his book. Heshould have been\nmore truthful. Coreference: False\n3 SuperGLUE Overview\n3.1 Design Process\nThe goal of SuperGLUE is to provide a simple, robust evaluation metric of any method capable of\nbeing applied to a broad range of language understanding tasks. To that end, in designing SuperGLUE,\nwe identify the following desiderata of tasks in the benchmark:\nTask substance: Tasks should test a system’s ability to understand and reason about texts in English.\nTask difﬁculty: Tasks should be beyond the scope of current state-of-the-art systems, but solvable by\nmost college-educated English speakers. We exclude tasks that require domain-speciﬁc knowledge,\ne.g. medical notes or scientiﬁc papers.\nEvaluability: Tasks must have an automatic performance metric that ']","By including coreference resolution and question answering formats, SuperGLUE expands the scope of evaluated linguistic phenomena, testing models' abilities in more varied contexts and reflecting a broader range of natural language understanding challenges.","SuperGLUE expands beyond GLUE's task formats by considering tasks with longer inputs, requiring understanding of individual tokens in context, complete sentences, inter-sentence relations, and entire paragraphs. This expansion is significant because it allows for a more comprehensive evaluation of a system's ability to understand and reason about texts in English, as opposed to just single sentence or sentence pair inputs.",1.0,1.0,0.7704882025718689
