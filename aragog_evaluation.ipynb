{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de766774-9542-4798-987f-60ede7e7e9ca",
   "metadata": {},
   "source": [
    "In this tutorial, we will show you how to use Haystack to evaluate the performance of a RAG pipeline.\n",
    "\n",
    "We will use the dataset from the [ARAGOG - Advanced Retrieval Augmented Generation Output Grading (ARAGOG)](https://arxiv.org/pdf/2404.01037) paper.\n",
    "The dataset is composed of a collection of 13 public AI/LLM-ArXiv research papers and 107 question-answer (QA) pairs. The (QA) pairs generated with the assistance of GPT-4, and then each pair was validated/corrected by humans.\n",
    "\n",
    "We will use the following Haystack components to evaluate the performance of a RAG pipeline: \n",
    "\n",
    "- [ContextRelevance](https://docs.haystack.deepset.ai/docs/contextrelevanceevaluator)\n",
    "- [Faithfulness](https://docs.haystack.deepset.ai/docs/faithfulnessevaluator)\n",
    "- [Semantic Answer Similarity](https://docs.haystack.deepset.ai/docs/sasevaluator)\n",
    "\n",
    "\n",
    "We will build a RAG pipeline and then evaluate it using the ARAGOG dataset by varying three parameters:\n",
    "\n",
    "- `top_k`: the maximum number of documents returned by the retriever\n",
    "- `embedding_model`: the model used to encode the documents and the question\n",
    "- `chunk_size`: the number of tokens in the input text that the model can process at once\n",
    "\n",
    "More specifically, we will evaluate the pipeline using the following values for each parameter:\n",
    "\n",
    "```\n",
    "embedding_models = {\n",
    "        \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"sentence-transformers/msmarco-distilroberta-base-v2\",\n",
    "        \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    }\n",
    "\n",
    "top_k_values = [1, 2, 3]\n",
    "chunk_sizes = [64, 128, 256]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aacedb7f-f795-4fcc-a2cb-dc07cbb31a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from openai import BadRequestError\n",
    "\n",
    "from haystack.components.evaluators import ContextRelevanceEvaluator, FaithfulnessEvaluator, SASEvaluator\n",
    "from haystack.evaluation import EvaluationRunResult\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a3ed6-8a7b-47e7-b550-211d7a3511d2",
   "metadata": {},
   "source": [
    "## We need to define a function to load the dataset the questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c981a088-e2bf-4ca5-b846-5e926874492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Tuple, List\n",
    "\n",
    "def read_question_answers() -> Tuple[List[str], List[str]]:\n",
    "    with open(\"datasets/ARAGOG/eval_questions.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        questions = data[\"questions\"]\n",
    "        answers = data[\"ground_truths\"]\n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4558007-0416-4902-99d6-e6834ec28255",
   "metadata": {},
   "source": [
    "## We will define an indexing pipeline which depend on two parameters `embedding_model` and the `chunk_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85a74f47-f3e0-4ee3-855e-ab0e70c584a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "\n",
    "def indexing(embedding_model: str, chunk_size: int):\n",
    "    files_path = \"datasets/ARAGOG/papers_for_questions\"\n",
    "    document_store = InMemoryDocumentStore()\n",
    "    pipeline = Pipeline()\n",
    "    pipeline.add_component(\"converter\", PyPDFToDocument())\n",
    "    pipeline.add_component(\"cleaner\", DocumentCleaner())\n",
    "    pipeline.add_component(\"splitter\", DocumentSplitter(split_length=chunk_size))  # splitting by word\n",
    "    pipeline.add_component(\"writer\", DocumentWriter(document_store=document_store, policy=DuplicatePolicy.SKIP))\n",
    "    pipeline.add_component(\"embedder\", SentenceTransformersDocumentEmbedder(embedding_model))\n",
    "    pipeline.connect(\"converter\", \"cleaner\")\n",
    "    pipeline.connect(\"cleaner\", \"splitter\")\n",
    "    pipeline.connect(\"splitter\", \"embedder\")\n",
    "    pipeline.connect(\"embedder\", \"writer\")\n",
    "    pdf_files = [files_path+\"/\"+f_name for f_name in os.listdir(files_path)]\n",
    "    pipeline.run({\"converter\": {\"sources\": pdf_files}})\n",
    "\n",
    "    return document_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f28209a-c9e8-4646-9c6d-62654cb84dbd",
   "metadata": {},
   "source": [
    "## We will define a function to run each query over a RAG architecture. This pipeline has the following components:\n",
    "- ToDo...\n",
    "- ....\n",
    "- ...\n",
    "- \n",
    "\n",
    "## Notice that we run the RAG pipeline wrapped within a try/except, to avoid breaking the whole process if at some point there's an error while communicating with OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a7e8e57-0242-4517-8b8b-f4fab1b58cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from architectures.basic_rag import basic_rag\n",
    "\n",
    "def run_basic_rag(doc_store, sample_questions, embedding_model, top_k):\n",
    "    \"\"\"\n",
    "    A function to run the basic rag model on a set of sample questions and answers\n",
    "    \"\"\"\n",
    "\n",
    "    rag = basic_rag(document_store=doc_store, embedding_model=embedding_model, top_k=top_k)\n",
    "\n",
    "    predicted_answers = []\n",
    "    retrieved_contexts = []\n",
    "    for q in tqdm(sample_questions):\n",
    "        try:\n",
    "            response = rag.run(\n",
    "                data={\"query_embedder\": {\"text\": q}, \"prompt_builder\": {\"question\": q}, \"answer_builder\": {\"query\": q}})\n",
    "            predicted_answers.append(response[\"answer_builder\"][\"answers\"][0].data)\n",
    "            retrieved_contexts.append([d.content for d in response['answer_builder']['answers'][0].documents])\n",
    "        except BadRequestError as e:\n",
    "            print(f\"Error with question: {q}\")\n",
    "            print(e)\n",
    "            predicted_answers.append(\"error\")\n",
    "            retrieved_contexts.append(retrieved_contexts)\n",
    "\n",
    "    return retrieved_contexts, predicted_answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da584c81-b248-4126-9f04-66c9cb3f49a4",
   "metadata": {},
   "source": [
    "## We will define another function to run every predicted answer and ground truth labels through the Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19214799-c50d-4124-b83c-8c379ba85cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(sample_questions, sample_answers, retrieved_contexts, predicted_answers, embedding_model):\n",
    "    context_relevance = ContextRelevanceEvaluator(raise_on_failure=False)\n",
    "    faithfulness = FaithfulnessEvaluator(raise_on_failure=False)\n",
    "    sas = SASEvaluator(model=embedding_model)\n",
    "    sas.warm_up()\n",
    "\n",
    "    results = {\n",
    "        \"context_relevance\": context_relevance.run(sample_questions, retrieved_contexts),\n",
    "        \"faithfulness\": faithfulness.run(sample_questions, retrieved_contexts, predicted_answers),\n",
    "        \"sas\": sas.run(predicted_answers, sample_answers),\n",
    "    }\n",
    "\n",
    "    inputs = {'questions': sample_questions, \"true_answers\": sample_answers, \"predicted_answers\": predicted_answers}\n",
    "\n",
    "    return results, inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375ccde-a654-4583-a720-75b0f3354482",
   "metadata": {},
   "source": [
    "## We also need a function to orchestrate everything, indexing, running the dataset over the RAG for each possible parameter combination, and running the evaluation.\n",
    "## Notice that for each parameter combination, two `.csv` files are generated, one containing the aggregated scores and one the detailed scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c72ee36-914e-4f26-9527-c54320159d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_tuning(questions, answers):\n",
    "    embedding_models = {\n",
    "        \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"sentence-transformers/msmarco-distilroberta-base-v2\",\n",
    "        \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    }\n",
    "    top_k_values = [1, 2, 3]\n",
    "    chunk_sizes = [64, 128, 256]\n",
    "\n",
    "    # create results directory if it does not exist using Pathlib\n",
    "    out_path = Path(\"aragog_results\")\n",
    "    out_path.mkdir(exist_ok=True)\n",
    "\n",
    "    for embedding_model in embedding_models:\n",
    "        for top_k in top_k_values:\n",
    "            for chunk_size in chunk_sizes:\n",
    "                name_params = f\"{embedding_model.split('/')[-1]}__top_k:{top_k}__chunk_size:{chunk_size}\"\n",
    "                print(name_params)\n",
    "                print(\"Indexing documents\")\n",
    "                doc_store = indexing(embedding_model, chunk_size)\n",
    "                print(\"Running RAG pipeline\")\n",
    "                retrieved_contexts, predicted_answers = run_basic_rag(doc_store, questions, embedding_model, top_k)\n",
    "                print(f\"Running evaluation\")\n",
    "                results, inputs = run_evaluation(questions, answers, retrieved_contexts, predicted_answers, embedding_model)\n",
    "                eval_results = EvaluationRunResult(run_name=name_params, inputs=inputs, results=results)\n",
    "                eval_results.score_report().to_csv(f\"{out_path}/score_report_{name_params}.csv\")\n",
    "                eval_results.to_pandas().to_csv(f\"{out_path}/detailed_{name_params}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2306a16e-4cba-4c30-b271-0050560353f2",
   "metadata": {},
   "source": [
    "# We can then start the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db18ab18-93d9-483f-85e0-76d9c6b348e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_tuning(questions, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00660aae-32be-4ff8-8c03-9de0df9dc1d3",
   "metadata": {},
   "source": [
    "## After the process is finished we can analyze de results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
